{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T19:32:56.533152Z",
     "start_time": "2018-06-11T19:32:54.579197Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "\n",
    "class Feature:\n",
    "    \n",
    "    class Utils:\n",
    "        def _set_type(series, dtype):\n",
    "            _max, _min = max(series), min(series)\n",
    "            if dtype == 'uint':\n",
    "                if _max <= 255: return np.uint8\n",
    "                elif _max <= 65535: return np.uint16\n",
    "                elif _max <= 4294967295: return np.uint32\n",
    "                else: return np.uint64\n",
    "            elif dtype == 'int':\n",
    "                if _min >= -128 and _max <= 127: return np.int8\n",
    "                elif _min >=-32768 and _max <= 32767: return np.int16\n",
    "                elif _min >= -2147483648 and _max <= 2147483647: return np.int32\n",
    "                else: return np.int64\n",
    "            elif dtype == 'float':\n",
    "                if max(abs(_min), _max) <= 3.4028235e+38: return np.float32\n",
    "                else: return np.float64\n",
    "\n",
    "        def save(df=None, flg='both', train_len=0, url='./', name='default'):\n",
    "            if flg == 'train':\n",
    "                df.reset_index(drop=True).to_pickle(url + 'train__' + name + '.pkl')\n",
    "            elif flg == 'test':\n",
    "                df.reset_index(drop=True).to_pickle(url + 'test__' + name + '.pkl')\n",
    "            else:\n",
    "                df[:train_len].reset_index(drop=True).to_pickle(url + 'train__' + name + '.pkl')\n",
    "                df[train_len:].reset_index(drop=True).to_pickle(url + 'test__' + name + '.pkl')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # params['col']\n",
    "    def count(df=None, cols=None, col_name=None, params=None):\n",
    "        \"\"\"Transforms features by scaling each feature to a given range.\n",
    "        \n",
    "        This estimator scales and translates each feature individually such\n",
    "        that it is in the given range on the training set, i.e. between\n",
    "        zero and one.\n",
    "        The transformation is given by::\n",
    "            X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "            X_scaled = X_std * (max - min) + min\n",
    "        where min, max = feature_range.\n",
    "        This transformation is often used as an alternative to zero mean,\n",
    "        unit variance scaling.\n",
    "        Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
    "        .. versionadded:: 0.17\n",
    "           *minmax_scale* function interface\n",
    "            to :class:`sklearn.preprocessing.MinMaxScaler`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : dataframe, shape (n_samples, n_features)\n",
    "            The data.\n",
    "        cols : array-like\n",
    "            Array of string names of columns that want to count.\n",
    "        col_name : str\n",
    "            The name of column that you want to return at the end.\n",
    "        params : dictionary\n",
    "            Params is a dictionary that has various parametors.\n",
    "            In this method, only params['col'] is used.\n",
    "            params['col'] is a array of string of column name.\n",
    "        See also\n",
    "        --------\n",
    "        MinMaxScaler: Performs scaling to a given range using the``Transformer`` API\n",
    "            (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).\n",
    "        Notes\n",
    "        -----\n",
    "        For a comparison of the different scalers, transformers, and normalizers,\n",
    "        see :ref:`examples/preprocessing/plot_all_scaling.py\n",
    "        <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
    "        \"\"\"    \n",
    "        r_col = params['col']\n",
    "        dtype = {x: df[x].dtype for x in cols if x in df.columns.values}\n",
    "        d_cols = list(cols)\n",
    "        d_cols.append(r_col)\n",
    "        result = df[d_cols].groupby(by=cols)[[r_col]].count().rename(index=str, columns={r_col: col_name}).reset_index()\n",
    "        dtype[col_name] = Feature.Utils._set_type(result[col_name], 'uint')\n",
    "        _df = df.merge(result.astype(dtype), on=cols, how='left')\n",
    "        r = _df[[col_name]].copy()\n",
    "        del _df, result, d_cols, dtype\n",
    "        gc.collect()\n",
    "        return r\n",
    "    \n",
    "    def unique_count(df=None, cols=None, col_name=None, params=None):\n",
    "        r_col = cols[-1]\n",
    "        dtype = {x: df[x].dtype for x in cols[:-1] if x in df.columns.values}\n",
    "        result = df[cols].groupby(by=cols[:-1])[[r_col]].nunique().rename(index=str, columns={r_col: col_name}).reset_index()\n",
    "        dtype[col_name] = Feature.Utils._set_type(result[col_name], 'uint')\n",
    "        _df = df.merge(result.astype(dtype), on=cols, how='left')\n",
    "        r = _df[[col_name]].copy()\n",
    "        del _df, result, d_cols, dtype\n",
    "        gc.collect()\n",
    "        return r\n",
    "    \n",
    "    def cumulative_count(df=None, cols=None, col_name=None, params=None):\n",
    "        result = df[cols].groupby(by=cols).cumcount().rename(col_name)\n",
    "        r = result.astype(Feature.Utils._set_type(result, 'uint'))\n",
    "        r = r.to_frame()\n",
    "        del result\n",
    "        gc.collect()\n",
    "        return r\n",
    "    \n",
    "    def reverse_cumulative_count(df=None, cols=None, col_name=None, params=None):\n",
    "        result = df.sort_index(ascending=False)[cols].groupby(cols).cumcount().rename(col_name)\n",
    "        r = result.astype(Feature.Utils._set_type(result, 'uint')).to_frame()\n",
    "        r.sort_index(inplace=True)\n",
    "        del result\n",
    "        gc.collect()\n",
    "        return r\n",
    "    \n",
    "    def variance(df=None, cols=None, col_name=None, params=None):\n",
    "        group_cols = cols[:-1]\n",
    "        calc_col = cols[-1]\n",
    "        group = df[cols].groupby(by=group_cols)[[calc_col]].var().reset_index().rename(index=str, columns={calc_col: col_name}).fillna(0)\n",
    "        dtype = {x: df[x].dtype for x in group_cols if x in df.columns.values}\n",
    "        dtype[col_name] = Feature.Utils._set_type(group[col_name], 'float')\n",
    "        _df = df.merge(group.astype(dtype), on=group_cols, how='left')\n",
    "        r = _df[[col_name]].copy()\n",
    "        del dtype, _df, group\n",
    "        gc.collect()\n",
    "        return r\n",
    "    \n",
    "    # params['col'] = : additional col to help count\n",
    "    # params['coefficient']: \n",
    "    def count_std_over_mean(df=None, cols=None, col_name=None, params=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        group_cols = cols[:-1]\n",
    "        calc_col = cols[-1]\n",
    "        d_cols = list(cols)\n",
    "        d_cols.append(params['col'])\n",
    "        group = df[d_cols].groupby(by=cols)[[params['col']]].count().reset_index().rename(index=str, columns={params['col']: 'count'})\n",
    "        result = group.groupby(by=group_cols)[['count']].agg(['mean','std'])['count'].reset_index()\n",
    "        result[col_name] = ((int(params['coefficient']) * result['std']) / result['mean']).fillna(-1)\n",
    "        dtype = {x: df[x].dtype for x in group_cols if x in df.columns.values}\n",
    "        dtype[col_name] = Feature.Utils._set_type(result[col_name], 'float')\n",
    "        _df = df.merge(result.astype(dtype), on=group_cols, how='left')\n",
    "        r = _df[[col_name]].copy()\n",
    "        del d_cols, group, result, _df\n",
    "        gc.collect()\n",
    "        return r\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # params['n']: n, params['fillna']: fillna, cols[-1]: time\n",
    "    def time_to_n_next(df=None, cols=None, col_name=None, params=None):\n",
    "        group_cols = cols[:-1]\n",
    "        calc_col = cols[-1]\n",
    "        n = int(params['n'])\n",
    "        m = int(params['fillna'])\n",
    "        result = (df[cols].groupby(by=group_cols)[calc_col].shift(-n) - df[calc_col]).fillna(m)\n",
    "        result = result.astype(Feature.Utils._set_type(result, 'uint')).to_frame()\n",
    "        del n, m\n",
    "        gc.collect()\n",
    "        return result\n",
    "    \n",
    "    # params['n']: n, cols[-1]: time\n",
    "    def count_in_previous_n_time_unit(df=None, cols=None, col_name=None, params=None):\n",
    "        group_cols = cols[:-1]\n",
    "        calc_col = cols[-1]\n",
    "        n = int(params['n'])\n",
    "        encodings = df[group_cols[0]].copy()\n",
    "        if len(group_cols) > 1:\n",
    "            for c in group_cols[1 : ]:\n",
    "                encodings = encodings * (10 ** (int(np.log(df[c].max() + 1) / np.log(10)) + 1)) + df[c]\n",
    "        encodings = encodings.values\n",
    "        times = df[calc_col].values\n",
    "        dict_count = defaultdict(int)\n",
    "        result = []\n",
    "        bound = 0\n",
    "        for cur in range(len(encodings)):\n",
    "            while times[cur] - times[bound] > n:\n",
    "                dict_count[encodings[bound]] -= 1\n",
    "                bound += 1\n",
    "            result.append(dict_count[encodings[cur]])\n",
    "            dict_count[encodings[cur]] += 1\n",
    "        r = pd.DataFrame(result, columns=[col_name], dtype=Feature.Utils._set_type(result, 'uint'))\n",
    "        del encodings, times, dict_count, result, bound, n\n",
    "        gc.collect()\n",
    "        return r\n",
    "    \n",
    "    # cols[-1]: time\n",
    "    def count_in_next_n_time_unit(df=None, cols=None, col_name=None, params=None):\n",
    "        r = Feature.count_in_previous_n_time_unit(df.sort_index(ascending=False), cols, col_name, params)\n",
    "        r = r.reindex(index=r.index[::-1]).reset_index(drop=True)\n",
    "        gc.collect()\n",
    "        return r\n",
    "    \n",
    "    \n",
    "    \n",
    "    class Encoding:\n",
    "        # params['trainLen'], params['splitCol'], params['col']\n",
    "        def woe(df=None, cols=None, col_name=None, params=None):\n",
    "            return Feature.Encoding._wrapper(df, cols, col_name,\\\n",
    "                                      {'train_len': params['trainLen'],\\\n",
    "                                       'function': Feature.Encoding._woe,\\\n",
    "                                       'split_col': params['splitCol'],\\\n",
    "                                       'col': params['col']})\n",
    "        \n",
    "        def chi_square(df=None, cols=None, col_name=None, params=None):\n",
    "            return Feature.Encoding._wrapper(df, cols, col_name,\\\n",
    "                                      {'train_len': params['trainLen'],\\\n",
    "                                       'function': Feature.Encoding._chi_square,\\\n",
    "                                       'split_col':params['splitCol'],\\\n",
    "                                       'col': params['col']})\n",
    "        \n",
    "        def mean(df=None, cols=None, col_name=None, params=None):\n",
    "            return Feature.Encoding._wrapper(df, cols, col_name,\\\n",
    "                                      {'train_len': params['trainLen'],\\\n",
    "                                       'function': Feature.Encoding._mean,\\\n",
    "                                       'split_col':params['splitCol'],\\\n",
    "                                       'col': params['col']})\n",
    "        \n",
    "        def _wrapper(df=None, cols=None, col_name=None, params=None):\n",
    "            train = df[ : params['train_len']]\n",
    "            test = df[params['train_len']:]\n",
    "            return pd.concat([Feature.Encoding._train_wrapper(df[:params['train_len']],\\\n",
    "                                                              cols, params['col'],\\\n",
    "                                                              col_name, params['function'],\\\n",
    "                                                              params['split_col']),\\\n",
    "                              Feature.Encoding._testset_wrapper(df[:params['train_len']],\\\n",
    "                                                             df[params['train_len']:],\\\n",
    "                                                             cols, params['col'],\\\n",
    "                                                             col_name, params['function'])],\\\n",
    "                             ignore_index=True)\n",
    "        \n",
    "        def _train_wrapper(df, group_cols, label, col_name, func, split_col):\n",
    "            r_list = []\n",
    "            for i in range(df[split_col].min(), df[split_col].max() + 1):\n",
    "                dictionary = func(df=df[df[split_col]!=i], group_cols=group_cols, label=label, col_name=col_name)\n",
    "                r_list.append(df[df[split_col]==i].merge(dictionary, on=group_cols, how='left')[[col_name]])\n",
    "            r = pd.concat(r_list).fillna(-1).reset_index(drop=True)\n",
    "            del r_list, dictionary\n",
    "            gc.collect()\n",
    "            return r\n",
    "        \n",
    "        def _testset_wrapper(train, test, group_cols, label, col_name, func):\n",
    "            dictionary = func(df=train, group_cols=group_cols, label=label, col_name=col_name)\n",
    "            _df = test.merge(dictionary, on=group_cols, how='left')\n",
    "            r = _df[[col_name]].copy().fillna(-1)\n",
    "            del _df, dictionary\n",
    "            gc.collect()\n",
    "            return r\n",
    "        \n",
    "        def _woe(df=None, group_cols=None, label=None, col_name=None, params=None):\n",
    "            d_cols = list(group_cols)\n",
    "            d_cols.append(label)\n",
    "            group = df[d_cols].groupby(by=group_cols)[[label]].agg(['count','sum'])[label].reset_index()\n",
    "            positive = df[label].sum()\n",
    "            negative = df.shape[0] - positive\n",
    "            group[col_name] = np.log((group['sum']+0.5) / positive / ((group['count']-group['sum']+0.5) / negative))\n",
    "            dtype = {x: df[x].dtype for x in group_cols if x in df.columns.values}\n",
    "            dtype[col_name] = Feature.Utils._set_type(group[col_name], 'float')\n",
    "            group.astype(dtype)\n",
    "            return_cols = list(group_cols)\n",
    "            return_cols.append(col_name)\n",
    "            r = group[return_cols]\n",
    "            del d_cols, group, positive, negative, dtype, return_cols\n",
    "            gc.collect()\n",
    "            return r\n",
    "        \n",
    "        def _chi_square(df=None, group_cols=None, label=None, col_name=None, params=None):\n",
    "            total_count = df.shape[0]\n",
    "            total_sum = df[label].sum()\n",
    "            group = df.groupby(by=group_cols)[[label]].agg(['count','sum'])[label].reset_index().rename(index=str, columns={'sum': 'n11'})\n",
    "            group['n12'] = group['count'] - group['n11']\n",
    "            group['n21'] = total_sum - group['n11']\n",
    "            group['n22'] = total_count - group['n11'] - group['n12'] - group['n21']\n",
    "            group['e11'] = (group['n11'] + group['n12']) * (group['n11'] + group['n21']) / total_count\n",
    "            group['e12'] = (group['n11'] + group['n12']) * (group['n12'] + group['n22']) / total_count\n",
    "            group['e21'] = (group['n21'] + group['n22']) * (group['n11'] + group['n21']) / total_count\n",
    "            group['e22'] = (group['n21'] + group['n22']) * (group['n12'] + group['n22']) / total_count\n",
    "            group[col_name] = (group['n11'] - group['e11']) ** 2 / group['e11'] + \\\n",
    "                                  (group['n12'] - group['e12']) ** 2 / group['e12'] + \\\n",
    "                                  (group['n21'] - group['e21']) ** 2 / group['e21'] + \\\n",
    "                                  (group['n22'] - group['e22']) ** 2 / group['e22']\n",
    "            dtype = {x: df[x].dtype for x in group_cols if x in df.columns.values}\n",
    "            dtype[col_name] = Feature.Utils._set_type(group[col_name], 'float')\n",
    "            group.astype(dtype)\n",
    "            return_cols = list(group_cols)\n",
    "            return_cols.append(col_name)\n",
    "            r = group[return_cols]\n",
    "            del group, total_count, total_sum, dtype, return_cols\n",
    "            gc.collect()\n",
    "            return r\n",
    "        \n",
    "        def _mean(df=None, group_cols=None, label=None, col_name=None, params=None):\n",
    "            r = df.groupby(by=group_cols)[[label]].mean().reset_index().rename(index=str, columns={label:col_name})\n",
    "            r.astype(Feature.Utils._set_type(r[col_name], 'float'))\n",
    "            gc.collect()\n",
    "            return r\n",
    "            \n",
    "        \n",
    "        \n",
    "    class Kernels:\n",
    "        def square(df=None, cols=None, col_name=None, params=None):\n",
    "            r = df[[cols]].apply(lambda x: x ** 2)\n",
    "            r = r.astype(Feature.Utils._set_type(r, 'float'))\n",
    "            gc.collect()\n",
    "            return r\n",
    "        \n",
    "func_map = {\n",
    "        'count':                         Feature.count,\n",
    "        'unique_count':                  Feature.unique_count,\n",
    "        'cumulative_count':              Feature.cumulative_count,\n",
    "        'reverse_cumulative_count':      Feature.reverse_cumulative_count,\n",
    "        'variance':                      Feature.variance,\n",
    "        'count_std_over_mean':           Feature.count_std_over_mean,\n",
    "        'time_to_n_next':                Feature.time_to_n_next,\n",
    "        'count_in_previous_n_time_unit': Feature.count_in_previous_n_time_unit,\n",
    "        'count_in_next_n_time_unit':     Feature.count_in_next_n_time_unit,\n",
    "        'woe':                           Feature.Encoding.woe,\n",
    "        'chi_square':                    Feature.Encoding.chi_square,\n",
    "        'mean':                          Feature.Encoding.mean,\n",
    "        'square':                        Feature.Kernels.square\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T19:42:31.147776Z",
     "start_time": "2018-06-11T19:42:31.079768Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      t\n",
      "0     1\n",
      "1     1\n",
      "2     1\n",
      "3     5\n",
      "4     1\n",
      "5     1\n",
      "6     1\n",
      "7     5\n",
      "8     1\n",
      "9     1\n",
      "10    5\n",
      "11    6\n",
      "12    1\n",
      "13    1\n",
      "14    2\n",
      "15  222\n",
      "16  222\n",
      "17  222\n"
     ]
    }
   ],
   "source": [
    "# count\n",
    "# unique_count\n",
    "# cummulative_count\n",
    "# reverse_cummulative_count\n",
    "# variance\n",
    "# count_std_over_mean\n",
    "# time_to_n_next\n",
    "# count_in_previous_n_time_unit\n",
    "# count_in_next_n_time_unit\n",
    "# woe\n",
    "# chi_square\n",
    "# mean\n",
    "\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "PATH = './'\n",
    "x = pd.read_csv(PATH + 'a.txt')\n",
    "# print(x)\n",
    "\n",
    "\n",
    "y_print = Feature.time_to_n_next(df=x, cols=['a','t'], col_name='ddd', params={'train_len':9,'split_col':'t','col':'l','coefficient':10,'n':'1','fillna':'222'})\n",
    "print(y_print)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T19:32:41.539222Z",
     "start_time": "2018-06-11T19:32:41.534291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\n"
     ]
    }
   ],
   "source": [
    "print(int('-2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T15:10:17.937053Z",
     "start_time": "2018-06-06T15:10:17.929807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, [3, 4, 5, 6, 7]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
