{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:40:37.572444Z",
     "start_time": "2018-10-02T17:40:36.553060Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "__file__=''\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../LIB/'))\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../../../../automl/automl_libs/'))\n",
    "from env import FILE\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csc_matrix, csr_matrix, hstack\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU,CuDNNGRU,Flatten,BatchNormalization,CuDNNLSTM,Activation,BatchNormalization,Conv2D\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D,GlobalAveragePooling2D, GlobalMaxPooling1D,GlobalMaxPooling2D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import log_loss,roc_auc_score\n",
    "from keras.regularizers import l1\n",
    "from keras.regularizers import l2\n",
    "from keras.regularizers import l1_l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import re\n",
    "from keras import backend as K\n",
    "# To get learning rate\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from keras.layers import Dot, multiply\n",
    "from keras.layers import Reshape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:40:41.561495Z",
     "start_time": "2018-10-02T17:40:37.574135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: (1001650, 35)\n",
      "test shape is: (40024, 34)\n",
      "(1041674, 35)\n",
      "(1041674, 45)\n",
      "(1041674, 36)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_pickle(FILE.train_ori.value)\n",
    "print('train shape is: {}'.format(train.shape))\n",
    "test = pd.read_pickle(FILE.test_ori.value)\n",
    "print('test shape is: {}'.format(test.shape))\n",
    "\n",
    "# X = pd.concat([train.drop(['click'],axis=1),test])\n",
    "X = pd.concat([train,test],sort=False)\n",
    "print(X.shape)\n",
    "\n",
    "X_clean = pd.read_csv('../../data/original/cleaned_data_price_final.csv')\n",
    "X_wei = pd.read_pickle('../../data/nn_features/clean.pkl')\n",
    "\n",
    "X_shiyi = pd.read_pickle(FILE.shiyi_fillna_ori.value)\n",
    "print(X_shiyi.shape)\n",
    "\n",
    "X = X.merge(X_shiyi[['time_hour','instance_id']],how='inner',on='instance_id')\n",
    "print(X.shape)\n",
    "assert np.sum(X_clean['instance_id'].values != X['instance_id'].values) == 0\n",
    "assert np.sum(X_wei['instance_id'].values != X['instance_id'].values) == 0\n",
    "X['model'] = X_wei['model_new'].values\n",
    "# X.drop('model',axis=1,inplace=True)\n",
    "# X['price'] = X_clean['price'].values\n",
    "# X['city_province'] = X_wei['city_province'].values\n",
    "# X['province'] = X_wei['province1'].values\n",
    "# X.drop(['city','province'],axis=1,inplace=True)\n",
    "# X['make'] = X_wei['make_new'].values\n",
    "\n",
    "ignore_columns = ['instance_id','time','click'] + ['creative_is_js', 'creative_is_voicead', 'app_paid']\n",
    "X_nu = X_clean[['price']].copy()\n",
    "X_nu['time'] = (X['time'] - X['time'].min()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:40:41.566653Z",
     "start_time": "2018-10-02T17:40:41.562847Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_col=['user_tags']\n",
    "numu_col=[]\n",
    "non_doc_col = list(set(X.columns) - set(ignore_columns) - set(doc_col))\n",
    "def combALL(X,sep=' '):\n",
    "    le = LabelEncoder()\n",
    "    ret = pd.DataFrame()\n",
    "    cols = list(X.columns)\n",
    "    if cols[0] in ['city','province']:\n",
    "        ret['comALL'] = 'cp_'+X[cols[0]].astype(str)\n",
    "    else:\n",
    "        ret['comALL'] = le.fit_transform(X[cols[0]].astype(str))\n",
    "        ret['comALL'] = cols[0]+'_'+ret['comALL'].astype(str)\n",
    "    for col in tqdm(cols[1:]):\n",
    "        if col in ['city','province']:\n",
    "            ret['tmp'] = 'cp_'+X[col].astype(str)\n",
    "            ret['comALL'] = ret['comALL'] + sep + ret['tmp'].astype(str)\n",
    "        else:\n",
    "            ret['tmp'] = le.fit_transform(X[col].astype(str))\n",
    "            ret['comALL'] = ret['comALL'] + sep + col + '_'+ ret['tmp'].astype(str)\n",
    "    return ret['comALL'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:41:41.579429Z",
     "start_time": "2018-10-02T17:40:41.567975Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:58<00:00,  2.08s/it]\n"
     ]
    }
   ],
   "source": [
    "X['comALL'] = combALL(X[non_doc_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:41:42.545096Z",
     "start_time": "2018-10-02T17:41:41.581227Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.32it/s]\n"
     ]
    }
   ],
   "source": [
    "need_process_col = list(set(X.columns) - set(ignore_columns))\n",
    "\n",
    "\n",
    "doc_col = ['user_tags']\n",
    "non_doc_col = [f for f in need_process_col if f not in doc_col]\n",
    "non_doc_col = ['comALL']\n",
    "X_ = X[non_doc_col].copy()\n",
    "\n",
    "X_doc = X[doc_col].copy()\n",
    "\n",
    "    \n",
    "    \n",
    "for col in tqdm(doc_col):\n",
    "    X_doc[col] = X_doc[col].astype(str)\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:41:42.554973Z",
     "start_time": "2018-10-02T17:41:42.546832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['creative_is_jump_1 os_name_0 creative_type_4 app_id_232 app_cate_id_7 creative_tp_dnf_13 carrier_1 advert_id_1 devtype_2 creative_is_download_0 adid_553 inner_slot_id_623 creative_width_2 osv_153 cp_137103102105100 cp_137103102100100 orderid_302 creative_id_209 f_channel_77 model_2269 os_2 creative_has_deeplink_0 time_hour_12 advert_industry_inner_17 creative_height_9 nnt_1 advert_name_25 campaign_id_1 make_864'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:41:42.563832Z",
     "start_time": "2018-10-02T17:41:42.556432Z"
    }
   },
   "outputs": [],
   "source": [
    "# # def comb_fe(X,cols,sep=' '):\n",
    "# #     ret = X[cols[0]].astype(str).copy()\n",
    "# #     for col in cols[1:]:\n",
    "# #         ret = ret + sep + X[col].astype(str)\n",
    "# #     return ret.values\n",
    "\n",
    "# def process_slot(x):\n",
    "#     x = re.sub(r'[-_]',' ',x)\n",
    "#     x = x.split(' ')\n",
    "#     ret = ['p{}_'.format(c)+x[c] for c in range(len(x))]\n",
    "#     ret = ' '.join(ret)\n",
    "#     return ret\n",
    "        \n",
    "    \n",
    "\n",
    "# def comb_fe(X,cols,sep=' '):\n",
    "#     def add_col_name(x,colName,splitter=' ' ,sep=' '):\n",
    "#         ret = [colName+'_'+each for each in x.split(splitter)]\n",
    "#         return sep.join(ret)\n",
    "#     ret = pd.DataFrame()\n",
    "#     if cols[0] == 'user_tags':\n",
    "#         ret['comb'] = X['user_tags'].astype(str).apply(add_col_name,colName='userTags',splitter=',',sep=sep)\n",
    "#     else:\n",
    "#         if cols[0] in original_name_col:\n",
    "#             ret['comb'] = X[cols[0]].astype(str).copy()\n",
    "#         else:\n",
    "#             ret['comb'] = X[cols[0]].astype(str).apply(add_col_name,colName=cols[0],splitter=' ',sep=sep)\n",
    "\n",
    "#     for col in tqdm(cols[1:]):\n",
    "#         if col == 'user_tags':\n",
    "#             ret['new_feature'] = X['user_tags'].astype(str).apply(add_col_name,colName='userTags',splitter=',',sep=sep)\n",
    "#         else:\n",
    "#             if col in original_name_col:\n",
    "#                 ret['new_feature'] = X[col].astype(str).copy()\n",
    "#             else:\n",
    "#                 ret['new_feature'] = X[col].astype(str).apply(add_col_name,colName=col,splitter=' ',sep=sep)\n",
    "#         ret['comb'] =( ret['comb'] + sep + ret['new_feature']).values\n",
    "#     return ret['comb'].values\n",
    "\n",
    "# processed_col = []\n",
    "# original_name_col = ['model','make','os','osv']\n",
    "\n",
    "# doc_col=['user_tags']\n",
    "# X_doc = X[doc_col].copy()\n",
    "# processed_col.extend(doc_col)\n",
    "# cob_col = []\n",
    "# for each in cob_col:\n",
    "#     feature_name = '_'.join(each)\n",
    "#     processed_col.extend(each)\n",
    "#     doc_col.append(feature_name)\n",
    "#     X_doc[feature_name] = comb_fe(X,each)\n",
    "    \n",
    "# non_doc_col = list(set(X.columns) - set(ignore_columns) - set(processed_col))\n",
    "# non_doc_col = ['comALL']\n",
    "# #!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# # 0.414797\n",
    "# # non_doc_col = list(set(X.columns) - set(ignore_columns)- set(doc_col))\n",
    "# X_ = X[non_doc_col].copy()\n",
    "\n",
    "# # for col in tqdm(non_doc_col):\n",
    "# #     X_[col] = le.fit_transform(X_[col].astype(str))\n",
    "# #     X_[col] = col + '_'+X_[col].astype(str)\n",
    "    \n",
    "# for col in tqdm(doc_col):\n",
    "#     if col=='inner_slot_id':\n",
    "#         X_doc[col] = X_doc[col].astype(str).apply(process_slot)\n",
    "#     else:\n",
    "#         X_doc[col] = X_doc[col].astype(str)\n",
    "# numu_col= list(X_nu.columns)\n",
    "# numu_col = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:41:42.576173Z",
     "start_time": "2018-10-02T17:41:42.565095Z"
    }
   },
   "outputs": [],
   "source": [
    "train_index = pickle.load(open(FILE.train_index.value,'rb'))\n",
    "holdout_index = pickle.load(open(FILE.holdout_index.value,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:41:47.411341Z",
     "start_time": "2018-10-02T17:41:42.578378Z"
    }
   },
   "outputs": [],
   "source": [
    "num_folds = 7\n",
    "seed = 1001\n",
    "train_index_list = []\n",
    "val_index_list = []\n",
    "folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=seed)\n",
    "for t,v in folds.split(train.loc[train_index],train.loc[train_index,'click']):\n",
    "    train_index_list.append(train.loc[train_index].iloc[t].index)\n",
    "    val_index_list.append(train.loc[train_index].iloc[v].index)\n",
    "    \n",
    "check = []\n",
    "for i in val_index_list:\n",
    "    check.extend(list(i))\n",
    "assert len(set(check+list(holdout_index))) == len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:41:47.901991Z",
     "start_time": "2018-10-02T17:41:47.413437Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fold_y = {}\n",
    "val_fold_y = {}\n",
    "holdout_y = train.loc[holdout_index,'click'].values\n",
    "for fold in range(num_folds):\n",
    "    train_fold_y[fold] = train.loc[train_index_list[fold],'click'].values\n",
    "    val_fold_y[fold] = train.loc[val_index_list[fold],'click'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:42:56.442468Z",
     "start_time": "2018-10-02T17:41:47.903738Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:40<00:00, 40.78s/it]\n",
      "100%|██████████| 1/1 [00:27<00:00, 27.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process numerical input\n"
     ]
    }
   ],
   "source": [
    "info_dict = {}\n",
    "train_all_dict = {}\n",
    "train_fold_dict = {}\n",
    "\n",
    "val_fold_dict = {}\n",
    "\n",
    "holdout_input_dict = {}\n",
    "test_input_dict = {}\n",
    "maxlen = 1\n",
    "prefix_input_nonDoc = 'input_'\n",
    "prefix_input_Doc = 'input_rnn_'\n",
    "prefix_input_nu = 'input_Num_'\n",
    "sequence_size_dict_nonDoc = {}\n",
    "for col in tqdm(non_doc_col):\n",
    "    \n",
    "    maxlen = 1\n",
    "    if col in ['city','province']:\n",
    "        maxlen = 5\n",
    "        filters = ' '\n",
    "    elif col == 'model':\n",
    "        maxlen = 1\n",
    "        filters = '@'\n",
    "    else:\n",
    "        maxlen = 50\n",
    "        filters=' '\n",
    "    sequence_size_dict_nonDoc[col]=maxlen\n",
    "    tok=text.Tokenizer(num_words=X_[col].nunique(),lower=False,filters=filters)\n",
    "    tok.fit_on_texts(list(X_[col]))\n",
    "    info_dict.update({prefix_input_nonDoc+col:{'tok':tok}})\n",
    "    t = tok.texts_to_sequences(list(X_[col].iloc[:1001650].values))\n",
    "    te = tok.texts_to_sequences(list(X_[col].iloc[1001650:].values))\n",
    "    train_all_dict[prefix_input_nonDoc+col] = sequence.pad_sequences(t,maxlen=maxlen)\n",
    "    test_input_dict[prefix_input_nonDoc+col] = sequence.pad_sequences(te,maxlen=maxlen)\n",
    "    holdout_input_dict[prefix_input_nonDoc+col] = train_all_dict[prefix_input_nonDoc+col][list(holdout_index)]\n",
    "    \n",
    "    for fold in range(num_folds):\n",
    "        if train_fold_dict.get(fold) is None:\n",
    "            train_fold_dict[fold] = {}\n",
    "            val_fold_dict[fold] = {}\n",
    "        train_fold_dict[fold].update({prefix_input_nonDoc+col: train_all_dict[prefix_input_nonDoc+col][list(train_index_list[fold])]})\n",
    "        val_fold_dict[fold].update({prefix_input_nonDoc+col:train_all_dict[prefix_input_nonDoc+col][list(val_index_list[fold])]})\n",
    "        \n",
    "sequence_size_dict = {}\n",
    "for col in tqdm(doc_col):\n",
    "    if 'user_tags' in col:\n",
    "        maxlen = 100\n",
    "        tok=text.Tokenizer(num_words=X_doc[col].nunique(),lower=False,filters=',')\n",
    "    else:\n",
    "        maxlen = 20\n",
    "        tok=text.Tokenizer(num_words=X_doc[col].nunique(),lower=False,filters=' ')\n",
    "    tok.fit_on_texts(list(X_doc[col]))\n",
    "    info_dict.update({prefix_input_Doc+col:{'tok':tok}})\n",
    "    sequence_size_dict[col] = maxlen\n",
    "    t = tok.texts_to_sequences(list(X_doc[col].iloc[:1001650].values))\n",
    "    te = tok.texts_to_sequences(list(X_doc[col].iloc[1001650:].values))\n",
    "    train_all_dict[prefix_input_Doc+col] = sequence.pad_sequences(t,maxlen=maxlen)\n",
    "    test_input_dict[prefix_input_Doc+col] = sequence.pad_sequences(te,maxlen=maxlen)\n",
    "    holdout_input_dict[prefix_input_Doc+col] = train_all_dict[prefix_input_Doc+col][list(holdout_index)]\n",
    "    \n",
    "    for fold in range(num_folds):\n",
    "        if train_fold_dict.get(fold) is None:\n",
    "            train_fold_dict[fold] = {}\n",
    "            val_fold_dict[fold] = {}\n",
    "        train_fold_dict[fold].update({prefix_input_Doc+col: train_all_dict[prefix_input_Doc+col][list(train_index_list[fold])]})\n",
    "        val_fold_dict[fold].update({prefix_input_Doc+col:train_all_dict[prefix_input_Doc+col][list(val_index_list[fold])]})\n",
    "        \n",
    "print('process numerical input')\n",
    "train_all_dict[prefix_input_nu] = X_nu[numu_col].iloc[:1001650].values\n",
    "test_input_dict[prefix_input_nu] = X_nu[numu_col].iloc[1001650:].values\n",
    "holdout_input_dict[prefix_input_nu] = train_all_dict[prefix_input_nu][list(holdout_index)]\n",
    "for fold in range(num_folds):\n",
    "        if train_fold_dict.get(fold) is None:\n",
    "            train_fold_dict[fold] = {}\n",
    "            val_fold_dict[fold] = {}\n",
    "        train_fold_dict[fold].update({prefix_input_nu: train_all_dict[prefix_input_nu][list(train_index_list[fold])]})\n",
    "        val_fold_dict[fold].update({prefix_input_nu:train_all_dict[prefix_input_nu][list(val_index_list[fold])]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build NN model only use model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.41456-- 71 epoch-- include price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T18:47:28.896741Z",
     "start_time": "2018-10-02T18:47:28.877071Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_nn_model(cols,doc_cols=[],numu_cols=[]):\n",
    "    \"\"\"\n",
    "    cols, used to do ebd and dense layers\n",
    "    doc_cols: used to do rnn\n",
    "    there can be overlaps\n",
    "    \"\"\"\n",
    "    input_list = []\n",
    "    concat_list = []\n",
    "    doc_list = []\n",
    "    numu_list = []\n",
    "    for col in cols:\n",
    "        max_feature = len(info_dict[prefix_input_nonDoc+col]['tok'].index_word)\n",
    "        embed_size = int(np.log2(max_feature)/np.log2(1.5))\n",
    "        print('non doc ebd size:{}'.format(embed_size))\n",
    "        if embed_size< 2:\n",
    "            embed_size = 2\n",
    "        input_shape = sequence_size_dict_nonDoc[col]\n",
    "        cur_input = Input(shape=(input_shape, ),name = prefix_input_nonDoc+col)\n",
    "        \n",
    "       \n",
    "        embed_layer = Embedding(max_feature,\n",
    "                            embed_size,\n",
    "#                                 100,\n",
    "                            input_length=input_shape,\n",
    "                            trainable=True,\n",
    "#                                 embeddings_initializer='he_normal',\n",
    "                            embeddings_regularizer=l2(0.0005),\n",
    "                            name='ebd_'+col)(cur_input)\n",
    "        embed_layer = SpatialDropout1D(0.5)(embed_layer)\n",
    "        x = Reshape((int(embed_layer.shape[1]),\n",
    "                     int(embed_layer.shape[2]),\n",
    "                     1),name='reshape')(embed_layer)\n",
    "        x = Flatten()(embed_layer)\n",
    "        input_list.append(cur_input)\n",
    "        concat_list.append(x)\n",
    "    for col in doc_cols:\n",
    "        max_feature = len(info_dict[prefix_input_Doc+col]['tok'].index_word)\n",
    "        embed_size = int(np.log2(max_feature)/np.log2(1.5))\n",
    "        if embed_size< 2:\n",
    "            embed_size = 2\n",
    "        input_shape = sequence_size_dict[col]\n",
    "        cur_input = Input(shape=(input_shape, ),name = prefix_input_Doc+col)\n",
    "        embed_layer = Embedding(max_feature,\n",
    "                            embed_size,\n",
    "#                                 embeddings_initializer='he_normal',\n",
    "                            input_length=input_shape,\n",
    "                            trainable=True,\n",
    "                            embeddings_regularizer=l2(0.0005),\n",
    "                            name='ebd_rnn_'+col)(cur_input)\n",
    "        x = SpatialDropout1D(0.5)(embed_layer)\n",
    "        x = Bidirectional(CuDNNGRU(25, return_sequences=True))(x)\n",
    "#         x = Reshape((int(x.shape[1]),\n",
    "#                      int(x.shape[2]),\n",
    "#                      1),name='reshape')(x)\n",
    "#         x1 = Conv2D(filters=64,kernel_size=(3,5))(x)\n",
    "#         x_aveP1 = GlobalAveragePooling2D()(x1)\n",
    "#         x_maxP1 = GlobalMaxPooling2D()(x1)\n",
    "#         x = concatenate([x_aveP1,x_maxP1])\n",
    "#         x = Flatten()(x)\n",
    "        x = Conv1D(25, kernel_size = 3, padding = \"valid\", kernel_initializer = \"he_normal\")(x)\n",
    "        x_aveP = GlobalAveragePooling1D()(x)\n",
    "        x_maxP = GlobalMaxPooling1D()(x)\n",
    "        x = concatenate([x_aveP,x_maxP])\n",
    "        input_list.append(cur_input)\n",
    "        concat_list.append(x)\n",
    "\n",
    "    if len(numu_cols) > 0:\n",
    "        print('add numu...')\n",
    "        nu_shape = len(numu_cols)\n",
    "        cur_input = Input(shape=(nu_shape, ),name = prefix_input_nu)\n",
    "        x = BatchNormalization()(cur_input)\n",
    "        x = Dense(1, activation=None)(x)\n",
    "#         x = Dropout(0.2)(x)\n",
    "        input_list.append(cur_input)\n",
    "        numu_list.append(x)\n",
    "       \n",
    "    if len(concat_list) > 1:\n",
    "        x = concatenate(concat_list)\n",
    "#     x = BatchNormalization()(x)\n",
    "    \n",
    "\n",
    "    if len(numu_list)>0:\n",
    "        x = concatenate([x]+numu_list)\n",
    "#         x = BatchNormalization()(x)\n",
    "        \n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    preds = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(input_list, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:42:56.476653Z",
     "start_time": "2018-10-02T17:42:56.454704Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_each_fold(input_train_dict,input_val_dict,y_train,y_val,cols,doc_col=[],numu_col=[],tolerance=30,):\n",
    "    model = get_nn_model(cols,doc_col,numu_col)\n",
    "    cur_to = 0\n",
    "    best_logloss = None\n",
    "    best_weights = None\n",
    "    count = 0\n",
    "    base_lr = 0.001\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    while True:\n",
    "#         decay = (80 - count)/80\n",
    "#         decay = max(decay,0.1)\n",
    "#         lr = base_lr * decay\n",
    "#         K.set_value(model.optimizer.lr, lr)\n",
    "        history = model.fit(input_train_dict, y_train, \n",
    "                  batch_size=5000, \n",
    "                  epochs=1,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  )\n",
    "        preds = model.predict(input_val_dict,5000,verbose=1)\n",
    "        logloss = log_loss(y_val,preds)\n",
    "        train_loss_list.append(history.history['loss'])\n",
    "        val_loss_list.append(logloss)\n",
    "        roc = roc_auc_score(y_val,preds)\n",
    "        print(logloss)\n",
    "        print(roc)\n",
    "        if best_logloss is None:\n",
    "            best_logloss = logloss\n",
    "            best_weights = model.get_weights()\n",
    "        else:\n",
    "            if best_logloss > logloss:\n",
    "                best_logloss = logloss\n",
    "                best_weights = model.get_weights()\n",
    "                cur_to = 0\n",
    "            else:\n",
    "                cur_to +=1\n",
    "        if cur_to == tolerance:\n",
    "            break\n",
    "        print('best logloss is: {}'.format(best_logloss))\n",
    "        print('remainning trial is: {}/{}'.format(cur_to,tolerance))\n",
    "        print('total epoch trained: {}'.format(count))\n",
    "        count+=1\n",
    "    model.set_weights(best_weights)\n",
    "    train_preds = model.predict(input_train_dict,5000,verbose=1)\n",
    "    print('training loss is: {}'.format(log_loss(y_train,train_preds)))\n",
    "    return model,train_loss_list,val_loss_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T19:06:35.936126Z",
     "start_time": "2018-10-02T18:47:38.566235Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non doc ebd size:23\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 16s 19us/step - loss: 0.4601 - acc: 0.7996\n",
      "137376/137376 [==============================] - 1s 7us/step\n",
      "0.4198495430053561\n",
      "0.7601145951060395\n",
      "best logloss is: 0.4198495430053561\n",
      "remainning trial is: 0/35\n",
      "total epoch trained: 0\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4260 - acc: 0.8047\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4179941665967334\n",
      "0.7634190618376229\n",
      "best logloss is: 0.4179941665967334\n",
      "remainning trial is: 0/35\n",
      "total epoch trained: 1\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4241 - acc: 0.8051\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41784807073865976\n",
      "0.7644252442172135\n",
      "best logloss is: 0.41784807073865976\n",
      "remainning trial is: 0/35\n",
      "total epoch trained: 2\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4234 - acc: 0.8054\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4176735252091553\n",
      "0.7645694502850404\n",
      "best logloss is: 0.4176735252091553\n",
      "remainning trial is: 0/35\n",
      "total epoch trained: 3\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4230 - acc: 0.8056\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4176113794017469\n",
      "0.7651177347906271\n",
      "best logloss is: 0.4176113794017469\n",
      "remainning trial is: 0/35\n",
      "total epoch trained: 4\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4226 - acc: 0.8056\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4172001178287088\n",
      "0.7663907544103334\n",
      "best logloss is: 0.4172001178287088\n",
      "remainning trial is: 0/35\n",
      "total epoch trained: 5\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4224 - acc: 0.8058\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41649754831957325\n",
      "0.7667197253472882\n",
      "best logloss is: 0.41649754831957325\n",
      "remainning trial is: 0/35\n",
      "total epoch trained: 6\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4221 - acc: 0.8057\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41795797856000066\n",
      "0.7661493446779992\n",
      "best logloss is: 0.41649754831957325\n",
      "remainning trial is: 1/35\n",
      "total epoch trained: 7\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4220 - acc: 0.8059\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.416538622791174\n",
      "0.7665722324867907\n",
      "best logloss is: 0.41649754831957325\n",
      "remainning trial is: 2/35\n",
      "total epoch trained: 8\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4218 - acc: 0.8059\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41615188018426297\n",
      "0.7673647860240922\n",
      "best logloss is: 0.41615188018426297\n",
      "remainning trial is: 0/35\n",
      "total epoch trained: 9\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4217 - acc: 0.8061\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41674448347242976\n",
      "0.7674539911256257\n",
      "best logloss is: 0.41615188018426297\n",
      "remainning trial is: 1/35\n",
      "total epoch trained: 10\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4216 - acc: 0.8058\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4158554588770295\n",
      "0.7682694509280825\n",
      "best logloss is: 0.4158554588770295\n",
      "remainning trial is: 0/35\n",
      "total epoch trained: 11\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4214 - acc: 0.8061\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4157151818041815\n",
      "0.7681562593148838\n",
      "best logloss is: 0.4157151818041815\n",
      "remainning trial is: 0/35\n",
      "total epoch trained: 12\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4213 - acc: 0.8062\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4161491452316199\n",
      "0.7671695078568732\n",
      "best logloss is: 0.4157151818041815\n",
      "remainning trial is: 1/35\n",
      "total epoch trained: 13\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4213 - acc: 0.8061\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4160282502722618\n",
      "0.7680014820488211\n",
      "best logloss is: 0.4157151818041815\n",
      "remainning trial is: 2/35\n",
      "total epoch trained: 14\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4212 - acc: 0.8061\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4156777543135351\n",
      "0.7681081452955236\n",
      "best logloss is: 0.4156777543135351\n",
      "remainning trial is: 0/35\n",
      "total epoch trained: 15\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4210 - acc: 0.8063\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41641226851174284\n",
      "0.7683094953024467\n",
      "best logloss is: 0.4156777543135351\n",
      "remainning trial is: 1/35\n",
      "total epoch trained: 16\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4210 - acc: 0.8064\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4157279896041379\n",
      "0.768069691423126\n",
      "best logloss is: 0.4156777543135351\n",
      "remainning trial is: 2/35\n",
      "total epoch trained: 17\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4209 - acc: 0.8064\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4155191916627147\n",
      "0.768497795250746\n",
      "best logloss is: 0.4155191916627147\n",
      "remainning trial is: 0/35\n",
      "total epoch trained: 18\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4209 - acc: 0.8063\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4154722416587469\n",
      "0.7682177404253623\n",
      "best logloss is: 0.4154722416587469\n",
      "remainning trial is: 0/35\n",
      "total epoch trained: 19\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4208 - acc: 0.8064\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4156166956244035\n",
      "0.7681469238759392\n",
      "best logloss is: 0.4154722416587469\n",
      "remainning trial is: 1/35\n",
      "total epoch trained: 20\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4207 - acc: 0.8063\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41573406329716706\n",
      "0.7689016290729074\n",
      "best logloss is: 0.4154722416587469\n",
      "remainning trial is: 2/35\n",
      "total epoch trained: 21\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4206 - acc: 0.8065\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41550471079368384\n",
      "0.7686803934530637\n",
      "best logloss is: 0.4154722416587469\n",
      "remainning trial is: 3/35\n",
      "total epoch trained: 22\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4207 - acc: 0.8064\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41530824480443934\n",
      "0.7687896183223207\n",
      "best logloss is: 0.41530824480443934\n",
      "remainning trial is: 0/35\n",
      "total epoch trained: 23\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4206 - acc: 0.8065\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.415468174388061\n",
      "0.7686849725702968\n",
      "best logloss is: 0.41530824480443934\n",
      "remainning trial is: 1/35\n",
      "total epoch trained: 24\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4204 - acc: 0.8066\n",
      "137376/137376 [==============================] - 1s 4us/step\n",
      "0.4150172980774767\n",
      "0.769306270326774\n",
      "best logloss is: 0.4150172980774767\n",
      "remainning trial is: 0/35\n",
      "total epoch trained: 25\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4203 - acc: 0.8066\n",
      "137376/137376 [==============================] - 1s 4us/step\n",
      "0.415475568001184\n",
      "0.7684753979061738\n",
      "best logloss is: 0.4150172980774767\n",
      "remainning trial is: 1/35\n",
      "total epoch trained: 26\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4203 - acc: 0.8066\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4154595321726929\n",
      "0.7688792689379383\n",
      "best logloss is: 0.4150172980774767\n",
      "remainning trial is: 2/35\n",
      "total epoch trained: 27\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4204 - acc: 0.8066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41565861117556213\n",
      "0.7690625732884226\n",
      "best logloss is: 0.4150172980774767\n",
      "remainning trial is: 3/35\n",
      "total epoch trained: 28\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4202 - acc: 0.8068\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4152901784192325\n",
      "0.7689842899563202\n",
      "best logloss is: 0.4150172980774767\n",
      "remainning trial is: 4/35\n",
      "total epoch trained: 29\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4202 - acc: 0.8068\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41657607268614677\n",
      "0.7688891027853362\n",
      "best logloss is: 0.4150172980774767\n",
      "remainning trial is: 5/35\n",
      "total epoch trained: 30\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4202 - acc: 0.8066\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41512871127400636\n",
      "0.7691692644006127\n",
      "best logloss is: 0.4150172980774767\n",
      "remainning trial is: 6/35\n",
      "total epoch trained: 31\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4201 - acc: 0.8068\n",
      "137376/137376 [==============================] - 1s 4us/step\n",
      "0.41523566663724754\n",
      "0.7692716603899412\n",
      "best logloss is: 0.4150172980774767\n",
      "remainning trial is: 7/35\n",
      "total epoch trained: 32\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4201 - acc: 0.8067\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4149688248922129\n",
      "0.7692999291763091\n",
      "best logloss is: 0.4149688248922129\n",
      "remainning trial is: 0/35\n",
      "total epoch trained: 33\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4201 - acc: 0.8069\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41531203112231213\n",
      "0.7693487631931499\n",
      "best logloss is: 0.4149688248922129\n",
      "remainning trial is: 1/35\n",
      "total epoch trained: 34\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4200 - acc: 0.8068\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4157573348109999\n",
      "0.7690715025920563\n",
      "best logloss is: 0.4149688248922129\n",
      "remainning trial is: 2/35\n",
      "total epoch trained: 35\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4198 - acc: 0.8069\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41515954597812854\n",
      "0.7690083588964337\n",
      "best logloss is: 0.4149688248922129\n",
      "remainning trial is: 3/35\n",
      "total epoch trained: 36\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4200 - acc: 0.8066\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41476793023364594\n",
      "0.7702360248486505\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 0/35\n",
      "total epoch trained: 37\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4198 - acc: 0.8070\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41571681956828516\n",
      "0.7694013443672152\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 1/35\n",
      "total epoch trained: 38\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4197 - acc: 0.8070\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41524813340761807\n",
      "0.7687832800084622\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 2/35\n",
      "total epoch trained: 39\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4198 - acc: 0.8069\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4151308350112613\n",
      "0.769148646275771\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 3/35\n",
      "total epoch trained: 40\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4195 - acc: 0.8071\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.415238834633809\n",
      "0.7688116464074666\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 4/35\n",
      "total epoch trained: 41\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4197 - acc: 0.8070\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41492265046798615\n",
      "0.7693185157902989\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 5/35\n",
      "total epoch trained: 42\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4197 - acc: 0.8070\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41535954890572285\n",
      "0.7689728644389414\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 6/35\n",
      "total epoch trained: 43\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4196 - acc: 0.8070\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41511157392940784\n",
      "0.7698963233629165\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 7/35\n",
      "total epoch trained: 44\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4196 - acc: 0.8071\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41520501759901796\n",
      "0.7690636425222246\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 8/35\n",
      "total epoch trained: 45\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4195 - acc: 0.8072\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.415439502748174\n",
      "0.7690036049107509\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 9/35\n",
      "total epoch trained: 46\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4195 - acc: 0.8071\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4148743900425765\n",
      "0.7696220542136959\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 10/35\n",
      "total epoch trained: 47\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4195 - acc: 0.8072\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41525044477694534\n",
      "0.7689806535936097\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 11/35\n",
      "total epoch trained: 48\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4194 - acc: 0.8072\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4152334180738798\n",
      "0.7692164710396024\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 12/35\n",
      "total epoch trained: 49\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4194 - acc: 0.8070\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4152380015954376\n",
      "0.7688676079822179\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 13/35\n",
      "total epoch trained: 50\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4195 - acc: 0.8073\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.415429855875509\n",
      "0.7694240906143903\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 14/35\n",
      "total epoch trained: 51\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4193 - acc: 0.8072\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4150498045395073\n",
      "0.7690817562572202\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 15/35\n",
      "total epoch trained: 52\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4192 - acc: 0.8073\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4157177620217756\n",
      "0.7692236578325314\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 16/35\n",
      "total epoch trained: 53\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4193 - acc: 0.8073\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41521464109416484\n",
      "0.7691304464414243\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 17/35\n",
      "total epoch trained: 54\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4192 - acc: 0.8074\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4150643702027069\n",
      "0.7691501933943434\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 18/35\n",
      "total epoch trained: 55\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4190 - acc: 0.8075\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4153754458441282\n",
      "0.768957872306001\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 19/35\n",
      "total epoch trained: 56\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4190 - acc: 0.8073\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4149606333983077\n",
      "0.7696404861492001\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 20/35\n",
      "total epoch trained: 57\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4190 - acc: 0.8073\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4154762307968536\n",
      "0.7690830364010606\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 21/35\n",
      "total epoch trained: 58\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4189 - acc: 0.8073\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4152565033353288\n",
      "0.7696640721987671\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 22/35\n",
      "total epoch trained: 59\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4191 - acc: 0.8075\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41503967230683186\n",
      "0.7696433352701693\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 23/35\n",
      "total epoch trained: 60\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4189 - acc: 0.8075\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4150434302277937\n",
      "0.7690294010105221\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 24/35\n",
      "total epoch trained: 61\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4189 - acc: 0.8074\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41523427722388484\n",
      "0.7693388892995423\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 25/35\n",
      "total epoch trained: 62\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4188 - acc: 0.8075\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41543877439160265\n",
      "0.7694574626227624\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 26/35\n",
      "total epoch trained: 63\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4190 - acc: 0.8072\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4149909769496251\n",
      "0.7695525855529515\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 27/35\n",
      "total epoch trained: 64\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4189 - acc: 0.8076\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.415365695429924\n",
      "0.7689263636143997\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 28/35\n",
      "total epoch trained: 65\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4187 - acc: 0.8076\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41568940322142933\n",
      "0.7693418941001523\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 29/35\n",
      "total epoch trained: 66\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4186 - acc: 0.8076\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4154391988789836\n",
      "0.7692786704789694\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 30/35\n",
      "total epoch trained: 67\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4187 - acc: 0.8077\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41482225269881196\n",
      "0.7698015823734536\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 31/35\n",
      "total epoch trained: 68\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4187 - acc: 0.8077\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4154468888385814\n",
      "0.7696632912976756\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 32/35\n",
      "total epoch trained: 69\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4185 - acc: 0.8076\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4151144306235215\n",
      "0.769234931174313\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 33/35\n",
      "total epoch trained: 70\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4185 - acc: 0.8079\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.41557405222024735\n",
      "0.7693001819680085\n",
      "best logloss is: 0.41476793023364594\n",
      "remainning trial is: 34/35\n",
      "total epoch trained: 71\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 15s 18us/step - loss: 0.4185 - acc: 0.8076\n",
      "137376/137376 [==============================] - 1s 5us/step\n",
      "0.4153573333732355\n",
      "0.7691440002479955\n",
      "824250/824250 [==============================] - 4s 5us/step\n",
      "training loss is: 0.4109992841045208\n"
     ]
    }
   ],
   "source": [
    "model,t,v = train_each_fold(train_fold_dict[0],val_fold_dict[0],train_fold_y[0],val_fold_y[0],non_doc_col,doc_col,numu_col=numu_col,tolerance=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:49:31.078674Z",
     "start_time": "2018-10-02T17:40:36.610Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_nn_model(non_doc_col,doc_col,numu_cols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:49:31.080653Z",
     "start_time": "2018-10-02T17:40:36.613Z"
    }
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:49:31.082488Z",
     "start_time": "2018-10-02T17:40:36.615Z"
    }
   },
   "outputs": [],
   "source": [
    "input_val_dict = val_fold_dict[0]\n",
    "y_val = val_fold_y[0]\n",
    "preds = model.predict(input_val_dict,5000,verbose=1)\n",
    "logloss = log_loss(y_val,preds)\n",
    "roc = roc_auc_score(y_val,preds)\n",
    "print(logloss)\n",
    "print(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:49:31.084329Z",
     "start_time": "2018-10-02T17:40:36.618Z"
    }
   },
   "outputs": [],
   "source": [
    "# predsub = model.predict(test_input_dict,5000,verbose=1)\n",
    "# test_save = test[['instance_id']].copy()\n",
    "# test_save['predicted_score'] = predsub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:49:31.086188Z",
     "start_time": "2018-10-02T17:40:36.622Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_save.to_csv('ebd_nn.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:49:31.087915Z",
     "start_time": "2018-10-02T17:40:36.627Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_nn_model(non_doc_col,doc_col,numu_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:49:31.089684Z",
     "start_time": "2018-10-02T17:40:36.630Z"
    }
   },
   "outputs": [],
   "source": [
    "numu_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:49:31.091382Z",
     "start_time": "2018-10-02T17:40:36.633Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T17:49:31.092544Z",
     "start_time": "2018-10-02T17:40:36.636Z"
    }
   },
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "# plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
