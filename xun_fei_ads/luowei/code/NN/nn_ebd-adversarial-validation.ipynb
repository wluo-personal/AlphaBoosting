{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T08:15:56.842857Z",
     "start_time": "2018-10-05T08:15:55.819003Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "__file__=''\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../LIB/'))\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../../../../automl/automl_libs/'))\n",
    "from env import FILE\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csc_matrix, csr_matrix, hstack\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU,CuDNNGRU,Flatten,BatchNormalization,CuDNNLSTM,Activation,BatchNormalization\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import log_loss,roc_auc_score\n",
    "from keras.regularizers import l1\n",
    "from keras.regularizers import l2\n",
    "from keras.regularizers import l1_l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import re\n",
    "from keras import backend as K\n",
    "import time\n",
    "import gc\n",
    "# To get learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T08:15:56.859544Z",
     "start_time": "2018-10-05T08:15:56.844494Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nn_model(cols,doc_cols=[],numu_cols=[]):\n",
    "    \"\"\"\n",
    "    cols, used to do ebd and dense layers\n",
    "    doc_cols: used to do rnn\n",
    "    there can be overlaps\n",
    "    \"\"\"\n",
    "    input_list = []\n",
    "    concat_list = []\n",
    "    numu_list = []\n",
    "    for col in cols:\n",
    "        max_feature = len(info_dict[prefix_input_nonDoc+col]['tok'].index_word)\n",
    "        embed_size = int(np.log2(max_feature)/np.log2(1.5))\n",
    "        if embed_size< 2:\n",
    "            embed_size = 2\n",
    "        cur_input = Input(shape=(1, ),name = prefix_input_nonDoc+col)\n",
    "        \n",
    "       \n",
    "        embed_layer = Embedding(max_feature,\n",
    "                            embed_size,\n",
    "                            input_length=1,\n",
    "                            trainable=True,\n",
    "                            embeddings_regularizer=l2(0.0005),\n",
    "                            name='ebd_'+col)(cur_input)\n",
    "        embed_layer = SpatialDropout1D(0.5)(embed_layer)\n",
    "        x = Flatten()(embed_layer)\n",
    "        input_list.append(cur_input)\n",
    "        concat_list.append(x)\n",
    "    for col in doc_cols:\n",
    "        max_feature = len(info_dict[prefix_input_Doc+col]['tok'].index_word)\n",
    "        embed_size = int(np.log2(max_feature)/np.log2(1.5))\n",
    "        if embed_size< 2:\n",
    "            embed_size = 2\n",
    "        input_shape = sequence_size_dict[col]\n",
    "        cur_input = Input(shape=(input_shape, ),name = prefix_input_Doc+col)\n",
    "        embed_layer = Embedding(max_feature,\n",
    "                            embed_size,\n",
    "                            input_length=input_shape,\n",
    "                            trainable=True,\n",
    "                            embeddings_regularizer=l2(0.0005),\n",
    "                            name='ebd_rnn_'+col)(cur_input)\n",
    "        x = SpatialDropout1D(0.5)(embed_layer)\n",
    "        x = Bidirectional(CuDNNGRU(25, return_sequences=True))(x)\n",
    "        x = Conv1D(25, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "        x_aveP = GlobalAveragePooling1D()(x)\n",
    "        x_maxP = GlobalMaxPooling1D()(x)\n",
    "        x = concatenate([x_aveP,x_maxP])\n",
    "        input_list.append(cur_input)\n",
    "        concat_list.append(x)\n",
    "\n",
    "    if len(numu_cols) > 0:\n",
    "        print('add numu...')\n",
    "        nu_shape = len(numu_cols)\n",
    "        cur_input = Input(shape=(nu_shape, ),name = prefix_input_nu)\n",
    "        x = BatchNormalization()(cur_input)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        input_list.append(cur_input)\n",
    "        numu_list.append(x)\n",
    "       \n",
    "    if len(concat_list) > 1:\n",
    "        x = concatenate(concat_list)\n",
    "#     x = BatchNormalization()(x)\n",
    "    \n",
    "\n",
    "    if len(numu_list)>0:\n",
    "        x = concatenate([x]+numu_list)\n",
    "#         x = BatchNormalization()(x)\n",
    "        \n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    preds = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(input_list, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])    \n",
    "    return model\n",
    "\n",
    "def train_each_fold(input_train_dict,input_val_dict,y_train,y_val,cols,doc_col=[],tolerance=30,train_batch_size=5000):\n",
    "    model = get_nn_model(cols,doc_col)\n",
    "    cur_to = 0\n",
    "    best_roc = None\n",
    "    best_weights = None\n",
    "    count = 0\n",
    "    base_lr = 0.001\n",
    "    while True:\n",
    "        model.fit(input_train_dict, y_train, \n",
    "                  batch_size=train_batch_size, \n",
    "                  epochs=1,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  )\n",
    "        preds = model.predict(input_val_dict,5000,verbose=1)\n",
    "        roc = roc_auc_score(y_val,preds)\n",
    "        print(roc)\n",
    "        if best_roc is None:\n",
    "            best_roc = roc\n",
    "            best_weights = model.get_weights()\n",
    "        else:\n",
    "            if best_roc < roc:\n",
    "                best_roc = roc\n",
    "                best_weights = model.get_weights()\n",
    "                cur_to = 0\n",
    "            else:\n",
    "                cur_to +=1\n",
    "        if cur_to == tolerance:\n",
    "            break\n",
    "        print('best roc is: {}'.format(best_roc))\n",
    "        print('remainning trial is: {}/{}'.format(cur_to,tolerance))\n",
    "        print('total epoch trained: {}'.format(count))\n",
    "        count+=1\n",
    "    model.set_weights(best_weights)\n",
    "    return model\n",
    "    \n",
    "\n",
    "def nn_K_fold(train_fold_dict,\n",
    "              val_fold_dict,\n",
    "              train_fold_y,\n",
    "              val_fold_y,\n",
    "              val_index_list,\n",
    "              train_df,\n",
    "              pred_col_name = 'predicted_score',\n",
    "              nondoc_cols=[],\n",
    "              doc_cols=[],\n",
    "              tolerance=30,\n",
    "              train_batch_size=5000,\n",
    "              preds_batch=5000):\n",
    "    \"\"\"\n",
    "    train_fold_dict: format, key - foldNum, value - nn input \n",
    "    train_fold_y: label for train fold, fotmat, key -foldNum, value - label\n",
    "    val_fold_dict: format, key - foldNum, value - nn input \n",
    "    val_fold_y: label for val fold, fotmat, key -foldNum, value - label\n",
    "    test_input_dict: format, key - foldNum, value - nn input \n",
    "    holdout_input_dict: Noneable, if none, not using holdout\n",
    "    holdout_y: Noneable, label for holdout\n",
    "    cols: all cols used to do ebd\n",
    "    doc_cols, cols that are used to do rnn\n",
    "    val_index_list: cv, going to predict and generate oof\n",
    "    holdout_index_list: going to predict on each fold\n",
    "    train_df: 'dataframe which only has id columns, which will be used to store oof prediction'\n",
    "    test_df: 'dataframe which only has id columns, which will be used to store test prediction'\n",
    "    \"\"\"\n",
    "    train_df = train_df.copy()\n",
    "    n_fold = len(train_fold_dict)\n",
    "    val_score_list = []\n",
    "    train_score_list = []\n",
    "    cv_score_list = []\n",
    "    train_df[pred_col_name] = np.nan\n",
    "    \n",
    "    for fold in range(n_fold):\n",
    "        print('start fold {}...'.format(fold))\n",
    "        model = train_each_fold(train_fold_dict[fold],\n",
    "                                val_fold_dict[fold],\n",
    "                                train_fold_y[fold],\n",
    "                                val_fold_y[fold],\n",
    "                                nondoc_cols,\n",
    "                                doc_cols,\n",
    "                                tolerance=tolerance,\n",
    "                                train_batch_size=train_batch_size)\n",
    "        train_preds =  model.predict(train_fold_dict[fold],preds_batch,verbose=1)\n",
    "        val_preds = model.predict(val_fold_dict[fold],preds_batch,verbose=1)\n",
    "        train_roc = roc_auc_score(train_fold_y[fold],train_preds)\n",
    "        val_roc = roc_auc_score(val_fold_y[fold],val_preds)\n",
    "        train_df.loc[val_index_list[fold],pred_col_name] = val_preds\n",
    "        val_score_list.append(val_roc)\n",
    "        train_score_list.append(train_roc)\n",
    "        print('Fold {} finish! val roc: {}.'.format(fold,val_roc))\n",
    "        del model\n",
    "        gc.collect()\n",
    "        time.sleep(5)\n",
    "            \n",
    "    print('finish training... calculating evl matrix')\n",
    "    cv_score_mean = np.mean(val_score_list)\n",
    "    train_score_mean = np.mean(train_score_list)\n",
    "    print('cv mean is: {}'.format(cv_score_mean))\n",
    "    return train_df,cv_score_mean,train_score_mean\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T08:20:58.369379Z",
     "start_time": "2018-10-05T08:15:56.860922Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: (1001650, 35)\n",
      "loading old index successfully!\n",
      "test shape is: (40024, 35)\n",
      "(84079, 38)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:03<00:00,  9.37it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 76.89it/s]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preparing folds\n",
      "start tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:22<00:00,  1.29it/s]\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fold 0...\n",
      "Epoch 1/1\n",
      "56052/56052 [==============================] - 8s 151us/step - loss: 0.8191 - acc: 0.5577\n",
      "28027/28027 [==============================] - 1s 18us/step\n",
      "0.6021888096537047\n",
      "best roc is: 0.6021888096537047\n",
      "remainning trial is: 0/10\n",
      "total epoch trained: 0\n",
      "Epoch 1/1\n",
      "56052/56052 [==============================] - 6s 106us/step - loss: 0.6722 - acc: 0.5886\n",
      "28027/28027 [==============================] - 0s 8us/step\n",
      "0.6100344709544516\n",
      "best roc is: 0.6100344709544516\n",
      "remainning trial is: 0/10\n",
      "total epoch trained: 1\n",
      "Epoch 1/1\n",
      "56052/56052 [==============================] - 6s 106us/step - loss: 0.6400 - acc: 0.6780\n",
      "28027/28027 [==============================] - 0s 8us/step\n",
      "0.6117695944010244\n",
      "best roc is: 0.6117695944010244\n",
      "remainning trial is: 0/10\n",
      "total epoch trained: 2\n",
      "Epoch 1/1\n",
      "56052/56052 [==============================] - 6s 106us/step - loss: 0.2776 - acc: 0.9420\n",
      "28027/28027 [==============================] - 0s 8us/step\n",
      "0.5874046655169542\n",
      "best roc is: 0.6117695944010244\n",
      "remainning trial is: 1/10\n",
      "total epoch trained: 3\n",
      "Epoch 1/1\n",
      "56052/56052 [==============================] - 6s 107us/step - loss: 0.2061 - acc: 0.9700\n",
      "28027/28027 [==============================] - 0s 8us/step\n",
      "0.5905498147348248\n",
      "best roc is: 0.6117695944010244\n",
      "remainning trial is: 2/10\n",
      "total epoch trained: 4\n",
      "Epoch 1/1\n",
      "56052/56052 [==============================] - 6s 107us/step - loss: 0.1901 - acc: 0.9713\n",
      "28027/28027 [==============================] - 0s 8us/step\n",
      "0.5864813815861365\n",
      "best roc is: 0.6117695944010244\n",
      "remainning trial is: 3/10\n",
      "total epoch trained: 5\n",
      "Epoch 1/1\n",
      "56052/56052 [==============================] - 6s 106us/step - loss: 0.1739 - acc: 0.9728\n",
      "28027/28027 [==============================] - 0s 7us/step\n",
      "0.5825128528560624\n",
      "best roc is: 0.6117695944010244\n",
      "remainning trial is: 4/10\n",
      "total epoch trained: 6\n",
      "Epoch 1/1\n",
      "56052/56052 [==============================] - 6s 106us/step - loss: 0.1526 - acc: 0.9754\n",
      "28027/28027 [==============================] - 0s 8us/step\n",
      "0.5807932810986445\n",
      "best roc is: 0.6117695944010244\n",
      "remainning trial is: 5/10\n",
      "total epoch trained: 7\n",
      "Epoch 1/1\n",
      "56052/56052 [==============================] - 6s 106us/step - loss: 0.1377 - acc: 0.9765\n",
      "28027/28027 [==============================] - 0s 8us/step\n",
      "0.5810885360674908\n",
      "best roc is: 0.6117695944010244\n",
      "remainning trial is: 6/10\n",
      "total epoch trained: 8\n",
      "Epoch 1/1\n",
      "56052/56052 [==============================] - 6s 105us/step - loss: 0.1288 - acc: 0.9776\n",
      "28027/28027 [==============================] - 0s 8us/step\n",
      "0.586255716215512\n",
      "best roc is: 0.6117695944010244\n",
      "remainning trial is: 7/10\n",
      "total epoch trained: 9\n",
      "Epoch 1/1\n",
      "56052/56052 [==============================] - 6s 106us/step - loss: 0.1221 - acc: 0.9785\n",
      "28027/28027 [==============================] - 0s 8us/step\n",
      "0.5836543886922938\n",
      "best roc is: 0.6117695944010244\n",
      "remainning trial is: 8/10\n",
      "total epoch trained: 10\n",
      "Epoch 1/1\n",
      "56052/56052 [==============================] - 6s 106us/step - loss: 0.1165 - acc: 0.9795\n",
      "28027/28027 [==============================] - 0s 8us/step\n",
      "0.5826924832872933\n",
      "best roc is: 0.6117695944010244\n",
      "remainning trial is: 9/10\n",
      "total epoch trained: 11\n",
      "Epoch 1/1\n",
      "56052/56052 [==============================] - 6s 106us/step - loss: 0.1135 - acc: 0.9812\n",
      "28027/28027 [==============================] - 0s 8us/step\n",
      "0.5818480449403496\n",
      "56052/56052 [==============================] - 0s 8us/step\n",
      "28027/28027 [==============================] - 0s 8us/step\n",
      "Fold 0 finish! val roc: 0.6117695944010244.\n",
      "start fold 1...\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 8s 139us/step - loss: 0.8183 - acc: 0.5625\n",
      "28026/28026 [==============================] - 1s 25us/step\n",
      "0.5948053643414484\n",
      "best roc is: 0.5948053643414484\n",
      "remainning trial is: 0/10\n",
      "total epoch trained: 0\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 108us/step - loss: 0.6706 - acc: 0.5892\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.6086627334328726\n",
      "best roc is: 0.6086627334328726\n",
      "remainning trial is: 0/10\n",
      "total epoch trained: 1\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 106us/step - loss: 0.6350 - acc: 0.6791\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.6017510003249663\n",
      "best roc is: 0.6086627334328726\n",
      "remainning trial is: 1/10\n",
      "total epoch trained: 2\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 109us/step - loss: 0.2765 - acc: 0.9436\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.5900536302963896\n",
      "best roc is: 0.6086627334328726\n",
      "remainning trial is: 2/10\n",
      "total epoch trained: 3\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 108us/step - loss: 0.2036 - acc: 0.9710\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.5901789285256993\n",
      "best roc is: 0.6086627334328726\n",
      "remainning trial is: 3/10\n",
      "total epoch trained: 4\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 108us/step - loss: 0.1859 - acc: 0.9730\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.5863129339036591\n",
      "best roc is: 0.6086627334328726\n",
      "remainning trial is: 4/10\n",
      "total epoch trained: 5\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 108us/step - loss: 0.1711 - acc: 0.9741\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.5835795260421887\n",
      "best roc is: 0.6086627334328726\n",
      "remainning trial is: 5/10\n",
      "total epoch trained: 6\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 108us/step - loss: 0.1540 - acc: 0.9760\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.5817685704060309\n",
      "best roc is: 0.6086627334328726\n",
      "remainning trial is: 6/10\n",
      "total epoch trained: 7\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 109us/step - loss: 0.1355 - acc: 0.9782\n",
      "28026/28026 [==============================] - 0s 7us/step\n",
      "0.5845552009841533\n",
      "best roc is: 0.6086627334328726\n",
      "remainning trial is: 7/10\n",
      "total epoch trained: 8\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 108us/step - loss: 0.1248 - acc: 0.9789\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.5829440742665919\n",
      "best roc is: 0.6086627334328726\n",
      "remainning trial is: 8/10\n",
      "total epoch trained: 9\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 107us/step - loss: 0.1194 - acc: 0.9794\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.5806379079730891\n",
      "best roc is: 0.6086627334328726\n",
      "remainning trial is: 9/10\n",
      "total epoch trained: 10\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 107us/step - loss: 0.1121 - acc: 0.9805\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.5834924642538917\n",
      "56053/56053 [==============================] - 0s 8us/step\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "Fold 1 finish! val roc: 0.6086627334328726.\n",
      "start fold 2...\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 8s 147us/step - loss: 0.8184 - acc: 0.5645\n",
      "28026/28026 [==============================] - 1s 32us/step\n",
      "0.5967984956147662\n",
      "best roc is: 0.5967984956147662\n",
      "remainning trial is: 0/10\n",
      "total epoch trained: 0\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 108us/step - loss: 0.6732 - acc: 0.5899\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.5990562576671632\n",
      "best roc is: 0.5990562576671632\n",
      "remainning trial is: 0/10\n",
      "total epoch trained: 1\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 107us/step - loss: 0.6229 - acc: 0.7030\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.5945239505670348\n",
      "best roc is: 0.5990562576671632\n",
      "remainning trial is: 1/10\n",
      "total epoch trained: 2\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 109us/step - loss: 0.2688 - acc: 0.9483\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.5824869494728988\n",
      "best roc is: 0.5990562576671632\n",
      "remainning trial is: 2/10\n",
      "total epoch trained: 3\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 109us/step - loss: 0.1996 - acc: 0.9716\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.5793483557985823\n",
      "best roc is: 0.5990562576671632\n",
      "remainning trial is: 3/10\n",
      "total epoch trained: 4\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 108us/step - loss: 0.1881 - acc: 0.9728\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.5793654756788595\n",
      "best roc is: 0.5990562576671632\n",
      "remainning trial is: 4/10\n",
      "total epoch trained: 5\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56053/56053 [==============================] - 6s 107us/step - loss: 0.1703 - acc: 0.9745\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.5754411846487555\n",
      "best roc is: 0.5990562576671632\n",
      "remainning trial is: 5/10\n",
      "total epoch trained: 6\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 109us/step - loss: 0.1479 - acc: 0.9764\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.5752793420596232\n",
      "best roc is: 0.5990562576671632\n",
      "remainning trial is: 6/10\n",
      "total epoch trained: 7\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 109us/step - loss: 0.1302 - acc: 0.9776\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.571029273081155\n",
      "best roc is: 0.5990562576671632\n",
      "remainning trial is: 7/10\n",
      "total epoch trained: 8\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 108us/step - loss: 0.1240 - acc: 0.9778\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.5737414597433851\n",
      "best roc is: 0.5990562576671632\n",
      "remainning trial is: 8/10\n",
      "total epoch trained: 9\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 108us/step - loss: 0.1158 - acc: 0.9791\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.5734773547089893\n",
      "best roc is: 0.5990562576671632\n",
      "remainning trial is: 9/10\n",
      "total epoch trained: 10\n",
      "Epoch 1/1\n",
      "56053/56053 [==============================] - 6s 108us/step - loss: 0.1120 - acc: 0.9781\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "0.5787320707345065\n",
      "56053/56053 [==============================] - 0s 8us/step\n",
      "28026/28026 [==============================] - 0s 8us/step\n",
      "Fold 2 finish! val roc: 0.5990562576671632.\n",
      "finish training... calculating evl matrix\n",
      "cv mean is: 0.6064961951670201\n"
     ]
    }
   ],
   "source": [
    "init = True\n",
    "select_size = 40024\n",
    "index_col = 'ori_index'\n",
    "target_col = 'target'\n",
    "ignore_columns = ['instance_id','time','click','ori_index','target'] + ['creative_is_js', 'creative_is_voicead', 'app_paid']\n",
    "while True:\n",
    "    if init:\n",
    "        train = pd.read_pickle(FILE.train_ori.value)\n",
    "        print('train shape is: {}'.format(train.shape))\n",
    "        train['ori_index'] = train.index\n",
    "        try:\n",
    "            selected_index = pickle.load(open('../../data/original/adversarial_ho_index.pickle','rb'))\n",
    "            train = train.loc[selected_index].copy()\n",
    "            print('loading old index successfully!')\n",
    "        except:\n",
    "            print('no old index found!')\n",
    "        train[target_col] = 0\n",
    "        test = pd.read_pickle(FILE.test_ori.value)\n",
    "        test[target_col] = 1\n",
    "        print('test shape is: {}'.format(test.shape))\n",
    "        train_length = len(train)\n",
    "        X_shiyi = pd.read_pickle(FILE.shiyi_fillna_ori.value)\n",
    "        X = pd.concat([train,test],sort=False)\n",
    "        init = False  \n",
    "    X = pd.concat([train,test],sort=False)\n",
    "    X.reset_index(inplace=True)\n",
    "    print(X.shape)\n",
    "    X = X.merge(X_shiyi[['time_hour','instance_id']],how='inner',on='instance_id')\n",
    "    need_process_col = list(set(X.columns) - set(ignore_columns))\n",
    "    doc_col = ['user_tags','model']\n",
    "    non_doc_col = [f for f in need_process_col if f not in doc_col]\n",
    "    X_ = X[need_process_col].copy()\n",
    "    X_doc = X[doc_col].copy()\n",
    "    for col in tqdm(non_doc_col):\n",
    "        X_[col] = le.fit_transform(X_[col].astype(str))\n",
    "        X_[col] = col + '_'+X_[col].astype(str)\n",
    "    \n",
    "    for col in tqdm(doc_col):\n",
    "        X_doc[col] = X_doc[col].astype(str)\n",
    "        \n",
    "    print('start preparing folds')\n",
    "    num_folds = 3\n",
    "    train_index_list = []\n",
    "    val_index_list = []\n",
    "    folds = StratifiedKFold(n_splits= num_folds, shuffle=True)\n",
    "    for t,v in folds.split(X,X[target_col]):\n",
    "        train_index_list.append(X.iloc[t].index)\n",
    "        val_index_list.append(X.iloc[v].index)\n",
    "    train_fold_y = {}\n",
    "    val_fold_y = {}\n",
    "\n",
    "    for fold in range(num_folds):\n",
    "        train_fold_y[fold] = X.loc[train_index_list[fold],target_col].values\n",
    "        val_fold_y[fold] = X.loc[val_index_list[fold],target_col].values\n",
    "        \n",
    "    print('start tokenizer')\n",
    "    info_dict = {}\n",
    "    train_all_dict = {}\n",
    "    train_fold_dict = {}\n",
    "    val_fold_dict = {}\n",
    "    maxlen = 1\n",
    "    prefix_input_nonDoc = 'input_'\n",
    "    prefix_input_Doc = 'input_rnn_'\n",
    "    for col in tqdm(non_doc_col):\n",
    "        maxlen = 1\n",
    "        tok=text.Tokenizer(num_words=X_[col].nunique(),lower=False,filters='@')\n",
    "        tok.fit_on_texts(list(X_[col]))\n",
    "        info_dict.update({prefix_input_nonDoc+col:{'tok':tok}})\n",
    "        t = tok.texts_to_sequences(list(X_[col].values))\n",
    "        train_all_dict[prefix_input_nonDoc+col] = sequence.pad_sequences(t,maxlen=maxlen)\n",
    "\n",
    "\n",
    "        for fold in range(num_folds):\n",
    "            if train_fold_dict.get(fold) is None:\n",
    "                train_fold_dict[fold] = {}\n",
    "                val_fold_dict[fold] = {}\n",
    "            train_fold_dict[fold].update({prefix_input_nonDoc+col: train_all_dict[prefix_input_nonDoc+col][list(train_index_list[fold])]})\n",
    "            val_fold_dict[fold].update({prefix_input_nonDoc+col:train_all_dict[prefix_input_nonDoc+col][list(val_index_list[fold])]})\n",
    "\n",
    "    sequence_size_dict = {}\n",
    "    \n",
    "    for col in tqdm(doc_col):\n",
    "        if col == 'user_tags':\n",
    "            maxlen = 50\n",
    "            tok=text.Tokenizer(num_words=X_doc[col].nunique(),lower=False, filters=',')\n",
    "\n",
    "        elif col == 'model':\n",
    "            maxlen = 15\n",
    "            tok=text.Tokenizer(num_words=X_doc[col].nunique(),lower=False)\n",
    "        else:\n",
    "            maxlen = 15\n",
    "            tok=text.Tokenizer(num_words=X_doc[col].nunique(),lower=False)\n",
    "        tok.fit_on_texts(list(X_doc[col]))\n",
    "        info_dict.update({prefix_input_Doc+col:{'tok':tok}})\n",
    "        sequence_size_dict[col] = maxlen\n",
    "        t = tok.texts_to_sequences(list(X_doc[col].values))\n",
    "        train_all_dict[prefix_input_Doc+col] = sequence.pad_sequences(t,maxlen=maxlen)\n",
    "\n",
    "        for fold in range(num_folds):\n",
    "            if train_fold_dict.get(fold) is None:\n",
    "                train_fold_dict[fold] = {}\n",
    "                val_fold_dict[fold] = {}\n",
    "            train_fold_dict[fold].update({prefix_input_Doc+col: train_all_dict[prefix_input_Doc+col][list(train_index_list[fold])]})\n",
    "            val_fold_dict[fold].update({prefix_input_Doc+col:train_all_dict[prefix_input_Doc+col][list(val_index_list[fold])]})\n",
    "            \n",
    "    train_df = X[[target_col,index_col]].copy()\n",
    "    train_batch_size = int(len(train) * 0.6 / 120)\n",
    "    if train_batch_size > 5000:\n",
    "        train_batch_size=5000\n",
    "    if train_batch_size < 500:\n",
    "        train_batch_size=500\n",
    "    \n",
    "    train_save,cv_,ta_ = nn_K_fold(train_fold_dict,\n",
    "                                          val_fold_dict,\n",
    "                                          train_fold_y,\n",
    "                                          val_fold_y,\n",
    "                                          val_index_list,\n",
    "                                          train_df,\n",
    "                                          pred_col_name = 'predicted_score',\n",
    "                                          nondoc_cols=non_doc_col,\n",
    "                                          doc_cols=doc_col,\n",
    "                                          tolerance=10,\n",
    "                                           train_batch_size=train_batch_size,\n",
    "                                          preds_batch=5000)\n",
    "    \n",
    "    \n",
    "    \n",
    "    real_train = train_save[train_save[target_col]==0].copy()\n",
    "    real_train_length = len(real_train)\n",
    "    ratio = 0.2\n",
    "    real_train = real_train.sort_values('predicted_score').iloc[min(int(real_train_length*ratio),real_train_length-select_size):]\n",
    "    selected_index = list(real_train[index_col].astype(int).values)\n",
    "    pickle.dump(selected_index,open('../../data/original/adversarial_ho_index.pickle','wb'))\n",
    "    train = train.loc[selected_index].copy()\n",
    "    if len(train) == select_size:\n",
    "        print('holdout selected! Done!')\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T08:20:58.373202Z",
     "start_time": "2018-10-05T08:20:58.371173Z"
    }
   },
   "outputs": [],
   "source": [
    "# ss = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T08:25:29.855633Z",
     "start_time": "2018-10-05T08:25:29.051097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961626\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_pickle(FILE.train_ori.value)\n",
    "selected_index = pickle.load(open('../../data/original/adversarial_ho_index.pickle','rb'))\n",
    "train_index = list(set(train.index)-set(selected_index))\n",
    "print(len(train_index))\n",
    "pickle.dump(train_index,open('../../data/original/adversarial_train_index.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T08:46:32.587260Z",
     "start_time": "2018-10-05T08:46:32.422249Z"
    }
   },
   "outputs": [],
   "source": [
    "__file__=''\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../LIB/'))\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../../../../automl/automl_libs/'))\n",
    "from env import FILE\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T08:46:33.612572Z",
     "start_time": "2018-10-05T08:46:32.810907Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle(FILE.train_ori.value)\n",
    "selected_index = pickle.load(open('../../data/original/adversarial_ho_index.pickle','rb'))\n",
    "ho = train.loc[selected_index].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T08:47:29.130635Z",
     "start_time": "2018-10-05T08:47:29.116121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3330501698980612"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(ho['user_tags'].isnull())/len(ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T08:32:50.544086Z",
     "start_time": "2018-10-05T08:32:50.534331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97005            HLTEM800\n",
       "137567             iPhone\n",
       "746511           Redmi-3S\n",
       "387509           OPPO A73\n",
       "591587           PRA-AL00\n",
       "236263          vivo Y55A\n",
       "315273           vivo-Y51\n",
       "484878          YU-FLY-F9\n",
       "605305          vivo-Y85A\n",
       "306155           OPPO A33\n",
       "825854           OPPO-R9s\n",
       "13906           OPPO-A37m\n",
       "433249           vivo-Y35\n",
       "259827            Le-X528\n",
       "38144       vivo-X9s-Plus\n",
       "609986            MI-4LTE\n",
       "206535           BAC-AL00\n",
       "161984           vivo Y31\n",
       "731138           CAM-AL00\n",
       "936239           PLE-703L\n",
       "371638            MI 4LTE\n",
       "153575         KINGSUN-F9\n",
       "496798                U20\n",
       "136417          iPhone7,2\n",
       "37745                MI-8\n",
       "634704          SUGAR+F11\n",
       "65923           boway-U15\n",
       "169856           vivo-Y51\n",
       "603516          OPPO R7sm\n",
       "444599           vivoX20A\n",
       "               ...       \n",
       "37067              M651CY\n",
       "20931             vivo X9\n",
       "9338             GT-I9508\n",
       "26137            COL-AL10\n",
       "17804                  15\n",
       "1925       iPhone 6s Plus\n",
       "2837            vivo Y66L\n",
       "27851           M6%20Note\n",
       "15228             m3 note\n",
       "17541               MI+5X\n",
       "24931            EML-AL00\n",
       "34333            iPhone 6\n",
       "23088            LLD-AL00\n",
       "29163          MI 5s Plus\n",
       "12208            vivo Y85\n",
       "37505         OPPO%20A37t\n",
       "2587        ONEPLUS A5010\n",
       "992       iPhone 5c (GSM)\n",
       "11558            OPPO+A33\n",
       "9292           GIONEE M7L\n",
       "9999            OPPO+A59m\n",
       "5329             DLI-AL10\n",
       "38427           OPPO A33m\n",
       "15462       iPhone 7 Plus\n",
       "32601            OPPO R9m\n",
       "19058           Mi Note 3\n",
       "18561          HM+NOTE+1S\n",
       "9998      HUAWEI TIT-AL00\n",
       "39661       Redmi Note 4X\n",
       "37592               MI 5X\n",
       "Name: model, Length: 40024, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ho['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
