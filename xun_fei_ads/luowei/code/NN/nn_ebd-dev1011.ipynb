{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T04:33:19.815686Z",
     "start_time": "2018-10-11T04:33:18.867785Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "__file__=''\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../LIB/'))\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../../../../automl/automl_libs/'))\n",
    "from env import FILE\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csc_matrix, csr_matrix, hstack\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU,CuDNNGRU,Flatten,BatchNormalization,CuDNNLSTM,Activation,BatchNormalization,Lambda,Add,Multiply,Subtract\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D,Concatenate\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import log_loss,roc_auc_score\n",
    "from keras.regularizers import l1\n",
    "from keras.regularizers import l2\n",
    "from keras.regularizers import l1_l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import re\n",
    "from keras import backend as K\n",
    "from keras.backend import square\n",
    "import time\n",
    "import gc\n",
    "# To get learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T04:37:21.863643Z",
     "start_time": "2018-10-11T04:37:19.479672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: (1001650, 36)\n",
      "test shape is: (40024, 35)\n",
      "(1041674, 36)\n",
      "(1041674, 36)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_pickle(FILE.train_ori.value)\n",
    "# train = train.iloc[:200000].copy()\n",
    "print('train shape is: {}'.format(train.shape))\n",
    "test = pd.read_pickle(FILE.test_ori.value)\n",
    "print('test shape is: {}'.format(test.shape))\n",
    "train_length = len(train)\n",
    "\n",
    "\n",
    "X = pd.concat([train,test],sort=False)\n",
    "print(X.shape)\n",
    "X = X.reset_index(drop=True)\n",
    "\n",
    "# X_clean = pd.read_csv('../../data/original/cleaned_data_price_final.csv')\n",
    "# X_append = pd.read_pickle('../../data/nn_features/clean.pkl')\n",
    "\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "# assert np.sum(X_clean['instance_id'].values != X['instance_id'].values) == 0\n",
    "# assert np.sum(X_append['instance_id'].values != X['instance_id'].values) == 0\n",
    "# X['model'] = X_append['model_new'].values\n",
    "\n",
    "\n",
    "ignore_columns = ['instance_id','time','click'] + ['creative_is_js', 'creative_is_voicead', 'app_paid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PB 4236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T12:42:44.019816Z",
     "start_time": "2018-10-10T12:42:44.017736Z"
    }
   },
   "outputs": [],
   "source": [
    "# need_process_col = list(set(X.columns) - set(ignore_columns))\n",
    "# X_ = X[need_process_col].copy()\n",
    "\n",
    "\n",
    "# doc_col = ['user_tags','model']\n",
    "# non_doc_col = [f for f in need_process_col if f not in doc_col]\n",
    "# # doc_col = doc_col + []\n",
    "# # non_doc_col = non_doc_col+['user_tags']\n",
    "# counter = 0\n",
    "\n",
    "# X_doc = X[doc_col].copy()\n",
    "# for col in tqdm(non_doc_col):\n",
    "#     X_[col] = le.fit_transform(X_[col].astype(str))\n",
    "#     X_[col] = col + '_'+X_[col].astype(str)\n",
    "    \n",
    "# for col in tqdm(doc_col):\n",
    "#     X_doc[col] = X_doc[col].astype(str)\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T04:38:51.593508Z",
     "start_time": "2018-10-11T04:37:37.807364Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:13<00:00,  2.61s/it]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.40it/s]\n"
     ]
    }
   ],
   "source": [
    "need_process_col = list(set(X.columns) - set(ignore_columns))\n",
    "X_ = X[need_process_col].copy()\n",
    "\n",
    "\n",
    "doc_col = ['user_tags','model']\n",
    "non_doc_col = [f for f in need_process_col if f not in doc_col]\n",
    "# doc_col = doc_col + []\n",
    "# non_doc_col = non_doc_col+['user_tags']\n",
    "counter = 0\n",
    "\n",
    "X_doc = X[doc_col].copy()\n",
    "for col in tqdm(non_doc_col):\n",
    "#     col = 'creative_id'\n",
    "    test_values = set(X_[col].iloc[train_length:].astype(str).unique())\n",
    "    train_values = set(X_[col].iloc[:train_length].astype(str).unique())\n",
    "    intersection = train_values.intersection(test_values)\n",
    "    out_liyer = list(train_values.union(test_values) - intersection)\n",
    "    if len(out_liyer) > 0:\n",
    "#         print(len(out_liyer))\n",
    "        out_liyer_mapping = pd.Series(index=list(out_liyer),data=1)\n",
    "        filtered = (X_[col].astype(str).map(out_liyer_mapping) == 1)\n",
    "#         print('col:{}, size:{}'.format(col,np.sum(filtered)))\n",
    "        X_.loc[filtered,col] = np.nan\n",
    "        \n",
    "        \n",
    "    X_[col] = le.fit_transform(X_[col].astype(str))\n",
    "    X_[col] = col + '_'+X_[col].astype(str)\n",
    "    \n",
    "for col in tqdm(doc_col):\n",
    "    X_doc[col] = X_doc[col].astype(str)\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T04:39:13.131551Z",
     "start_time": "2018-10-11T04:39:13.121138Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_tok(X,col,train_length,d_filter='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
    "    X[col] = X[col].astype(str)\n",
    "\n",
    "    tok_all=text.Tokenizer(num_words=X[col].nunique(),lower=False,filters=d_filter)\n",
    "    tok_all.fit_on_texts(list(X[col].values))\n",
    "\n",
    "    tok_train=text.Tokenizer(num_words=X[col].iloc[:train_length].nunique(),lower=False,filters=d_filter)\n",
    "    tok_train.fit_on_texts(list(X[col].iloc[:train_length].values))\n",
    "\n",
    "    tok_test=text.Tokenizer(num_words=X[col].iloc[train_length:].nunique(),lower=False,filters=d_filter)\n",
    "    tok_test.fit_on_texts(list(X[col].iloc[train_length:].values))\n",
    "    word_intersection = set(tok_train.word_index.keys()).intersection(set(tok_test.word_index.keys()))\n",
    "\n",
    "    self_index = {}\n",
    "    count = 1\n",
    "    for word in word_intersection:\n",
    "        self_index[word] = count\n",
    "        count+=1\n",
    "    self_index['unknown'] = count\n",
    "    print('max index is: {}'.format(count))\n",
    "\n",
    "    for word in tok_all.word_index.keys():\n",
    "        tok_all.word_index[word] = self_index.get(word,count)\n",
    "    return tok_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T04:39:18.477564Z",
     "start_time": "2018-10-11T04:39:18.474071Z"
    }
   },
   "outputs": [],
   "source": [
    "train_index = train.index\n",
    "holdout_index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T04:39:31.019545Z",
     "start_time": "2018-10-11T04:39:27.651932Z"
    }
   },
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "seed = 1001\n",
    "train_index_list = []\n",
    "val_index_list = []\n",
    "folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=seed)\n",
    "for t,v in folds.split(train.loc[train_index],train.loc[train_index,'click']):\n",
    "    train_index_list.append(train.loc[train_index].iloc[t].index)\n",
    "    val_index_list.append(train.loc[train_index].iloc[v].index)\n",
    "    \n",
    "# check = []\n",
    "# for i in val_index_list:\n",
    "#     check.extend(list(i))\n",
    "# assert len(set(check+list(holdout_index))) == len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T04:39:44.561868Z",
     "start_time": "2018-10-11T04:39:44.258538Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fold_y = {}\n",
    "val_fold_y = {}\n",
    "if holdout_index is not None:\n",
    "    holdout_y = train.loc[holdout_index,'click'].values\n",
    "else:\n",
    "    holdout_y = None\n",
    "for fold in range(num_folds):\n",
    "    train_fold_y[fold] = train.loc[train_index_list[fold],'click'].values\n",
    "    val_fold_y[fold] = train.loc[val_index_list[fold],'click'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T04:45:22.790113Z",
     "start_time": "2018-10-11T04:39:54.097957Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [04:28<00:00,  9.59s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max index is: 1313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:00<00:00, 30.04s/it]\n"
     ]
    }
   ],
   "source": [
    "info_dict = {}\n",
    "train_all_dict = {}\n",
    "train_fold_dict = {}\n",
    "\n",
    "val_fold_dict = {}\n",
    "\n",
    "holdout_input_dict = {}\n",
    "test_input_dict = {}\n",
    "maxlen = 1\n",
    "prefix_input_nonDoc = 'input_'\n",
    "prefix_input_Doc = 'input_rnn_'\n",
    "for col in tqdm(non_doc_col):\n",
    "\n",
    "    maxlen = 1\n",
    "    tok=text.Tokenizer(num_words=X_[col].nunique(),lower=False,filters='@')\n",
    "    tok.fit_on_texts(list(X_[col]))\n",
    "    info_dict.update({prefix_input_nonDoc+col:{'tok':tok}})\n",
    "    t = tok.texts_to_sequences(list(X_[col].iloc[:train_length].values))\n",
    "    te = tok.texts_to_sequences(list(X_[col].iloc[train_length:].values))\n",
    "    train_all_dict[prefix_input_nonDoc+col] = sequence.pad_sequences(t,maxlen=maxlen)\n",
    "    test_input_dict[prefix_input_nonDoc+col] = sequence.pad_sequences(te,maxlen=maxlen)\n",
    "    if holdout_index is not None:\n",
    "        holdout_input_dict[prefix_input_nonDoc+col] = train_all_dict[prefix_input_nonDoc+col][list(holdout_index)]\n",
    "    \n",
    "    for fold in range(num_folds):\n",
    "        if train_fold_dict.get(fold) is None:\n",
    "            train_fold_dict[fold] = {}\n",
    "            val_fold_dict[fold] = {}\n",
    "        train_fold_dict[fold].update({prefix_input_nonDoc+col: train_all_dict[prefix_input_nonDoc+col][list(train_index_list[fold])]})\n",
    "        val_fold_dict[fold].update({prefix_input_nonDoc+col:train_all_dict[prefix_input_nonDoc+col][list(val_index_list[fold])]})\n",
    "        \n",
    "sequence_size_dict = {}\n",
    "for col in tqdm(doc_col):\n",
    "    if col == 'user_tags':\n",
    "        maxlen = 50\n",
    "        tok = get_tok(X_doc,col=col,train_length=train_length,d_filter=',')\n",
    "#         tok=text.Tokenizer(num_words=X_doc[col].nunique(),lower=False, filters=',')\n",
    "#         tok.fit_on_texts(list(X_doc[col]))\n",
    "\n",
    "        \n",
    "    elif col == 'model':\n",
    "        maxlen = 15\n",
    "#         tok = get_tok(X_doc,col=col,train_length=train_length)\n",
    "        tok=text.Tokenizer(num_words=X_doc[col].nunique(),lower=False)\n",
    "        tok.fit_on_texts(list(X_doc[col]))\n",
    "    else:\n",
    "        maxlen = 15\n",
    "#         tok = get_tok(X_doc,col=col,train_length=train_length)\n",
    "        tok=text.Tokenizer(num_words=X_doc[col].nunique(),lower=False)\n",
    "        tok.fit_on_texts(list(X_doc[col]))\n",
    "    info_dict.update({prefix_input_Doc+col:{'tok':tok}})\n",
    "    sequence_size_dict[col] = maxlen\n",
    "    t = tok.texts_to_sequences(list(X_doc[col].iloc[:train_length].values))\n",
    "    te = tok.texts_to_sequences(list(X_doc[col].iloc[train_length:].values))\n",
    "    train_all_dict[prefix_input_Doc+col] = sequence.pad_sequences(t,maxlen=maxlen)\n",
    "    test_input_dict[prefix_input_Doc+col] = sequence.pad_sequences(te,maxlen=maxlen)\n",
    "    if holdout_index is not None:\n",
    "        holdout_input_dict[prefix_input_Doc+col] = train_all_dict[prefix_input_Doc+col][list(holdout_index)]\n",
    "    \n",
    "    for fold in range(num_folds):\n",
    "        if train_fold_dict.get(fold) is None:\n",
    "            train_fold_dict[fold] = {}\n",
    "            val_fold_dict[fold] = {}\n",
    "        train_fold_dict[fold].update({prefix_input_Doc+col: train_all_dict[prefix_input_Doc+col][list(train_index_list[fold])]})\n",
    "        val_fold_dict[fold].update({prefix_input_Doc+col:train_all_dict[prefix_input_Doc+col][list(val_index_list[fold])]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build NN model only use model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.41479"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('get second order x')\n",
    "        ebd_dense_list = []\n",
    "        print('project each in concat_list to a dense layer respectively')\n",
    "        for each in concat_list:\n",
    "            x_e =  Dense(MLP_unit, activation='relu')(each)\n",
    "            x_e = Dropout(0.5)(x_e)\n",
    "            ebd_dense_list.append(x_e)\n",
    "        print('interaction with the dense layers')\n",
    "        x_add = Add()(ebd_dense_list)\n",
    "        x_inter1 = Multiply()([x_add,x_add])\n",
    "        multi_list = []\n",
    "        for each in ebd_dense_list:\n",
    "            multi_list.append(Multiply()([each,each]))\n",
    "        x_inter2 = Add()(multi_list)\n",
    "        x_second = Subtract()([x_inter1,x_inter2])\n",
    "        x_second = Dropout(0.5)(x_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T06:24:54.249523Z",
     "start_time": "2018-10-11T06:24:54.225082Z"
    }
   },
   "outputs": [],
   "source": [
    "def selfpool(x,dim=1):\n",
    "    if dim == 1:\n",
    "        def pol(x,n):\n",
    "            return x[:,n,:]\n",
    "    elif dim == 2:\n",
    "        def pol(x,n):\n",
    "            return x[:,:,n]\n",
    "    else:\n",
    "        raise ValueError('dim {} not support'.format(dim))\n",
    "    f_pol = Lambda(pol,arguments={'n'})\n",
    "    l = int(x.shape[dim])\n",
    "    inter_list = []\n",
    "    for index in range(l):\n",
    "        f_pol = Lambda(pol,arguments={'n':index})\n",
    "        inter_list.append(f_pol(x))\n",
    "    x_add = Add()(inter_list)\n",
    "    x_inter1 = Multiply()([x_add,x_add])\n",
    "    multi_list = []\n",
    "    for each in inter_list:\n",
    "        multi_list.append(Multiply()([each,each]))\n",
    "    x_inter2 = Add()(multi_list)\n",
    "    x_second = Subtract()([x_inter1,x_inter2])\n",
    "    return x_second\n",
    "    \n",
    "    \n",
    "\n",
    "def get_nn_model(cols,doc_cols=[],numu_cols=[]):\n",
    "    \"\"\"\n",
    "    cols, used to do ebd and dense layers\n",
    "    doc_cols: used to do rnn\n",
    "    there can be overlaps\n",
    "    \"\"\"\n",
    "    input_list = []\n",
    "    concat_list = []\n",
    "    numu_list = []\n",
    "    for col in cols:\n",
    "        max_feature = len(info_dict[prefix_input_nonDoc+col]['tok'].index_word)\n",
    "        embed_size = int(np.log2(max_feature)/np.log2(1.5))\n",
    "        if embed_size< 2:\n",
    "            embed_size = 2\n",
    "        cur_input = Input(shape=(1, ),name = prefix_input_nonDoc+col)\n",
    "        \n",
    "       \n",
    "        embed_layer = Embedding(max_feature,\n",
    "                            embed_size,\n",
    "                            input_length=1,\n",
    "                            trainable=True,\n",
    "                            embeddings_regularizer=l2(0.0005),\n",
    "                            name='ebd_'+col)(cur_input)\n",
    "        embed_layer = SpatialDropout1D(0.5)(embed_layer)\n",
    "        x = Flatten()(embed_layer)\n",
    "        input_list.append(cur_input)\n",
    "        concat_list.append(x)\n",
    "    for col in doc_cols:\n",
    "        max_feature = len(info_dict[prefix_input_Doc+col]['tok'].index_word)\n",
    "        embed_size = int(np.log2(max_feature)/np.log2(1.5))\n",
    "        if embed_size< 2:\n",
    "            embed_size = 2\n",
    "        input_shape = sequence_size_dict[col]\n",
    "        cur_input = Input(shape=(input_shape, ),name = prefix_input_Doc+col)\n",
    "        embed_layer = Embedding(max_feature,\n",
    "                            embed_size,\n",
    "                            input_length=input_shape,\n",
    "                            trainable=True,\n",
    "                            embeddings_regularizer=l2(0.0005),\n",
    "                            name='ebd_rnn_'+col)(cur_input)\n",
    "        x = SpatialDropout1D(0.5)(embed_layer)\n",
    "        if col == 'user_tags':\n",
    "            print('user_tags')\n",
    "            x = Bidirectional(CuDNNGRU(25, return_sequences=True))(x)\n",
    "            x = Conv1D(25, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "            x_aveP = GlobalAveragePooling1D()(x)\n",
    "            x_maxP = GlobalMaxPooling1D()(x)\n",
    "            x = concatenate([x_aveP,x_maxP])\n",
    "        else:\n",
    "#             x = selfpool(x,dim=2)\n",
    "#             print(x.shape)\n",
    "#             x = Bidirectional(CuDNNGRU(25, return_sequences=True))(x)\n",
    "#             x = Flatten()(x)\n",
    "            x = Conv1D(25, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "            x_aveP = GlobalAveragePooling1D()(x)\n",
    "            x_maxP = GlobalMaxPooling1D()(x)\n",
    "            x = concatenate([x_aveP,x_maxP])\n",
    "#             x = Dense(128, activation='relu')(x)\n",
    "#             x = Dropout(0.5)(x)\n",
    "#             x = Dense(64, activation='relu')(x)\n",
    "#             x = Dropout(0.3)(x)\n",
    "        input_list.append(cur_input)\n",
    "        concat_list.append(x)\n",
    "\n",
    "    if len(numu_cols) > 0:\n",
    "        print('add numu...')\n",
    "        nu_shape = len(numu_cols)\n",
    "        cur_input = Input(shape=(nu_shape, ),name = prefix_input_nu)\n",
    "        x = BatchNormalization()(cur_input)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        input_list.append(cur_input)\n",
    "        numu_list.append(x)\n",
    "       \n",
    "    if len(concat_list) > 1:\n",
    "        x = concatenate(concat_list)\n",
    "#     x = BatchNormalization()(x)\n",
    "    \n",
    "\n",
    "    if len(numu_list)>0:\n",
    "        x = concatenate([x]+numu_list)\n",
    "#         x = BatchNormalization()(x)\n",
    "        \n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "#     x1 = Dense(512, activation='relu')(x)\n",
    "#     x1 = Dropout(0.2)(x1)\n",
    "    \n",
    "#     x2 = concatenate([x,x1])\n",
    "#     x2_ = Dense(128, activation='relu')(x2)\n",
    "#     x2_ = Dropout(0.2)(x2_)\n",
    "    \n",
    "#     x3 = concatenate([x,x1,x2_])\n",
    "#     x3_ = Dense(64, activation='relu')(x3)\n",
    "#     x3_ = Dropout(0.2)(x3_)\n",
    "    \n",
    "#     x = concatenate([x,x1,x2_,x3_])\n",
    "#     x = Dropout(0.5)(x)\n",
    "\n",
    "\n",
    "    preds = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(input_list, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T06:33:33.189040Z",
     "start_time": "2018-10-11T06:24:57.449589Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_tags\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.5867 - acc: 0.7797\n",
      "200331/200331 [==============================] - 4s 18us/step\n",
      "0.4405876414155443\n",
      "0.7295258563222319\n",
      "best logloss is: 0.4405876414155443\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 0\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4458 - acc: 0.8015\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4262632521295027\n",
      "0.7521730182832231\n",
      "best logloss is: 0.4262632521295027\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 1\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4312 - acc: 0.8015\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.42305903859311544\n",
      "0.7574877957774959\n",
      "best logloss is: 0.42305903859311544\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 2\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4282 - acc: 0.8017\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.42156582668871134\n",
      "0.7602021260593359\n",
      "best logloss is: 0.42156582668871134\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 3\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4264 - acc: 0.8034\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4212303521359181\n",
      "0.7613525828948434\n",
      "best logloss is: 0.4212303521359181\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 4\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4251 - acc: 0.8044\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4195626577245648\n",
      "0.763027470312208\n",
      "best logloss is: 0.4195626577245648\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 5\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4241 - acc: 0.8047\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4201221572740685\n",
      "0.7633687727192504\n",
      "best logloss is: 0.4195626577245648\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 6\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4234 - acc: 0.8047\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41956684177807074\n",
      "0.7639505832322881\n",
      "best logloss is: 0.4195626577245648\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 7\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4229 - acc: 0.8049\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41919066212666267\n",
      "0.764201748425746\n",
      "best logloss is: 0.41919066212666267\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 8\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4224 - acc: 0.8051\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4185803862971403\n",
      "0.7647696041073154\n",
      "best logloss is: 0.4185803862971403\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 9\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4220 - acc: 0.8054\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41811813028053396\n",
      "0.7650240166066676\n",
      "best logloss is: 0.41811813028053396\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 10\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4217 - acc: 0.8053\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4180690259389309\n",
      "0.7654598137198911\n",
      "best logloss is: 0.4180690259389309\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 11\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4215 - acc: 0.8056\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41808270801716807\n",
      "0.76566295995113\n",
      "best logloss is: 0.4180690259389309\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 12\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4213 - acc: 0.8055\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4183626190569039\n",
      "0.7658979789181329\n",
      "best logloss is: 0.4180690259389309\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 13\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4211 - acc: 0.8057\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41760711935081646\n",
      "0.7660065738961148\n",
      "best logloss is: 0.41760711935081646\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 14\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4210 - acc: 0.8058\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4173254214141749\n",
      "0.7663047351136241\n",
      "best logloss is: 0.4173254214141749\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 15\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4209 - acc: 0.8058\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41685967993064904\n",
      "0.7666273852092662\n",
      "best logloss is: 0.41685967993064904\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 16\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4207 - acc: 0.8059\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4167332361006117\n",
      "0.7666199720599363\n",
      "best logloss is: 0.4167332361006117\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 17\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4204 - acc: 0.8058\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41765331649126886\n",
      "0.7668567818148722\n",
      "best logloss is: 0.4167332361006117\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 18\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4207 - acc: 0.8058\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4165985570318378\n",
      "0.7668018095986544\n",
      "best logloss is: 0.4165985570318378\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 19\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4203 - acc: 0.8059\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4175276288684297\n",
      "0.7665411405804643\n",
      "best logloss is: 0.4165985570318378\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 20\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4203 - acc: 0.8060\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41708609458200047\n",
      "0.7667515566353424\n",
      "best logloss is: 0.4165985570318378\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 21\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4202 - acc: 0.8059\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41694917108561147\n",
      "0.7670649982511587\n",
      "best logloss is: 0.4165985570318378\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 22\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4201 - acc: 0.8060\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41774821673337087\n",
      "0.7675776431536394\n",
      "best logloss is: 0.4165985570318378\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 23\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4199 - acc: 0.8060\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41629909930547365\n",
      "0.7674010111570146\n",
      "best logloss is: 0.41629909930547365\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 24\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4201 - acc: 0.8061\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41750782217780824\n",
      "0.7674452221984245\n",
      "best logloss is: 0.41629909930547365\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 25\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4198 - acc: 0.8061\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4167502506786918\n",
      "0.7674149572166375\n",
      "best logloss is: 0.41629909930547365\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 26\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4196 - acc: 0.8061\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4161744386413022\n",
      "0.7676057280968471\n",
      "best logloss is: 0.4161744386413022\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 27\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4197 - acc: 0.8062\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4166426473691045\n",
      "0.7677687199519627\n",
      "best logloss is: 0.4161744386413022\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 28\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4197 - acc: 0.8062\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4170829696920182\n",
      "0.7678262562882827\n",
      "best logloss is: 0.4161744386413022\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 29\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4196 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4161366772973735\n",
      "0.7674923288887061\n",
      "best logloss is: 0.4161366772973735\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 30\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4196 - acc: 0.8061\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.416241047539593\n",
      "0.7671410909568182\n",
      "best logloss is: 0.4161366772973735\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 31\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4194 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41706409099153635\n",
      "0.7680241277425046\n",
      "best logloss is: 0.4161366772973735\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 32\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4194 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4164860736768754\n",
      "0.7678060332682888\n",
      "best logloss is: 0.4161366772973735\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 33\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4193 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4160713498611774\n",
      "0.7676552732035515\n",
      "best logloss is: 0.4160713498611774\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 34\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4191 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41653013046728793\n",
      "0.7679986582190315\n",
      "best logloss is: 0.4160713498611774\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 35\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4191 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4160740248336209\n",
      "0.7683743720279239\n",
      "best logloss is: 0.4160713498611774\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 36\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4189 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4168269990409937\n",
      "0.7682688314535544\n",
      "best logloss is: 0.4160713498611774\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 37\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4190 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41627207396037663\n",
      "0.7683185332786779\n",
      "best logloss is: 0.4160713498611774\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 38\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4190 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4161531307752723\n",
      "0.7683920512910285\n",
      "best logloss is: 0.4160713498611774\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 39\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4190 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4160591517960373\n",
      "0.7682869593339028\n",
      "best logloss is: 0.4160591517960373\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 40\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4188 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41693026438942365\n",
      "0.7680898274559089\n",
      "best logloss is: 0.4160591517960373\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 41\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4188 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4164263239705985\n",
      "0.7686055021695066\n",
      "best logloss is: 0.4160591517960373\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 42\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4189 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41597332257903963\n",
      "0.7688638309846163\n",
      "best logloss is: 0.41597332257903963\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 43\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4189 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41592697834950487\n",
      "0.7682081016955342\n",
      "best logloss is: 0.41592697834950487\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 44\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4188 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4161008762406412\n",
      "0.7686734506864937\n",
      "best logloss is: 0.41592697834950487\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 45\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4188 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41680417215886323\n",
      "0.768197613466099\n",
      "best logloss is: 0.41592697834950487\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 46\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4185 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41567254185486424\n",
      "0.7692306416052361\n",
      "best logloss is: 0.41567254185486424\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 47\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4187 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4159503409254655\n",
      "0.7682838577816207\n",
      "best logloss is: 0.41567254185486424\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 48\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4186 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4161526096698905\n",
      "0.7689865219267957\n",
      "best logloss is: 0.41567254185486424\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 49\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4186 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41574186903090987\n",
      "0.7686754073562916\n",
      "best logloss is: 0.41567254185486424\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 50\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4187 - acc: 0.8067\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41629668118458646\n",
      "0.7688954675071109\n",
      "best logloss is: 0.41567254185486424\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 51\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 10us/step - loss: 0.4185 - acc: 0.8067\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4159713623093113\n",
      "0.768744677837395\n",
      "best logloss is: 0.41567254185486424\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 52\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 8s 11us/step - loss: 0.4183 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4163401000737267\n",
      "0.7687419064041217\n",
      "best logloss is: 0.41567254185486424\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 53\n",
      "Epoch 1/1\n",
      "210000/801319 [======>.......................] - ETA: 6s - loss: 0.4182 - acc: 0.8060"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m?\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                 \u001b[0mdoc_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                 \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                 train_batch_size=30000)\n\u001b[0m",
      "\u001b[0;32m?\u001b[0m in \u001b[0;36mtrain_each_fold\u001b[0;34m(input_train_dict, input_val_dict, y_train, y_val, cols, doc_col, tolerance, train_batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                   )\n\u001b[1;32m     15\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_val_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2670\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2672\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2652\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2654\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2655\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_each_fold(train_fold_dict[0],\n",
    "                                val_fold_dict[0],\n",
    "                                train_fold_y[0],\n",
    "                                val_fold_y[0],\n",
    "                                non_doc_col,\n",
    "                                doc_col,\n",
    "                                tolerance=30,\n",
    "                                train_batch_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T06:08:14.931625Z",
     "start_time": "2018-10-11T06:01:17.463793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.5816 - acc: 0.7943\n",
      "200331/200331 [==============================] - 3s 15us/step\n",
      "0.44435263369015127\n",
      "0.7305303186330571\n",
      "best logloss is: 0.44435263369015127\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 0\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4467 - acc: 0.8015\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4254868407125474\n",
      "0.7527701897075791\n",
      "best logloss is: 0.4254868407125474\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 1\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4313 - acc: 0.8015\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.42282694798092885\n",
      "0.756725486763627\n",
      "best logloss is: 0.42282694798092885\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 2\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4283 - acc: 0.8015\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.42177119585871203\n",
      "0.7581953283752709\n",
      "best logloss is: 0.42177119585871203\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 3\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4270 - acc: 0.8018\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4204347820032639\n",
      "0.7595937579223765\n",
      "best logloss is: 0.4204347820032639\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 4\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4254 - acc: 0.8037\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4200047148048922\n",
      "0.7612861862113182\n",
      "best logloss is: 0.4200047148048922\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 5\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4244 - acc: 0.8042\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4195467994487418\n",
      "0.7631089377312678\n",
      "best logloss is: 0.4195467994487418\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 6\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4234 - acc: 0.8046\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4189629604066365\n",
      "0.7640703886288822\n",
      "best logloss is: 0.4189629604066365\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 7\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4228 - acc: 0.8049\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4189210960007801\n",
      "0.7642704419230444\n",
      "best logloss is: 0.4189210960007801\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 8\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4224 - acc: 0.8051\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4182099980329445\n",
      "0.7643350612896853\n",
      "best logloss is: 0.4182099980329445\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 9\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4221 - acc: 0.8051\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41894650099752445\n",
      "0.7648415353554621\n",
      "best logloss is: 0.4182099980329445\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 10\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4219 - acc: 0.8052\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4175172382393698\n",
      "0.765429006215121\n",
      "best logloss is: 0.4175172382393698\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 11\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4216 - acc: 0.8051\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4172764984279069\n",
      "0.7656384883034083\n",
      "best logloss is: 0.4172764984279069\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 12\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4214 - acc: 0.8051\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41837840821799877\n",
      "0.7658248244674524\n",
      "best logloss is: 0.4172764984279069\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 13\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4210 - acc: 0.8054\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4178787549656448\n",
      "0.7660425138703719\n",
      "best logloss is: 0.4172764984279069\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 14\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4208 - acc: 0.8055\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41708938399665135\n",
      "0.7661186725998331\n",
      "best logloss is: 0.41708938399665135\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 15\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4209 - acc: 0.8056\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41683753957340236\n",
      "0.7662650761901344\n",
      "best logloss is: 0.41683753957340236\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 16\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4207 - acc: 0.8057\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41678749925972497\n",
      "0.7665357781633118\n",
      "best logloss is: 0.41678749925972497\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 17\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4207 - acc: 0.8057\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41669890346425376\n",
      "0.766342726368299\n",
      "best logloss is: 0.41669890346425376\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 18\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4203 - acc: 0.8060\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41653490072674193\n",
      "0.7669771846368041\n",
      "best logloss is: 0.41653490072674193\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 19\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4203 - acc: 0.8058\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41725197873718206\n",
      "0.7667252337366094\n",
      "best logloss is: 0.41653490072674193\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 20\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4202 - acc: 0.8059\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41742812665553564\n",
      "0.7669947376479889\n",
      "best logloss is: 0.41653490072674193\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 21\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4200 - acc: 0.8059\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4171214279541755\n",
      "0.7670515959676398\n",
      "best logloss is: 0.41653490072674193\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 22\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4200 - acc: 0.8061\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4161194095745543\n",
      "0.767407060989242\n",
      "best logloss is: 0.4161194095745543\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 23\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4199 - acc: 0.8061\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41665650140521515\n",
      "0.7676017649457001\n",
      "best logloss is: 0.4161194095745543\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 24\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4199 - acc: 0.8061\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41782413548513114\n",
      "0.767332383761838\n",
      "best logloss is: 0.4161194095745543\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 25\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4199 - acc: 0.8061\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41622308914429496\n",
      "0.7678049656876933\n",
      "best logloss is: 0.4161194095745543\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 26\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4197 - acc: 0.8062\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4164243881823341\n",
      "0.7675204451599336\n",
      "best logloss is: 0.4161194095745543\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 27\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4193 - acc: 0.8064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4162281248455081\n",
      "0.7672435064045706\n",
      "best logloss is: 0.4161194095745543\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 28\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4196 - acc: 0.8062\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4164340475350506\n",
      "0.767745790974257\n",
      "best logloss is: 0.4161194095745543\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 29\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4193 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.416222573920551\n",
      "0.7675813217680467\n",
      "best logloss is: 0.4161194095745543\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 30\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4192 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4169682972281847\n",
      "0.7677529865504893\n",
      "best logloss is: 0.4161194095745543\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 31\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4194 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4163178406173104\n",
      "0.7675681200621749\n",
      "best logloss is: 0.4161194095745543\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 32\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4193 - acc: 0.8060\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4161778548059729\n",
      "0.7677018945444862\n",
      "best logloss is: 0.4161194095745543\n",
      "remainning trial is: 10/30\n",
      "total epoch trained: 33\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4193 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4157456210034075\n",
      "0.7680366794702853\n",
      "best logloss is: 0.4157456210034075\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 34\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4193 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41701079876327013\n",
      "0.7676256001646993\n",
      "best logloss is: 0.4157456210034075\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 35\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4190 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.4163583240156478\n",
      "0.7679145661384713\n",
      "best logloss is: 0.4157456210034075\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 36\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 12us/step - loss: 0.4189 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 4us/step\n",
      "0.41576209378143464\n",
      "0.7680599406816413\n",
      "best logloss is: 0.4157456210034075\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 37\n",
      "Epoch 1/1\n",
      "270000/801319 [=========>....................] - ETA: 6s - loss: 0.4179 - acc: 0.8067"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m?\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                 \u001b[0mdoc_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                 \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                 train_batch_size=30000)\n\u001b[0m",
      "\u001b[0;32m?\u001b[0m in \u001b[0;36mtrain_each_fold\u001b[0;34m(input_train_dict, input_val_dict, y_train, y_val, cols, doc_col, tolerance, train_batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                   )\n\u001b[1;32m     15\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_val_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2670\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2672\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2652\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2654\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2655\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_each_fold(train_fold_dict[0],\n",
    "                                val_fold_dict[0],\n",
    "                                train_fold_y[0],\n",
    "                                val_fold_y[0],\n",
    "                                non_doc_col,\n",
    "                                doc_col,\n",
    "                                tolerance=30,\n",
    "                                train_batch_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T05:53:46.216191Z",
     "start_time": "2018-10-11T05:46:22.006285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 50)\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 10s 13us/step - loss: 0.5826 - acc: 0.7902\n",
      "200331/200331 [==============================] - 2s 12us/step\n",
      "0.43977965604093977\n",
      "0.7339098027688216\n",
      "best logloss is: 0.43977965604093977\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 0\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4456 - acc: 0.8015\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.42587318110852546\n",
      "0.7524068100533696\n",
      "best logloss is: 0.42587318110852546\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 1\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4308 - acc: 0.8015\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.422990472554352\n",
      "0.7566450518377805\n",
      "best logloss is: 0.422990472554352\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 2\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4285 - acc: 0.8015\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.42221585641956094\n",
      "0.758532063940224\n",
      "best logloss is: 0.42221585641956094\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 3\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4270 - acc: 0.8016\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.42109741386250654\n",
      "0.7593534836017288\n",
      "best logloss is: 0.42109741386250654\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 4\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4256 - acc: 0.8030\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4200156764040943\n",
      "0.760438748237753\n",
      "best logloss is: 0.4200156764040943\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 5\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4246 - acc: 0.8039\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41980437547763466\n",
      "0.7614119784708197\n",
      "best logloss is: 0.41980437547763466\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 6\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4238 - acc: 0.8043\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4194140854042065\n",
      "0.763198848443269\n",
      "best logloss is: 0.4194140854042065\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 7\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4233 - acc: 0.8046\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4184194970486161\n",
      "0.7638091852329939\n",
      "best logloss is: 0.4184194970486161\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 8\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4227 - acc: 0.8045\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4179707326448619\n",
      "0.7644913549792033\n",
      "best logloss is: 0.4179707326448619\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 9\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4224 - acc: 0.8047\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41842260757675603\n",
      "0.7647244423388242\n",
      "best logloss is: 0.4179707326448619\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 10\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4221 - acc: 0.8049\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4183845551469113\n",
      "0.765301861681644\n",
      "best logloss is: 0.4179707326448619\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 11\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4217 - acc: 0.8049\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41829947704460446\n",
      "0.765091445705086\n",
      "best logloss is: 0.4179707326448619\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 12\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4215 - acc: 0.8050\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4180469488731719\n",
      "0.7650068724261414\n",
      "best logloss is: 0.4179707326448619\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 13\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4213 - acc: 0.8050\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41743876724103385\n",
      "0.7654990719580076\n",
      "best logloss is: 0.41743876724103385\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 14\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4212 - acc: 0.8053\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4175108105998528\n",
      "0.7652575573610553\n",
      "best logloss is: 0.41743876724103385\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 15\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4213 - acc: 0.8053\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41777846101035426\n",
      "0.7658840408471551\n",
      "best logloss is: 0.41743876724103385\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 16\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4210 - acc: 0.8053\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41750170697705075\n",
      "0.7654697690597093\n",
      "best logloss is: 0.41743876724103385\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 17\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4209 - acc: 0.8054\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41745237356346054\n",
      "0.7653442282075671\n",
      "best logloss is: 0.41743876724103385\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 18\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4209 - acc: 0.8053\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4170486299485483\n",
      "0.7657148224697805\n",
      "best logloss is: 0.4170486299485483\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 19\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4207 - acc: 0.8056\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4173905865681947\n",
      "0.7659209214452074\n",
      "best logloss is: 0.4170486299485483\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 20\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4206 - acc: 0.8056\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41706943091472504\n",
      "0.7657724781658559\n",
      "best logloss is: 0.4170486299485483\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 21\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4203 - acc: 0.8057\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4178667958104487\n",
      "0.7660764285664678\n",
      "best logloss is: 0.4170486299485483\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 22\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4205 - acc: 0.8057\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4171529203155626\n",
      "0.7660361412029003\n",
      "best logloss is: 0.4170486299485483\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 23\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4201 - acc: 0.8059\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41689582214034926\n",
      "0.7658315333628967\n",
      "best logloss is: 0.41689582214034926\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 24\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4203 - acc: 0.8057\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41688975479922324\n",
      "0.7661293826316489\n",
      "best logloss is: 0.41688975479922324\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 25\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4203 - acc: 0.8059\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.416987407122946\n",
      "0.7658940770915845\n",
      "best logloss is: 0.41688975479922324\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 26\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4202 - acc: 0.8058\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4168747776367769\n",
      "0.7661056511867153\n",
      "best logloss is: 0.4168747776367769\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 27\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4199 - acc: 0.8060\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4168574711862803\n",
      "0.7659333042366412\n",
      "best logloss is: 0.4168574711862803\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 28\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4200 - acc: 0.8060\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41688931386451344\n",
      "0.7656081117155507\n",
      "best logloss is: 0.4168574711862803\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 29\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4201 - acc: 0.8060\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4167094241978728\n",
      "0.7658157671453224\n",
      "best logloss is: 0.4167094241978728\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 30\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4199 - acc: 0.8059\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41701589007484036\n",
      "0.7663359303026396\n",
      "best logloss is: 0.4167094241978728\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 31\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4198 - acc: 0.8059\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.416704642979733\n",
      "0.766417465390222\n",
      "best logloss is: 0.416704642979733\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 32\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4197 - acc: 0.8060\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41667035668086355\n",
      "0.766496962041875\n",
      "best logloss is: 0.41667035668086355\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 33\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4198 - acc: 0.8059\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4181050799890953\n",
      "0.7664189419581162\n",
      "best logloss is: 0.41667035668086355\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 34\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4197 - acc: 0.8061\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41714927812984465\n",
      "0.7665159133792245\n",
      "best logloss is: 0.41667035668086355\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 35\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4198 - acc: 0.8059\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41706658412822023\n",
      "0.7666944508243347\n",
      "best logloss is: 0.41667035668086355\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 36\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4196 - acc: 0.8060\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4168566338893555\n",
      "0.7663807335219439\n",
      "best logloss is: 0.41667035668086355\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 37\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4194 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41677303431542384\n",
      "0.7662073406076083\n",
      "best logloss is: 0.41667035668086355\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 38\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4196 - acc: 0.8062\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.416969200136353\n",
      "0.766434972819138\n",
      "best logloss is: 0.41667035668086355\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 39\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4195 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4164457680231697\n",
      "0.7666010930119926\n",
      "best logloss is: 0.4164457680231697\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 40\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4194 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41674914314898975\n",
      "0.7665472479779281\n",
      "best logloss is: 0.4164457680231697\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 41\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4195 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4167592400936529\n",
      "0.766490591724005\n",
      "best logloss is: 0.4164457680231697\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 42\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4192 - acc: 0.8061\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41667306230256\n",
      "0.7666998778555281\n",
      "best logloss is: 0.4164457680231697\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 43\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4192 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4171103428515504\n",
      "0.7663005091987106\n",
      "best logloss is: 0.4164457680231697\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 44\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4192 - acc: 0.8062\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41650444379494017\n",
      "0.7665804838741543\n",
      "best logloss is: 0.4164457680231697\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 45\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4191 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41687176480859495\n",
      "0.7667615899819299\n",
      "best logloss is: 0.4164457680231697\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 46\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4190 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4173730457592865\n",
      "0.7668398026546904\n",
      "best logloss is: 0.4164457680231697\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 47\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4191 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4165980058118201\n",
      "0.7665105829949721\n",
      "best logloss is: 0.4164457680231697\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 48\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4188 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41724266631612583\n",
      "0.7667267655201384\n",
      "best logloss is: 0.4164457680231697\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 49\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4188 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4165520621181493\n",
      "0.7668647578503982\n",
      "best logloss is: 0.4164457680231697\n",
      "remainning trial is: 10/30\n",
      "total epoch trained: 50\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4190 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4166876668378281\n",
      "0.7670223101431124\n",
      "best logloss is: 0.4164457680231697\n",
      "remainning trial is: 11/30\n",
      "total epoch trained: 51\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4186 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41696119068260556\n",
      "0.7668635165559329\n",
      "best logloss is: 0.4164457680231697\n",
      "remainning trial is: 12/30\n",
      "total epoch trained: 52\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4188 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4164818336710374\n",
      "0.7673869633596475\n",
      "best logloss is: 0.4164457680231697\n",
      "remainning trial is: 13/30\n",
      "total epoch trained: 53\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4187 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41706206151151654\n",
      "0.7662276742155122\n",
      "best logloss is: 0.4164457680231697\n",
      "remainning trial is: 14/30\n",
      "total epoch trained: 54\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4186 - acc: 0.8067\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4163791980052896\n",
      "0.7670917908352288\n",
      "best logloss is: 0.4163791980052896\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 55\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4186 - acc: 0.8068\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41637641105618656\n",
      "0.7671889570383004\n",
      "best logloss is: 0.41637641105618656\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 56\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4184 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4168636924970626\n",
      "0.7665591970329275\n",
      "best logloss is: 0.41637641105618656\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 57\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4184 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41666960670360725\n",
      "0.7669484555111888\n",
      "best logloss is: 0.41637641105618656\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 58\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4182 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41620433833772746\n",
      "0.7671130365584298\n",
      "best logloss is: 0.41620433833772746\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 59\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4183 - acc: 0.8068\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4164402337642076\n",
      "0.7671052437918198\n",
      "best logloss is: 0.41620433833772746\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 60\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4184 - acc: 0.8067\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4166866660090178\n",
      "0.7670405183798317\n",
      "best logloss is: 0.41620433833772746\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 61\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4182 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4175739367423465\n",
      "0.7667643885922604\n",
      "best logloss is: 0.41620433833772746\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 62\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4187 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4164644691311455\n",
      "0.7673752760502535\n",
      "best logloss is: 0.41620433833772746\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 63\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4181 - acc: 0.8068\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4169100283031255\n",
      "0.7674198046794642\n",
      "best logloss is: 0.41620433833772746\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 64\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4179 - acc: 0.8069\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4164784238696339\n",
      "0.7670231997805552\n",
      "best logloss is: 0.41620433833772746\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 65\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 7us/step - loss: 0.4181 - acc: 0.8069\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41646524870197366\n",
      "0.7666105350422061\n",
      "best logloss is: 0.41620433833772746\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 66\n",
      "Epoch 1/1\n",
      "660000/801319 [=======================>......] - ETA: 1s - loss: 0.4172 - acc: 0.8068"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m?\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                 \u001b[0mdoc_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                 \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                 train_batch_size=30000)\n\u001b[0m",
      "\u001b[0;32m?\u001b[0m in \u001b[0;36mtrain_each_fold\u001b[0;34m(input_train_dict, input_val_dict, y_train, y_val, cols, doc_col, tolerance, train_batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                   )\n\u001b[1;32m     15\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_val_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;31m# will be handled by on_epoch_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_each_fold(train_fold_dict[0],\n",
    "                                val_fold_dict[0],\n",
    "                                train_fold_y[0],\n",
    "                                val_fold_y[0],\n",
    "                                non_doc_col,\n",
    "                                doc_col,\n",
    "                                tolerance=30,\n",
    "                                train_batch_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T05:46:04.513575Z",
     "start_time": "2018-10-11T05:38:12.364468Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 17)\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 11s 13us/step - loss: 0.6044 - acc: 0.7902\n",
      "200331/200331 [==============================] - 2s 11us/step\n",
      "0.49339919530444026\n",
      "0.673679950581753\n",
      "best logloss is: 0.49339919530444026\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 0\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4627 - acc: 0.8015\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.44066223640252233\n",
      "0.7413590133123868\n",
      "best logloss is: 0.44066223640252233\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 1\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4353 - acc: 0.8021\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.42808033100569653\n",
      "0.7531161448738048\n",
      "best logloss is: 0.42808033100569653\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 2\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4306 - acc: 0.8037\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.42609187656596786\n",
      "0.7564367685944917\n",
      "best logloss is: 0.42609187656596786\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 3\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4283 - acc: 0.8040\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4221139943302295\n",
      "0.7580793807893577\n",
      "best logloss is: 0.4221139943302295\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 4\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4266 - acc: 0.8044\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4211266047584711\n",
      "0.7602500241737182\n",
      "best logloss is: 0.4211266047584711\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 5\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4252 - acc: 0.8045\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41983387982788195\n",
      "0.7615988363247138\n",
      "best logloss is: 0.41983387982788195\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 6\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4244 - acc: 0.8047\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41957737646221477\n",
      "0.7621285011138021\n",
      "best logloss is: 0.41957737646221477\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 7\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4236 - acc: 0.8051\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4186686051317469\n",
      "0.7625493903188745\n",
      "best logloss is: 0.4186686051317469\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 8\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4231 - acc: 0.8049\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41885726450907873\n",
      "0.7626768525184724\n",
      "best logloss is: 0.4186686051317469\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 9\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4228 - acc: 0.8053\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4187855311891303\n",
      "0.7632521421825049\n",
      "best logloss is: 0.4186686051317469\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 10\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4223 - acc: 0.8052\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41811782031830347\n",
      "0.7635667328317682\n",
      "best logloss is: 0.41811782031830347\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 11\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4222 - acc: 0.8050\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41842162375950376\n",
      "0.7636801707300138\n",
      "best logloss is: 0.41811782031830347\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 12\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4218 - acc: 0.8055\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41798512242128594\n",
      "0.7639051208714469\n",
      "best logloss is: 0.41798512242128594\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 13\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4216 - acc: 0.8056\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41861239904091896\n",
      "0.7644925282918655\n",
      "best logloss is: 0.41798512242128594\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 14\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4218 - acc: 0.8055\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41840063367785707\n",
      "0.7645487464234315\n",
      "best logloss is: 0.41798512242128594\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 15\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4213 - acc: 0.8057\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4176240997009027\n",
      "0.7644220786240962\n",
      "best logloss is: 0.4176240997009027\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 16\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4214 - acc: 0.8056\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4186592316381796\n",
      "0.7642450084267941\n",
      "best logloss is: 0.4176240997009027\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 17\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4208 - acc: 0.8059\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41762215324325\n",
      "0.7649689300431775\n",
      "best logloss is: 0.41762215324325\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 18\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4208 - acc: 0.8058\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4176364921455679\n",
      "0.7647905411712012\n",
      "best logloss is: 0.41762215324325\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 19\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4208 - acc: 0.8059\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41714886574559273\n",
      "0.7650375357437004\n",
      "best logloss is: 0.41714886574559273\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 20\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4206 - acc: 0.8059\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41803226074860816\n",
      "0.7651620241310169\n",
      "best logloss is: 0.41714886574559273\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 21\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4206 - acc: 0.8058\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4184536755397792\n",
      "0.7647159086645454\n",
      "best logloss is: 0.41714886574559273\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 22\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4206 - acc: 0.8058\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41714416645180025\n",
      "0.7653773300981885\n",
      "best logloss is: 0.41714416645180025\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 23\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4204 - acc: 0.8058\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41710496687904874\n",
      "0.7657609859517055\n",
      "best logloss is: 0.41710496687904874\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 24\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4203 - acc: 0.8061\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4173502908394886\n",
      "0.7655991420335562\n",
      "best logloss is: 0.41710496687904874\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 25\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4203 - acc: 0.8060\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41668054124428267\n",
      "0.7661562051339779\n",
      "best logloss is: 0.41668054124428267\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 26\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4204 - acc: 0.8059\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4171533329775356\n",
      "0.7657909541788246\n",
      "best logloss is: 0.41668054124428267\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 27\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4202 - acc: 0.8058\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.416834291058754\n",
      "0.7655359719794321\n",
      "best logloss is: 0.41668054124428267\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 28\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4202 - acc: 0.8062\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4172520734904193\n",
      "0.7658968595679848\n",
      "best logloss is: 0.41668054124428267\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 29\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4200 - acc: 0.8061\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41698770629582405\n",
      "0.765912440558642\n",
      "best logloss is: 0.41668054124428267\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 30\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4201 - acc: 0.8061\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4167269125953221\n",
      "0.7660016102846545\n",
      "best logloss is: 0.41668054124428267\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 31\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4201 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4168028436595024\n",
      "0.7661649024188402\n",
      "best logloss is: 0.41668054124428267\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 32\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4197 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41657825019321565\n",
      "0.7661913127227483\n",
      "best logloss is: 0.41657825019321565\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 33\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4198 - acc: 0.8061\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41711488725612134\n",
      "0.7662310653953375\n",
      "best logloss is: 0.41657825019321565\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 34\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4199 - acc: 0.8061\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4174588641517774\n",
      "0.7656755715154288\n",
      "best logloss is: 0.41657825019321565\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 35\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4199 - acc: 0.8060\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4170811905871717\n",
      "0.7663425768553245\n",
      "best logloss is: 0.41657825019321565\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 36\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4195 - acc: 0.8062\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41657907793578547\n",
      "0.7659247455782667\n",
      "best logloss is: 0.41657825019321565\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 37\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4194 - acc: 0.8061\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.417647073514123\n",
      "0.7662794035900784\n",
      "best logloss is: 0.41657825019321565\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 38\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4194 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41700029338808436\n",
      "0.7664159592173493\n",
      "best logloss is: 0.41657825019321565\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 39\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4196 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4167744779736838\n",
      "0.7657974931980815\n",
      "best logloss is: 0.41657825019321565\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 40\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4193 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4166446272642012\n",
      "0.7663530491857897\n",
      "best logloss is: 0.41657825019321565\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 41\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4193 - acc: 0.8062\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.416726479093887\n",
      "0.7663849371156433\n",
      "best logloss is: 0.41657825019321565\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 42\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4193 - acc: 0.8062\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4176754622458581\n",
      "0.7661902876699398\n",
      "best logloss is: 0.41657825019321565\n",
      "remainning trial is: 10/30\n",
      "total epoch trained: 43\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4195 - acc: 0.8062\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41674962163798845\n",
      "0.7667142809126402\n",
      "best logloss is: 0.41657825019321565\n",
      "remainning trial is: 11/30\n",
      "total epoch trained: 44\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4192 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41651056138104775\n",
      "0.7665870465460662\n",
      "best logloss is: 0.41651056138104775\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 45\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4190 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41707212649359315\n",
      "0.7668219237537794\n",
      "best logloss is: 0.41651056138104775\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 46\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4192 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41661152569646975\n",
      "0.7670253275796615\n",
      "best logloss is: 0.41651056138104775\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 47\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4190 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4163597035163182\n",
      "0.7667965390509202\n",
      "best logloss is: 0.4163597035163182\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 48\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4189 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41624663647761145\n",
      "0.7669905583335048\n",
      "best logloss is: 0.41624663647761145\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 49\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4189 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41636694947688885\n",
      "0.7666728231342521\n",
      "best logloss is: 0.41624663647761145\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 50\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4190 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41714123770711803\n",
      "0.7672903141472232\n",
      "best logloss is: 0.41624663647761145\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 51\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4188 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41623250582964183\n",
      "0.7669312924555308\n",
      "best logloss is: 0.41623250582964183\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 52\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4188 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4167203649253156\n",
      "0.7669744595690001\n",
      "best logloss is: 0.41623250582964183\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 53\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4189 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4171081942079254\n",
      "0.7670724144549705\n",
      "best logloss is: 0.41623250582964183\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 54\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4186 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41664566480743354\n",
      "0.7671047466161452\n",
      "best logloss is: 0.41623250582964183\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 55\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4185 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41692092950553333\n",
      "0.7670410607461748\n",
      "best logloss is: 0.41623250582964183\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 56\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4188 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4166996649287039\n",
      "0.7670213915272514\n",
      "best logloss is: 0.41623250582964183\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 57\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4187 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41634180601870924\n",
      "0.7672921258466092\n",
      "best logloss is: 0.41623250582964183\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 58\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4187 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4163452524022979\n",
      "0.7671762849326034\n",
      "best logloss is: 0.41623250582964183\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 59\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4186 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41618225733418435\n",
      "0.7675482459580016\n",
      "best logloss is: 0.41618225733418435\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 60\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4185 - acc: 0.8067\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41681143842258467\n",
      "0.767113670715871\n",
      "best logloss is: 0.41618225733418435\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 61\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4185 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41694288396301665\n",
      "0.7669330960879519\n",
      "best logloss is: 0.41618225733418435\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 62\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4184 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4165839466419431\n",
      "0.7669912590629882\n",
      "best logloss is: 0.41618225733418435\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 63\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4183 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4164227100823515\n",
      "0.7671340914156699\n",
      "best logloss is: 0.41618225733418435\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 64\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4184 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41666604272400326\n",
      "0.7677196186849793\n",
      "best logloss is: 0.41618225733418435\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 65\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4182 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.41650573475748914\n",
      "0.7675749332016051\n",
      "best logloss is: 0.41618225733418435\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 66\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 6s 8us/step - loss: 0.4182 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 3us/step\n",
      "0.4165707370263501\n",
      "0.7673234159595247\n",
      "best logloss is: 0.41618225733418435\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 67\n",
      "Epoch 1/1\n",
      "270000/801319 [=========>....................] - ETA: 4s - loss: 0.4176 - acc: 0.8066"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m?\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                 \u001b[0mdoc_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                 \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                 train_batch_size=30000)\n\u001b[0m",
      "\u001b[0;32m?\u001b[0m in \u001b[0;36mtrain_each_fold\u001b[0;34m(input_train_dict, input_val_dict, y_train, y_val, cols, doc_col, tolerance, train_batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                   )\n\u001b[1;32m     15\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_val_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    183\u001b[0m                         \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                         ins_batch = slice_arrays(\n\u001b[0;32m--> 185\u001b[0;31m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0m\u001b[1;32m    186\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_each_fold(train_fold_dict[0],\n",
    "                                val_fold_dict[0],\n",
    "                                train_fold_y[0],\n",
    "                                val_fold_y[0],\n",
    "                                non_doc_col,\n",
    "                                doc_col,\n",
    "                                tolerance=30,\n",
    "                                train_batch_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T16:23:24.504368Z",
     "start_time": "2018-10-10T16:10:44.188524Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get second order x\n",
      "project each in concat_list to a dense layer respectively\n",
      "interaction with the dense layers\n",
      "Epoch 1/1\n",
      "2394110/2394110 [==============================] - 43s 18us/step - loss: 0.4907 - acc: 0.7977\n",
      "598529/598529 [==============================] - 6s 9us/step\n",
      "0.42736679567997976\n",
      "0.7562519439802371\n",
      "best logloss is: 0.42736679567997976\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 0\n",
      "Epoch 1/1\n",
      "2394110/2394110 [==============================] - 37s 15us/step - loss: 0.4280 - acc: 0.8031\n",
      "598529/598529 [==============================] - 4s 7us/step\n",
      "0.4261155568545741\n",
      "0.7586742639007411\n",
      "best logloss is: 0.4261155568545741\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 1\n",
      "Epoch 1/1\n",
      "2394110/2394110 [==============================] - 36s 15us/step - loss: 0.4254 - acc: 0.8040\n",
      "598529/598529 [==============================] - 4s 7us/step\n",
      "0.42107705933936457\n",
      "0.7624200949357899\n",
      "best logloss is: 0.42107705933936457\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 2\n",
      "Epoch 1/1\n",
      "2394110/2394110 [==============================] - 37s 15us/step - loss: 0.4240 - acc: 0.8043\n",
      "598529/598529 [==============================] - 4s 7us/step\n",
      "0.42234088590906976\n",
      "0.7635232145830972\n",
      "best logloss is: 0.42107705933936457\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 3\n",
      "Epoch 1/1\n",
      "2394110/2394110 [==============================] - 37s 15us/step - loss: 0.4231 - acc: 0.8046\n",
      "598529/598529 [==============================] - 4s 7us/step\n",
      "0.4199840713888361\n",
      "0.7641397342386427\n",
      "best logloss is: 0.4199840713888361\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 4\n",
      "Epoch 1/1\n",
      "2394110/2394110 [==============================] - 37s 15us/step - loss: 0.4226 - acc: 0.8047\n",
      "598529/598529 [==============================] - 4s 7us/step\n",
      "0.42111599986883347\n",
      "0.7653509566056839\n",
      "best logloss is: 0.4199840713888361\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 5\n",
      "Epoch 1/1\n",
      "2394110/2394110 [==============================] - 37s 15us/step - loss: 0.4223 - acc: 0.8049\n",
      "598529/598529 [==============================] - 4s 7us/step\n",
      "0.42005422847552487\n",
      "0.765650413997895\n",
      "best logloss is: 0.4199840713888361\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 6\n",
      "Epoch 1/1\n",
      "2394110/2394110 [==============================] - 37s 15us/step - loss: 0.4220 - acc: 0.8049\n",
      "598529/598529 [==============================] - 4s 7us/step\n",
      "0.4185025092568941\n",
      "0.7664445036680845\n",
      "best logloss is: 0.4185025092568941\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 7\n",
      "Epoch 1/1\n",
      "2394110/2394110 [==============================] - 37s 15us/step - loss: 0.4218 - acc: 0.8049\n",
      "598529/598529 [==============================] - 4s 7us/step\n",
      "0.4200895420780072\n",
      "0.7662721332441813\n",
      "best logloss is: 0.4185025092568941\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 8\n",
      "Epoch 1/1\n",
      "2394110/2394110 [==============================] - 37s 15us/step - loss: 0.4215 - acc: 0.8050\n",
      "598529/598529 [==============================] - 4s 7us/step\n",
      "0.41868693420640984\n",
      "0.7663252645417904\n",
      "best logloss is: 0.4185025092568941\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 9\n",
      "Epoch 1/1\n",
      "2394110/2394110 [==============================] - 37s 15us/step - loss: 0.4215 - acc: 0.8050\n",
      "598529/598529 [==============================] - 4s 7us/step\n",
      "0.4190316384469263\n",
      "0.7668612495146565\n",
      "best logloss is: 0.4185025092568941\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 10\n",
      "Epoch 1/1\n",
      "2394110/2394110 [==============================] - 37s 15us/step - loss: 0.4213 - acc: 0.8052\n",
      "598529/598529 [==============================] - 4s 7us/step\n",
      "0.41832513810573513\n",
      "0.767166899856754\n",
      "best logloss is: 0.41832513810573513\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 11\n",
      "Epoch 1/1\n",
      "2394110/2394110 [==============================] - 37s 15us/step - loss: 0.4212 - acc: 0.8052\n",
      "598529/598529 [==============================] - 4s 7us/step\n",
      "0.4182274941395143\n",
      "0.7676855122017809\n",
      "best logloss is: 0.4182274941395143\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 12\n",
      "Epoch 1/1\n",
      "2394110/2394110 [==============================] - 37s 15us/step - loss: 0.4211 - acc: 0.8051\n",
      "598529/598529 [==============================] - 4s 7us/step\n",
      "0.4171459083821686\n",
      "0.7676557662413519\n",
      "best logloss is: 0.4171459083821686\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 13\n",
      "Epoch 1/1\n",
      "2394110/2394110 [==============================] - 37s 15us/step - loss: 0.4209 - acc: 0.8053\n",
      "598529/598529 [==============================] - 4s 7us/step\n",
      "0.4173128498228918\n",
      "0.7679923080140987\n",
      "best logloss is: 0.4171459083821686\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 14\n",
      "Epoch 1/1\n",
      "2394110/2394110 [==============================] - 37s 15us/step - loss: 0.4208 - acc: 0.8052\n",
      "598529/598529 [==============================] - 4s 7us/step\n",
      "0.4171621802749555\n",
      "0.7680969321202085\n",
      "best logloss is: 0.4171459083821686\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 15\n",
      "Epoch 1/1\n",
      "2394110/2394110 [==============================] - 37s 15us/step - loss: 0.4206 - acc: 0.8053\n",
      "598529/598529 [==============================] - 4s 7us/step\n",
      "0.417500936226285\n",
      "0.7681447893522895\n",
      "best logloss is: 0.4171459083821686\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 16\n",
      "Epoch 1/1\n",
      "2394110/2394110 [==============================] - 37s 15us/step - loss: 0.4206 - acc: 0.8054\n",
      "598529/598529 [==============================] - 4s 7us/step\n",
      "0.41768701278313686\n",
      "0.7683194034605241\n",
      "best logloss is: 0.4171459083821686\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 17\n",
      "Epoch 1/1\n",
      "  60000/2394110 [..............................] - ETA: 41s - loss: 0.4205 - acc: 0.8063"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m?\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                 \u001b[0mdoc_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                 \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                 train_batch_size=30000)\n\u001b[0m",
      "\u001b[0;32m?\u001b[0m in \u001b[0;36mtrain_each_fold\u001b[0;34m(input_train_dict, input_val_dict, y_train, y_val, cols, doc_col, tolerance, train_batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                   )\n\u001b[1;32m     15\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_val_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    183\u001b[0m                         \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                         ins_batch = slice_arrays(\n\u001b[0;32m--> 185\u001b[0;31m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0m\u001b[1;32m    186\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_each_fold(train_fold_dict[0],\n",
    "                                val_fold_dict[0],\n",
    "                                train_fold_y[0],\n",
    "                                val_fold_y[0],\n",
    "                                non_doc_col,\n",
    "                                doc_col,\n",
    "                                tolerance=30,\n",
    "                                train_batch_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_nn_model(cols,doc_cols=[],numu_cols=[]):\n",
    "#     \"\"\"\n",
    "#     cols, used to do ebd and dense layers\n",
    "#     doc_cols: used to do rnn\n",
    "#     there can be overlaps\n",
    "#     \"\"\"\n",
    "#     MLP_unit=128\n",
    "#     input_list = []\n",
    "#     concat_list = []\n",
    "#     numu_list = []\n",
    "#     for col in cols:\n",
    "#         max_feature = len(info_dict[prefix_input_nonDoc+col]['tok'].index_word)\n",
    "# #         max_feature = len(set(info_dict[prefix_input_nonDoc+col]['tok'].index_word.values()))\n",
    "#         embed_size = int(np.log2(max_feature)/np.log2(1.5))\n",
    "#         if embed_size< 2:\n",
    "#             embed_size = 2\n",
    "#         cur_input = Input(shape=(1, ),name = prefix_input_nonDoc+col)\n",
    "        \n",
    "       \n",
    "#         embed_layer = Embedding(max_feature,\n",
    "#                             embed_size,\n",
    "#                             input_length=1,\n",
    "#                             trainable=True,\n",
    "#                             embeddings_regularizer=l2(0.0005),\n",
    "#                             name='ebd_'+col)(cur_input)\n",
    "#         embed_layer = SpatialDropout1D(0.5)(embed_layer)\n",
    "#         x = Flatten()(embed_layer)\n",
    "#         input_list.append(cur_input)\n",
    "#         concat_list.append(x)\n",
    "#     for col in doc_cols:\n",
    "#         max_feature = len(info_dict[prefix_input_Doc+col]['tok'].index_word)\n",
    "# #         max_feature = len(set(info_dict[prefix_input_Doc+col]['tok'].index_word.values()))\n",
    "#         embed_size = int(np.log2(max_feature)/np.log2(1.5))\n",
    "#         if embed_size< 2:\n",
    "#             embed_size = 2\n",
    "#         input_shape = sequence_size_dict[col]\n",
    "#         cur_input = Input(shape=(input_shape, ),name = prefix_input_Doc+col)\n",
    "#         embed_layer = Embedding(max_feature,\n",
    "#                             embed_size,\n",
    "#                             input_length=input_shape,\n",
    "#                             trainable=True,\n",
    "#                             embeddings_regularizer=l2(0.0005),\n",
    "#                             name='ebd_rnn_'+col)(cur_input)\n",
    "#         x = SpatialDropout1D(0.5)(embed_layer)\n",
    "#         x = Bidirectional(CuDNNGRU(25, return_sequences=True))(x)\n",
    "#         x = Conv1D(25, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "#         x_aveP = GlobalAveragePooling1D()(x)\n",
    "#         x_maxP = GlobalMaxPooling1D()(x)\n",
    "#         x = concatenate([x_aveP,x_maxP])\n",
    "\n",
    "#         concat_list.append(x)\n",
    "#         input_list.append(cur_input)\n",
    "\n",
    "#     if len(numu_cols) > 0:\n",
    "#         print('add numu...')\n",
    "#         nu_shape = len(numu_cols)\n",
    "#         cur_input = Input(shape=(nu_shape, ),name = prefix_input_nu)\n",
    "#         x = BatchNormalization()(cur_input)\n",
    "#         x = Dense(128, activation='relu')(x)\n",
    "#         x = Dropout(0.2)(x)\n",
    "#         input_list.append(cur_input)\n",
    "#         numu_list.append(x)\n",
    "       \n",
    "#     if len(concat_list) > 1:\n",
    "#         x = concatenate(concat_list)\n",
    "# #     x = BatchNormalization()(x)\n",
    "    \n",
    "\n",
    "#     if len(numu_list)>0:\n",
    "#         x = concatenate([x]+numu_list)\n",
    "# #         x = BatchNormalization()(x)\n",
    "        \n",
    "#     x1 = Dense(512, activation='relu')(x)\n",
    "#     x1 = Dropout(0.2)(x1)\n",
    "    \n",
    "#     x2 = concatenate([x,x1])\n",
    "#     x2_ = Dense(128, activation='relu')(x2)\n",
    "#     x2_ = Dropout(0.2)(x2_)\n",
    "    \n",
    "#     x3 = concatenate([x,x1,x2_])\n",
    "#     x3_ = Dense(64, activation='relu')(x3)\n",
    "#     x3_ = Dropout(0.2)(x3_)\n",
    "    \n",
    "#     x = concatenate([x,x1,x2_,x3_])\n",
    "#     x = Dropout(0.5)(x)\n",
    "\n",
    "#     preds = Dense(1, activation=\"sigmoid\")(x)\n",
    "#     model = Model(input_list, preds)\n",
    "#     model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])    \n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-10T12:42:36.874Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_nn_model(cols,doc_cols=[],numu_cols=[]):\n",
    "#     \"\"\"\n",
    "#     cols, used to do ebd and dense layers\n",
    "#     doc_cols: used to do rnn\n",
    "#     there can be overlaps\n",
    "#     \"\"\"\n",
    "#     MLP_unit=128\n",
    "#     input_list = []\n",
    "#     concat_list = []\n",
    "#     numu_list = []\n",
    "#     for col in cols:\n",
    "#         max_feature = len(info_dict[prefix_input_nonDoc+col]['tok'].index_word)\n",
    "# #         max_feature = len(set(info_dict[prefix_input_nonDoc+col]['tok'].index_word.values()))\n",
    "#         embed_size = int(np.log2(max_feature)/np.log2(1.5))\n",
    "#         if embed_size< 2:\n",
    "#             embed_size = 2\n",
    "#         cur_input = Input(shape=(1, ),name = prefix_input_nonDoc+col)\n",
    "        \n",
    "       \n",
    "#         embed_layer = Embedding(max_feature,\n",
    "#                             embed_size,\n",
    "#                             input_length=1,\n",
    "#                             trainable=True,\n",
    "#                             embeddings_regularizer=l2(0.0005),\n",
    "#                             name='ebd_'+col)(cur_input)\n",
    "#         embed_layer = SpatialDropout1D(0.5)(embed_layer)\n",
    "#         x = Flatten()(embed_layer)\n",
    "#         input_list.append(cur_input)\n",
    "#         concat_list.append(x)\n",
    "#     for col in doc_cols:\n",
    "#         max_feature = len(info_dict[prefix_input_Doc+col]['tok'].index_word)\n",
    "# #         max_feature = len(set(info_dict[prefix_input_Doc+col]['tok'].index_word.values()))\n",
    "#         embed_size = int(np.log2(max_feature)/np.log2(1.5))\n",
    "#         if embed_size< 2:\n",
    "#             embed_size = 2\n",
    "#         input_shape = sequence_size_dict[col]\n",
    "#         cur_input = Input(shape=(input_shape, ),name = prefix_input_Doc+col)\n",
    "#         embed_layer = Embedding(max_feature,\n",
    "#                             embed_size,\n",
    "#                             input_length=input_shape,\n",
    "#                             trainable=True,\n",
    "#                             embeddings_regularizer=l2(0.0005),\n",
    "#                             name='ebd_rnn_'+col)(cur_input)\n",
    "#         x = SpatialDropout1D(0.5)(embed_layer)\n",
    "#         x = Bidirectional(CuDNNGRU(25, return_sequences=True))(x)\n",
    "#         x = Conv1D(25, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "#         x_aveP = GlobalAveragePooling1D()(x)\n",
    "#         x_maxP = GlobalMaxPooling1D()(x)\n",
    "#         x = concatenate([x_aveP,x_maxP])\n",
    "\n",
    "#         concat_list.append(x)\n",
    "#         input_list.append(cur_input)\n",
    "\n",
    "#     if len(numu_cols) > 0:\n",
    "#         print('add numu...')\n",
    "#         nu_shape = len(numu_cols)\n",
    "#         cur_input = Input(shape=(nu_shape, ),name = prefix_input_nu)\n",
    "#         x = BatchNormalization()(cur_input)\n",
    "#         x = Dense(128, activation='relu')(x)\n",
    "#         x = Dropout(0.2)(x)\n",
    "#         input_list.append(cur_input)\n",
    "#         numu_list.append(x)\n",
    "       \n",
    "#     if len(concat_list) > 1:\n",
    "#         x = concatenate(concat_list)\n",
    "# #     x = BatchNormalization()(x)\n",
    "    \n",
    "\n",
    "#     if len(numu_list)>0:\n",
    "#         x = concatenate([x]+numu_list)\n",
    "# #         x = BatchNormalization()(x)\n",
    "        \n",
    "#     x = Dense(512, activation='relu')(x)\n",
    "#     x = Dropout(0.2)(x)\n",
    "    \n",
    "#     x = Dense(128, activation='relu')(x)\n",
    "#     x = Dropout(0.2)(x)\n",
    "\n",
    "#     preds = Dense(1, activation=\"sigmoid\")(x)\n",
    "#     model = Model(input_list, preds)\n",
    "#     model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])    \n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T04:46:30.433965Z",
     "start_time": "2018-10-11T04:46:30.422775Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_each_fold(input_train_dict,input_val_dict,y_train,y_val,cols,doc_col=[],tolerance=30,train_batch_size=5000):\n",
    "    model = get_nn_model(cols,doc_col)\n",
    "    cur_to = 0\n",
    "    best_logloss = None\n",
    "    best_weights = None\n",
    "    count = 0\n",
    "    base_lr = 0.001\n",
    "    while True:\n",
    "        model.fit(input_train_dict, y_train, \n",
    "                  batch_size=train_batch_size, \n",
    "                  epochs=1,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  )\n",
    "        preds = model.predict(input_val_dict,30000,verbose=1)\n",
    "        logloss = log_loss(y_val,preds)\n",
    "        roc = roc_auc_score(y_val,preds)\n",
    "        print(logloss)\n",
    "        print(roc)\n",
    "        if best_logloss is None:\n",
    "            best_logloss = logloss\n",
    "            best_weights = model.get_weights()\n",
    "        else:\n",
    "            if best_logloss > logloss:\n",
    "                best_logloss = logloss\n",
    "                best_weights = model.get_weights()\n",
    "                cur_to = 0\n",
    "            else:\n",
    "                cur_to +=1\n",
    "        if cur_to == tolerance:\n",
    "            break\n",
    "        print('best logloss is: {}'.format(best_logloss))\n",
    "        print('remainning trial is: {}/{}'.format(cur_to,tolerance))\n",
    "        print('total epoch trained: {}'.format(count))\n",
    "        count+=1\n",
    "    model.set_weights(best_weights)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T04:46:36.273232Z",
     "start_time": "2018-10-11T04:46:36.251816Z"
    }
   },
   "outputs": [],
   "source": [
    "def nn_K_fold(train_fold_dict,\n",
    "              val_fold_dict,\n",
    "              train_fold_y,\n",
    "              val_fold_y,\n",
    "              test_input_dict,\n",
    "              val_index_list,\n",
    "              train_df,\n",
    "              test_df,\n",
    "              pred_col_name = 'predicted_score',\n",
    "              holdout_input_dict=None,\n",
    "              holdout_y=None,\n",
    "              holdout_index_list=None,\n",
    "              nondoc_cols=[],\n",
    "              doc_cols=[],\n",
    "              tolerance=30,\n",
    "              train_batch_size=5000,\n",
    "              preds_batch=5000):\n",
    "    \"\"\"\n",
    "    train_fold_dict: format, key - foldNum, value - nn input \n",
    "    train_fold_y: label for train fold, fotmat, key -foldNum, value - label\n",
    "    val_fold_dict: format, key - foldNum, value - nn input \n",
    "    val_fold_y: label for val fold, fotmat, key -foldNum, value - label\n",
    "    test_input_dict: format, key - foldNum, value - nn input \n",
    "    holdout_input_dict: Noneable, if none, not using holdout\n",
    "    holdout_y: Noneable, label for holdout\n",
    "    cols: all cols used to do ebd\n",
    "    doc_cols, cols that are used to do rnn\n",
    "    val_index_list: cv, going to predict and generate oof\n",
    "    holdout_index_list: going to predict on each fold\n",
    "    train_df: 'dataframe which only has id columns, which will be used to store oof prediction'\n",
    "    test_df: 'dataframe which only has id columns, which will be used to store test prediction'\n",
    "    \"\"\"\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    n_fold = len(train_fold_dict)\n",
    "    test_preds_list = []\n",
    "    val_score_list = []\n",
    "    hold_out_preds_list = []\n",
    "    holdout_score_list = []\n",
    "    train_score_list = []\n",
    "    cv_score_list = []\n",
    "    train_df[pred_col_name] = np.nan\n",
    "    test_df[pred_col_name] = np.nan\n",
    "    \n",
    "    for fold in range(n_fold):\n",
    "        print('start fold {}...'.format(fold))\n",
    "        model = train_each_fold(train_fold_dict[fold],\n",
    "                                val_fold_dict[fold],\n",
    "                                train_fold_y[fold],\n",
    "                                val_fold_y[fold],\n",
    "                                nondoc_cols,\n",
    "                                doc_cols,\n",
    "                                tolerance=tolerance,\n",
    "                                train_batch_size=train_batch_size)\n",
    "        train_preds =  model.predict(train_fold_dict[fold],preds_batch,verbose=1)\n",
    "        val_preds = model.predict(val_fold_dict[fold],preds_batch,verbose=1)\n",
    "        train_loss = log_loss(train_fold_y[fold],train_preds)\n",
    "        val_loss = log_loss(val_fold_y[fold],val_preds)\n",
    "        train_df.loc[val_index_list[fold],pred_col_name] = val_preds\n",
    "        test_preds = model.predict(test_input_dict,preds_batch,verbose=1)\n",
    "        test_preds_list.append(test_preds)\n",
    "        val_score_list.append(val_loss)\n",
    "        train_score_list.append(train_loss)\n",
    "        print('Fold {} finish! val loss: {}.'.format(fold,val_loss))\n",
    "        if holdout_index_list is not None:\n",
    "            ho_preds = model.predict(holdout_input_dict,preds_batch,verbose=1)\n",
    "            ho_loss = log_loss(holdout_y,ho_preds)\n",
    "            ho_roc = roc_auc_score(holdout_y,ho_preds)\n",
    "            holdout_score_list.append(ho_loss)\n",
    "            hold_out_preds_list.append(ho_preds)\n",
    "            print('hold out loss: {}'.format(ho_loss))\n",
    "        del model\n",
    "        gc.collect()\n",
    "        time.sleep(5)\n",
    "            \n",
    "    print('finish training... calculating evl matrix')\n",
    "    test_preds_list = np.array(test_preds_list)\n",
    "    hold_out_preds_list = np.array(hold_out_preds_list)\n",
    "    test_preds_final = np.mean(test_preds_list,axis=0)\n",
    "    cv_score_mean = np.mean(val_score_list)\n",
    "    train_score_mean = np.mean(train_score_list)\n",
    "    test_df[pred_col_name] = test_preds_final\n",
    "    print('cv mean is: {}'.format(cv_score_mean))\n",
    "    if holdout_index_list is not None:\n",
    "        ho_preds_final = np.mean(hold_out_preds_list,axis=0)\n",
    "        ho_loss_overall = log_loss(holdout_y,ho_preds_final)\n",
    "        ho_roc_overall = roc_auc_score(holdout_y,ho_preds_final)\n",
    "        train_df.loc[holdout_index_list,pred_col_name] = ho_preds_final\n",
    "        print('holdout loss overall is: {}'.format(ho_loss_overall))\n",
    "        print('holdout roc overall is: {}'.format(ho_roc_overall))\n",
    "        return train_df,test_df,cv_score_mean,ho_loss_overall,train_score_mean\n",
    "    else:\n",
    "        return train_df,test_df,cv_score_mean,0.0,train_score_mean\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-10T12:42:36.886Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = train[['instance_id']].copy()\n",
    "test_df = test[['instance_id']].copy()\n",
    "\n",
    "train_save,test_save,cv_,ho_,ta_ = nn_K_fold(train_fold_dict,\n",
    "                                      val_fold_dict,\n",
    "                                      train_fold_y,\n",
    "                                      val_fold_y,\n",
    "                                      test_input_dict,\n",
    "                                      val_index_list,\n",
    "                                      train_df,\n",
    "                                      test_df,\n",
    "                                      pred_col_name = 'predicted_score',\n",
    "                                      holdout_input_dict=holdout_input_dict,\n",
    "                                      holdout_y=holdout_y,\n",
    "                                      holdout_index_list=holdout_index,\n",
    "                                      nondoc_cols=non_doc_col,\n",
    "                                      doc_cols=doc_col,\n",
    "                                      tolerance=30,\n",
    "                                      preds_batch=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-10T12:42:36.890Z"
    }
   },
   "outputs": [],
   "source": [
    "model = train_each_fold(train_fold_dict[0],\n",
    "                                val_fold_dict[0],\n",
    "                                train_fold_y[0],\n",
    "                                val_fold_y[0],\n",
    "                                non_doc_col,\n",
    "                                doc_col,\n",
    "                                tolerance=30,\n",
    "                                train_batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-10T12:42:36.893Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_oof(train_df,test_df,cv,ta,ho=None,file_name='',path='../../data/nn_ebd/'):\n",
    "    try:\n",
    "        report = pd.read_csv(path+'report.csv')\n",
    "    except:\n",
    "        print('no report found! generate a new one!')\n",
    "        report = pd.DataFrame()\n",
    "    new_record = pd.DataFrame({'ho':[ho],'cv':[cv],'train_mean':[ta],'file':[file_name]})\n",
    "    report = pd.concat([report,new_record],sort=False)\n",
    "    train_df.to_pickle(path+'train/'+file_name+'.pkl')\n",
    "    print(train_df.shape)\n",
    "    test_df.to_csv(path+'test/'+file_name+'.csv',index=False)\n",
    "    print(test_df.shape)\n",
    "    report.to_csv(path+'report.csv',index=False)\n",
    "    print('done!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-10T12:42:36.894Z"
    }
   },
   "outputs": [],
   "source": [
    "save_oof(train_save,test_save,cv_,ta_,ho_,file_name='oldModel_RNNModelTag_ebdNANExclude_TagNAExclude_noholdout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-10T12:42:36.896Z"
    }
   },
   "outputs": [],
   "source": [
    "name_dict = {'new_modelMakeOsOSV_rnnEBD':\n",
    "             [['model','make','os','osv']],\n",
    "             'new_modelMakeOsOSV+modelAppidInnerslotCreateWCreativeH_rnnEBD':\n",
    "             [['model','make','os','osv'],['model','app_id','inner_slot_id','creative_width', 'creative_height']],\n",
    "             'new_modelOSV':[['model', 'osv']],'new_model_onlyuTagRNN_100patience':[],}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-10T12:42:36.900Z"
    }
   },
   "outputs": [],
   "source": [
    " model = get_nn_model(non_doc_col,doc_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-10T12:42:36.913Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-10T12:42:36.916Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-10T12:42:36.918Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-10T12:42:36.921Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(np.random.randint(1000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
