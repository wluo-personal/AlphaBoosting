{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T15:58:39.259252Z",
     "start_time": "2018-10-07T15:58:38.236913Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "__file__=''\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../LIB/'))\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../../../../automl/automl_libs/'))\n",
    "from env import FILE\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csc_matrix, csr_matrix, hstack\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU,CuDNNGRU,Flatten,BatchNormalization,CuDNNLSTM,Activation,BatchNormalization\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import log_loss,roc_auc_score\n",
    "from keras.regularizers import l1\n",
    "from keras.regularizers import l2\n",
    "from keras.regularizers import l1_l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import re\n",
    "from keras import backend as K\n",
    "import time\n",
    "import gc\n",
    "# To get learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T15:58:44.632989Z",
     "start_time": "2018-10-07T15:58:39.261240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: (1001650, 35)\n",
      "test shape is: (40024, 34)\n",
      "(1041674, 45)\n",
      "(1041674, 36)\n",
      "(1041674, 36)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_pickle(FILE.train_ori.value)\n",
    "print('train shape is: {}'.format(train.shape))\n",
    "test = pd.read_pickle(FILE.test_ori.value)\n",
    "print('test shape is: {}'.format(test.shape))\n",
    "train_length = len(train)\n",
    "\n",
    "X_shiyi = pd.read_pickle(FILE.shiyi_fillna_ori.value)\n",
    "# X_shiyi['weekDay'] = X_shiyi['time_day'] % 7 +1\n",
    "print(X_shiyi.shape)\n",
    "\n",
    "train = train.merge(X_shiyi[['time_hour','instance_id']],how='inner',on='instance_id')\n",
    "test = test.merge(X_shiyi[['time_hour','instance_id']],how='inner',on='instance_id')\n",
    "\n",
    "\n",
    "X = pd.concat([train,test],sort=False)\n",
    "print(X.shape)\n",
    "X = X.reset_index(drop=True)\n",
    "\n",
    "X_clean = pd.read_csv('../../data/original/cleaned_data_price_final.csv')\n",
    "X_append = pd.read_pickle('../../data/nn_features/clean.pkl')\n",
    "\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "assert np.sum(X_clean['instance_id'].values != X['instance_id'].values) == 0\n",
    "assert np.sum(X_append['instance_id'].values != X['instance_id'].values) == 0\n",
    "# X['model'] = X_append['model_v3'].values\n",
    "\n",
    "\n",
    "ignore_columns = ['instance_id','time','click'] + ['creative_is_js', 'creative_is_voicead', 'app_paid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PB 4236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T15:58:44.636520Z",
     "start_time": "2018-10-07T15:58:44.634504Z"
    }
   },
   "outputs": [],
   "source": [
    "# need_process_col = list(set(X.columns) - set(ignore_columns))\n",
    "# X_ = X[need_process_col].copy()\n",
    "\n",
    "\n",
    "# doc_col = ['user_tags','model']\n",
    "# non_doc_col = [f for f in need_process_col if f not in doc_col]\n",
    "# # doc_col = doc_col + []\n",
    "# # non_doc_col = non_doc_col+['user_tags']\n",
    "# counter = 0\n",
    "\n",
    "# X_doc = X[doc_col].copy()\n",
    "# for col in tqdm(non_doc_col):\n",
    "#     X_[col] = le.fit_transform(X_[col].astype(str))\n",
    "#     X_[col] = col + '_'+X_[col].astype(str)\n",
    "    \n",
    "# for col in tqdm(doc_col):\n",
    "#     X_doc[col] = X_doc[col].astype(str)\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T16:00:03.550700Z",
     "start_time": "2018-10-07T15:58:44.637877Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:18<00:00,  2.79s/it]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.49it/s]\n"
     ]
    }
   ],
   "source": [
    "need_process_col = list(set(X.columns) - set(ignore_columns))\n",
    "X_ = X[need_process_col].copy()\n",
    "\n",
    "\n",
    "doc_col = ['user_tags','model']\n",
    "non_doc_col = [f for f in need_process_col if f not in doc_col]\n",
    "# doc_col = doc_col + []\n",
    "# non_doc_col = non_doc_col+['user_tags']\n",
    "counter = 0\n",
    "\n",
    "X_doc = X[doc_col].copy()\n",
    "for col in tqdm(non_doc_col):\n",
    "#     col = 'creative_id'\n",
    "    test_values = set(X_[col].iloc[train_length:].astype(str).unique())\n",
    "    train_values = set(X_[col].iloc[:train_length].astype(str).unique())\n",
    "    intersection = train_values.intersection(test_values)\n",
    "    out_liyer = list(train_values.union(test_values) - intersection)\n",
    "    if len(out_liyer) > 0:\n",
    "#         print(len(out_liyer))\n",
    "        out_liyer_mapping = pd.Series(index=list(out_liyer),data=1)\n",
    "        filtered = (X_[col].astype(str).map(out_liyer_mapping) == 1)\n",
    "#         print('col:{}, size:{}'.format(col,np.sum(filtered)))\n",
    "        X_.loc[filtered,col] = np.nan\n",
    "        \n",
    "        \n",
    "    X_[col] = le.fit_transform(X_[col].astype(str))\n",
    "    X_[col] = col + '_'+X_[col].astype(str)\n",
    "    \n",
    "for col in tqdm(doc_col):\n",
    "    X_doc[col] = X_doc[col].astype(str)\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T16:00:03.556809Z",
     "start_time": "2018-10-07T16:00:03.552365Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_tok(X,col,train_length,d_filter='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
    "    X[col] = X[col].astype(str)\n",
    "\n",
    "    tok_all=text.Tokenizer(num_words=X[col].nunique(),lower=False,filters=d_filter)\n",
    "    tok_all.fit_on_texts(list(X[col].values))\n",
    "\n",
    "    tok_train=text.Tokenizer(num_words=X[col].iloc[:train_length].nunique(),lower=False,filters=d_filter)\n",
    "    tok_train.fit_on_texts(list(X[col].iloc[:train_length].values))\n",
    "\n",
    "    tok_test=text.Tokenizer(num_words=X[col].iloc[train_length:].nunique(),lower=False,filters=d_filter)\n",
    "    tok_test.fit_on_texts(list(X[col].iloc[train_length:].values))\n",
    "    word_intersection = set(tok_train.word_index.keys()).intersection(set(tok_test.word_index.keys()))\n",
    "\n",
    "    self_index = {}\n",
    "    count = 1\n",
    "    for word in word_intersection:\n",
    "        self_index[word] = count\n",
    "        count+=1\n",
    "    self_index['unknown'] = count\n",
    "    print('max index is: {}'.format(count))\n",
    "\n",
    "    for word in tok_all.word_index.keys():\n",
    "        tok_all.word_index[word] = self_index.get(word,count)\n",
    "    return tok_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T16:00:03.580560Z",
     "start_time": "2018-10-07T16:00:03.558369Z"
    }
   },
   "outputs": [],
   "source": [
    "# # def comb_fe(X,cols,sep=' '):\n",
    "# #     ret = X[cols[0]].astype(str).copy()\n",
    "# #     for col in cols[1:]:\n",
    "# #         ret = ret + sep + X[col].astype(str)\n",
    "# #     return ret.values\n",
    "\n",
    "# def process_slot(x):\n",
    "#     x = re.sub(r'[-_]',' ',x)\n",
    "#     x = x.split(' ')\n",
    "#     ret = ['p{}_'.format(c)+x[c] for c in range(len(x))]\n",
    "#     ret = ' '.join(ret)\n",
    "#     return ret\n",
    "\n",
    "# def comb_fe(X,cols,sep=' '):\n",
    "#     def add_col_name(x,colName,splitter=' ' ,sep=' '):\n",
    "#         ret = [colName+'_'+each for each in x.split(splitter)]\n",
    "#         return sep.join(ret)\n",
    "#     ret = pd.DataFrame()\n",
    "#     if cols[0] == 'user_tags':\n",
    "#         ret['comb'] = X['user_tags'].astype(str).apply(add_col_name,colName='userTags',splitter=',',sep=sep)\n",
    "#     else:\n",
    "#         if cols[0] in original_name_col:\n",
    "#             ret['comb'] = X[cols[0]].astype(str).copy()\n",
    "#         else:\n",
    "#             ret['comb'] = X[cols[0]].astype(str).apply(add_col_name,colName=cols[0],splitter=' ',sep=sep)\n",
    "\n",
    "#     for col in tqdm(cols[1:]):\n",
    "#         if col == 'user_tags':\n",
    "#             ret['new_feature'] = X['user_tags'].astype(str).apply(add_col_name,colName='userTags',splitter=',',sep=sep)\n",
    "#         else:\n",
    "#             if col in original_name_col:\n",
    "#                 ret['new_feature'] = X[col].astype(str).copy()\n",
    "#             else:\n",
    "#                 ret['new_feature'] = X[col].astype(str).apply(add_col_name,colName=col,splitter=' ',sep=sep)\n",
    "#         ret['comb'] =( ret['comb'] + sep + ret['new_feature']).values\n",
    "#     return ret['comb'].values\n",
    "\n",
    "# processed_col = []\n",
    "# original_name_col = ['model','make','os','osv']\n",
    "# # cob_col = [['model','make','os','osv']]\n",
    "# # cob_col = [['model','make','os','osv'],['model','app_id','inner_slot_id','creative_width', 'creative_height']]\n",
    "# # cob_col = [['model', 'osv']]\n",
    "# cob_col=[]\n",
    "# doc_col=['user_tags','model']\n",
    "# # doc_col=['user_tags']\n",
    "# X_doc = X[doc_col].copy()\n",
    "# processed_col.extend(doc_col)\n",
    "# for each in cob_col:\n",
    "#     feature_name = '_'.join(each)\n",
    "#     processed_col.extend(each)\n",
    "#     doc_col.append(feature_name)\n",
    "#     X_doc[feature_name] = comb_fe(X,each)\n",
    "    \n",
    "# non_doc_col = list(set(X.columns) - set(ignore_columns) - set(processed_col))\n",
    "# X_ = X[non_doc_col].copy()\n",
    "\n",
    "# for col in tqdm(non_doc_col):\n",
    "#     X_[col] = le.fit_transform(X_[col].astype(str))\n",
    "#     X_[col] = col + '_'+X_[col].astype(str)\n",
    "    \n",
    "# for col in tqdm(doc_col):\n",
    "#     if col=='inner_slot_id':\n",
    "#         X_doc[col] = X_doc[col].astype(str).apply(process_slot)\n",
    "#     else:\n",
    "#         X_doc[col] = X_doc[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T16:00:03.589941Z",
     "start_time": "2018-10-07T16:00:03.582801Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_index = pickle.load(open(FILE.train_index.value,'rb'))\n",
    "# holdout_index = pickle.load(open(FILE.holdout_index.value,'rb'))\n",
    "\n",
    "# train_index = pickle.load(open('../../data/original/adversarial_train_index.pickle','rb'))\n",
    "# holdout_index = pickle.load(open('../../data/original/adversarial_ho_index.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T16:00:03.597416Z",
     "start_time": "2018-10-07T16:00:03.592762Z"
    }
   },
   "outputs": [],
   "source": [
    "train_index = train.index\n",
    "holdout_index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T16:00:07.066790Z",
     "start_time": "2018-10-07T16:00:03.600651Z"
    }
   },
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "seed = 1001\n",
    "train_index_list = []\n",
    "val_index_list = []\n",
    "folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=seed)\n",
    "for t,v in folds.split(train.loc[train_index],train.loc[train_index,'click']):\n",
    "    train_index_list.append(train.loc[train_index].iloc[t].index)\n",
    "    val_index_list.append(train.loc[train_index].iloc[v].index)\n",
    "    \n",
    "# check = []\n",
    "# for i in val_index_list:\n",
    "#     check.extend(list(i))\n",
    "# assert len(set(check+list(holdout_index))) == len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T16:00:07.327343Z",
     "start_time": "2018-10-07T16:00:07.068387Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fold_y = {}\n",
    "val_fold_y = {}\n",
    "if holdout_index is not None:\n",
    "    holdout_y = train.loc[holdout_index,'click'].values\n",
    "else:\n",
    "    holdout_y = None\n",
    "for fold in range(num_folds):\n",
    "    train_fold_y[fold] = train.loc[train_index_list[fold],'click'].values\n",
    "    val_fold_y[fold] = train.loc[val_index_list[fold],'click'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T16:05:30.118205Z",
     "start_time": "2018-10-07T16:00:07.329051Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [04:26<00:00,  9.51s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max index is: 1313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:56<00:00, 28.32s/it]\n"
     ]
    }
   ],
   "source": [
    "info_dict = {}\n",
    "train_all_dict = {}\n",
    "train_fold_dict = {}\n",
    "\n",
    "val_fold_dict = {}\n",
    "\n",
    "holdout_input_dict = {}\n",
    "test_input_dict = {}\n",
    "maxlen = 1\n",
    "prefix_input_nonDoc = 'input_'\n",
    "prefix_input_Doc = 'input_rnn_'\n",
    "for col in tqdm(non_doc_col):\n",
    "\n",
    "    maxlen = 1\n",
    "    tok=text.Tokenizer(num_words=X_[col].nunique(),lower=False,filters='@')\n",
    "    tok.fit_on_texts(list(X_[col]))\n",
    "    info_dict.update({prefix_input_nonDoc+col:{'tok':tok}})\n",
    "    t = tok.texts_to_sequences(list(X_[col].iloc[:train_length].values))\n",
    "    te = tok.texts_to_sequences(list(X_[col].iloc[train_length:].values))\n",
    "    train_all_dict[prefix_input_nonDoc+col] = sequence.pad_sequences(t,maxlen=maxlen)\n",
    "    test_input_dict[prefix_input_nonDoc+col] = sequence.pad_sequences(te,maxlen=maxlen)\n",
    "    if holdout_index is not None:\n",
    "        holdout_input_dict[prefix_input_nonDoc+col] = train_all_dict[prefix_input_nonDoc+col][list(holdout_index)]\n",
    "    \n",
    "    for fold in range(num_folds):\n",
    "        if train_fold_dict.get(fold) is None:\n",
    "            train_fold_dict[fold] = {}\n",
    "            val_fold_dict[fold] = {}\n",
    "        train_fold_dict[fold].update({prefix_input_nonDoc+col: train_all_dict[prefix_input_nonDoc+col][list(train_index_list[fold])]})\n",
    "        val_fold_dict[fold].update({prefix_input_nonDoc+col:train_all_dict[prefix_input_nonDoc+col][list(val_index_list[fold])]})\n",
    "        \n",
    "sequence_size_dict = {}\n",
    "for col in tqdm(doc_col):\n",
    "    if col == 'user_tags':\n",
    "        maxlen = 50\n",
    "        tok = get_tok(X_doc,col=col,train_length=train_length,d_filter=',')\n",
    "#         tok=text.Tokenizer(num_words=X_doc[col].nunique(),lower=False, filters=',')\n",
    "#         tok.fit_on_texts(list(X_doc[col]))\n",
    "\n",
    "        \n",
    "    elif col == 'model':\n",
    "        maxlen = 15\n",
    "#         tok = get_tok(X_doc,col=col,train_length=train_length)\n",
    "        tok=text.Tokenizer(num_words=X_doc[col].nunique(),lower=False,filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "        tok.fit_on_texts(list(X_doc[col]))\n",
    "    else:\n",
    "        maxlen = 15\n",
    "#         tok = get_tok(X_doc,col=col,train_length=train_length)\n",
    "        tok=text.Tokenizer(num_words=X_doc[col].nunique(),lower=False)\n",
    "        tok.fit_on_texts(list(X_doc[col]))\n",
    "    info_dict.update({prefix_input_Doc+col:{'tok':tok}})\n",
    "    sequence_size_dict[col] = maxlen\n",
    "    t = tok.texts_to_sequences(list(X_doc[col].iloc[:train_length].values))\n",
    "    te = tok.texts_to_sequences(list(X_doc[col].iloc[train_length:].values))\n",
    "    train_all_dict[prefix_input_Doc+col] = sequence.pad_sequences(t,maxlen=maxlen)\n",
    "    test_input_dict[prefix_input_Doc+col] = sequence.pad_sequences(te,maxlen=maxlen)\n",
    "    if holdout_index is not None:\n",
    "        holdout_input_dict[prefix_input_Doc+col] = train_all_dict[prefix_input_Doc+col][list(holdout_index)]\n",
    "    \n",
    "    for fold in range(num_folds):\n",
    "        if train_fold_dict.get(fold) is None:\n",
    "            train_fold_dict[fold] = {}\n",
    "            val_fold_dict[fold] = {}\n",
    "        train_fold_dict[fold].update({prefix_input_Doc+col: train_all_dict[prefix_input_Doc+col][list(train_index_list[fold])]})\n",
    "        val_fold_dict[fold].update({prefix_input_Doc+col:train_all_dict[prefix_input_Doc+col][list(val_index_list[fold])]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T16:05:30.121975Z",
     "start_time": "2018-10-07T16:05:30.119798Z"
    }
   },
   "outputs": [],
   "source": [
    "tok = info_dict['input_rnn_model']['tok']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T16:05:30.165854Z",
     "start_time": "2018-10-07T16:05:30.123244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OPPOR9PlusmA': 2101,\n",
       " '25252C3': 4893,\n",
       " 'H32': 5863,\n",
       " 'NX40X': 9315,\n",
       " 'K10e70': 801,\n",
       " 'WDB': 5701,\n",
       " 'Cong': 2870,\n",
       " 'T350': 5415,\n",
       " 'D5503': 8916,\n",
       " 'KT40Q': 6695,\n",
       " 'bodu': 8353,\n",
       " 'TCL580': 8752,\n",
       " 'A6600': 4789,\n",
       " 'mlledn9': 6092,\n",
       " 'Y65': 3593,\n",
       " 'G0245D': 2706,\n",
       " '850M': 4236,\n",
       " 'A10S': 6774,\n",
       " 'ovvi': 8813,\n",
       " '20Y31': 1185,\n",
       " 'V': 343,\n",
       " 'x18': 7975,\n",
       " '20G628': 2636,\n",
       " '2525252525252BPlus': 810,\n",
       " 'A73': 87,\n",
       " '252BA33m': 2889,\n",
       " 'vivoX710L': 5866,\n",
       " '2525252525252BF7': 7518,\n",
       " 'BD': 5519,\n",
       " '252525252525252BA3010': 6205,\n",
       " 'U5483': 3093,\n",
       " '2525252BY71A': 2254,\n",
       " '8718': 990,\n",
       " 'G3509I': 3158,\n",
       " 'S9L': 963,\n",
       " '252525252BY83A': 4710,\n",
       " '25252BA8': 5776,\n",
       " '4W': 635,\n",
       " 'NT1': 2902,\n",
       " 'D58X': 8086,\n",
       " 'I9128': 4178,\n",
       " 'qh106': 7535,\n",
       " 'iPhone4.1': 4302,\n",
       " 'G950U': 4359,\n",
       " 'y51a': 1796,\n",
       " 'a0001': 4110,\n",
       " 'a03': 2444,\n",
       " '2525252BMAX': 1729,\n",
       " 'k52t38': 6766,\n",
       " '2525252525252BK8300': 8656,\n",
       " '2BPlust': 5645,\n",
       " 'XIAOMI': 5273,\n",
       " '2525252525252BA5000': 4357,\n",
       " '25252525252BX6A': 2843,\n",
       " 'ONEPLUS': 166,\n",
       " '25252525252B6X': 2239,\n",
       " 'a31u': 5367,\n",
       " 'vivoX9s': 1965,\n",
       " '25252525252BY67': 1706,\n",
       " 'R9S': 1103,\n",
       " '20Z2': 8622,\n",
       " '25252525252BR11s': 1135,\n",
       " 'VOLTE': 3373,\n",
       " 'g6000': 2977,\n",
       " 'm3': 70,\n",
       " '1607': 419,\n",
       " 'LGL23': 7561,\n",
       " '25252525252BPluskt': 2810,\n",
       " 'MP1503': 612,\n",
       " '201LTEW': 7077,\n",
       " '20M8': 9248,\n",
       " 'C5502': 5603,\n",
       " 'I9300I': 2223,\n",
       " 'C630Lw': 3603,\n",
       " 'starlet': 7929,\n",
       " '25252BA79k': 3860,\n",
       " 'S860e': 6199,\n",
       " 'e7s': 7259,\n",
       " 'BV0800': 1823,\n",
       " '25221707': 8999,\n",
       " 'W2015': 1404,\n",
       " '2525252BCRR': 9407,\n",
       " '25252525252BY55': 2875,\n",
       " '252525252525252BY69A': 8763,\n",
       " 'P689L': 4879,\n",
       " '252525252BX9': 1361,\n",
       " '20C880U': 5131,\n",
       " 'X829': 7081,\n",
       " 'D855': 7904,\n",
       " '20F22M': 5834,\n",
       " 'G955F': 1771,\n",
       " '2BR11s': 3972,\n",
       " 'SAMSUNG': 2389,\n",
       " 'H831': 7428,\n",
       " 'T9000': 5308,\n",
       " '25252BM9et': 9540,\n",
       " 'PhilipsX596': 8843,\n",
       " 'N920T': 9190,\n",
       " '252BX21i': 3161,\n",
       " 'M651G': 8033,\n",
       " 'BA510': 393,\n",
       " 'X5V': 403,\n",
       " 'ztec880s': 9421,\n",
       " 'lex650': 5443,\n",
       " '252BS': 8505,\n",
       " 'C2016': 958,\n",
       " 'XM50t': 6013,\n",
       " '20K10e70': 4836,\n",
       " '252525252BR9m': 1531,\n",
       " 'R7kt': 7373,\n",
       " 'T6': 647,\n",
       " 'GIONEEF6L': 5719,\n",
       " '20A9w': 6303,\n",
       " 'tl20': 2404,\n",
       " 'SPH': 5728,\n",
       " 'V957': 8644,\n",
       " 'E91w': 9249,\n",
       " 'N939St': 4491,\n",
       " 'BAC': 160,\n",
       " '2525252525252BR7sPlus': 4204,\n",
       " 'F5': 340,\n",
       " 'U950': 7568,\n",
       " 'CPH1605': 6186,\n",
       " 'R7': 145,\n",
       " 'G890A': 7740,\n",
       " 'T25': 813,\n",
       " 'hs': 6326,\n",
       " 'Y55': 90,\n",
       " '8690': 2172,\n",
       " 'LT680': 4539,\n",
       " '20A936': 6054,\n",
       " 'Y627': 1478,\n",
       " '6.0': 1681,\n",
       " 'NCE': 168,\n",
       " 'oppor9plusma': 984,\n",
       " 'A8010Q': 8794,\n",
       " 'g531h': 4408,\n",
       " '252BX9i': 2438,\n",
       " '25252BY29L': 6396,\n",
       " 'C900F': 8372,\n",
       " 'nx549j': 5647,\n",
       " 'al20': 1652,\n",
       " 'Q9Plus': 8519,\n",
       " 'J100F': 5717,\n",
       " 'oppoa37t': 4119,\n",
       " 'E97': 9583,\n",
       " '2B8060': 7956,\n",
       " '20R9s': 268,\n",
       " '20Y66i': 866,\n",
       " 'one': 3950,\n",
       " '2580': 4800,\n",
       " '2525252BY82': 7863,\n",
       " 'vivoY66L': 1587,\n",
       " 'HUAGAN': 1027,\n",
       " 'y67l': 2377,\n",
       " '25252C12': 8683,\n",
       " 'G3508': 5376,\n",
       " '2525252525252BX6S': 1691,\n",
       " '20X2': 4520,\n",
       " 'a37m': 1021,\n",
       " 'D10w': 2416,\n",
       " 'SHL25': 9569,\n",
       " '252BX5S': 7644,\n",
       " 'W800': 5102,\n",
       " '20f': 6912,\n",
       " 'P80': 6480,\n",
       " 'HUAWEIY635': 9621,\n",
       " 't': 678,\n",
       " 'p7': 2994,\n",
       " '2525252BY85': 2256,\n",
       " 'B8000': 4740,\n",
       " 'minote3': 3977,\n",
       " '2BPlus': 1766,\n",
       " '252BC880U': 6645,\n",
       " 'V987': 4487,\n",
       " '252BA5000': 3623,\n",
       " 'SCDMA': 7528,\n",
       " 'E250S': 4845,\n",
       " 't3': 1994,\n",
       " 'u51gt': 5813,\n",
       " 'D500': 6826,\n",
       " 'S326': 7196,\n",
       " 'hx903': 9336,\n",
       " '8F': 5901,\n",
       " 'g628': 6387,\n",
       " 'AOSON': 3487,\n",
       " '20A59t': 6210,\n",
       " '2BA83': 5291,\n",
       " '20X9Plus': 481,\n",
       " 'CYM1': 3199,\n",
       " '80': 1283,\n",
       " 'Extreme': 8661,\n",
       " '670N': 2181,\n",
       " 'ba520': 4532,\n",
       " 'S938t': 2986,\n",
       " 'A388t': 2705,\n",
       " '25252525252B3': 1606,\n",
       " 'T97': 5119,\n",
       " 'SW17G09': 8231,\n",
       " '25252525252BR11st': 2261,\n",
       " 'a83t': 4951,\n",
       " '20Z2121': 3586,\n",
       " 'Z1': 350,\n",
       " 'E76MINIM': 2707,\n",
       " 'IPH8': 4827,\n",
       " '252525252BY75A': 3636,\n",
       " 'r7c': 2472,\n",
       " 'T55': 3265,\n",
       " 'OBEE': 5173,\n",
       " 'GN5003': 291,\n",
       " 'N9005': 955,\n",
       " 'J71bAP': 7687,\n",
       " 'Tate': 5985,\n",
       " '252525252BV3Max': 2926,\n",
       " 'X820': 313,\n",
       " 'M3s': 189,\n",
       " '252525252C4': 5817,\n",
       " '252BXplay3S': 6958,\n",
       " '2BX5S': 8016,\n",
       " 'S36h': 9063,\n",
       " 'DL': 9298,\n",
       " 'D557': 1973,\n",
       " '2525252BR9tm': 1113,\n",
       " 'X7Plus': 81,\n",
       " 'G0215D': 732,\n",
       " 'Y1': 2003,\n",
       " '2525252525252BY83': 3745,\n",
       " 'D200': 4817,\n",
       " 'CT02': 6887,\n",
       " '25252525252BY9': 6702,\n",
       " 'U781': 6711,\n",
       " 'L3C': 2516,\n",
       " '20A628t': 6470,\n",
       " 'i7s': 8271,\n",
       " '25252525252BX20': 2121,\n",
       " 'A1457': 2087,\n",
       " 'LenovoA788t': 6570,\n",
       " '8712S': 1789,\n",
       " 'N7108D': 3205,\n",
       " 'Y21': 1918,\n",
       " '25252BM4': 8125,\n",
       " 'R8000': 2363,\n",
       " 'P739': 8733,\n",
       " 'A3890': 3923,\n",
       " '2525252BM9': 4944,\n",
       " 'T310': 4461,\n",
       " 's17': 7248,\n",
       " 'Y53n': 634,\n",
       " 'MTK': 3722,\n",
       " '2A': 216,\n",
       " '25252525252BRIO': 2598,\n",
       " 'vivoX21i': 4051,\n",
       " 'LeX620': 1589,\n",
       " 'SH': 7457,\n",
       " 'a388t': 6995,\n",
       " '20M30': 4543,\n",
       " '2522BLA': 8748,\n",
       " '8298': 902,\n",
       " '25252B9190L': 9462,\n",
       " '252BR7': 2554,\n",
       " 'U705T': 3391,\n",
       " 'e81': 8737,\n",
       " 'W2S': 9281,\n",
       " '2525252525252BY79A': 2697,\n",
       " 'E9007': 8025,\n",
       " 'Q201T': 8501,\n",
       " 'kingsun': 3882,\n",
       " 'G5108Q': 918,\n",
       " 'C199s': 1036,\n",
       " 'L39t': 7960,\n",
       " 'G720T': 3047,\n",
       " 'S10B': 624,\n",
       " 'n1t': 5531,\n",
       " 'CPH1719': 5854,\n",
       " 'K1plus': 7608,\n",
       " '9.0': 8330,\n",
       " '2525252BSE': 2874,\n",
       " '2525252525252BA5': 7787,\n",
       " 'gt20': 9186,\n",
       " '25252BPlustm': 2842,\n",
       " '8': 86,\n",
       " '20T8': 9041,\n",
       " 'G86': 7576,\n",
       " 'G718C': 3616,\n",
       " 'CM3': 2931,\n",
       " 'vivoX9sPlus': 2607,\n",
       " 'CM820': 8651,\n",
       " 'D2533': 7694,\n",
       " 'F333': 5088,\n",
       " 'BGO': 1288,\n",
       " 'X12': 2651,\n",
       " '5380CA': 1616,\n",
       " 'UIMI': 4889,\n",
       " 'D6653': 6067,\n",
       " 'R7s': 136,\n",
       " 'Quad': 6042,\n",
       " 'Redmi5Plus': 1834,\n",
       " 'Y803': 522,\n",
       " 'GALAXY': 9065,\n",
       " 'HUAWEIG7': 3100,\n",
       " '1841': 5550,\n",
       " 'A308t': 7791,\n",
       " 'V6': 1432,\n",
       " '25252525252B6s': 5596,\n",
       " 'l7': 4492,\n",
       " 'T520': 5790,\n",
       " 'COOLPAD8012': 7972,\n",
       " 'N9009': 542,\n",
       " '8.0': 1738,\n",
       " '20Y627': 6185,\n",
       " '20C8817E': 3960,\n",
       " 'CPH1701': 4796,\n",
       " 'mt2': 5004,\n",
       " 'x8': 7609,\n",
       " '252525252B1LTE': 4610,\n",
       " '252525252525252BG7': 4499,\n",
       " 'hht6a': 7971,\n",
       " 'oppor9km': 3444,\n",
       " 'QQ': 3614,\n",
       " 'YP09': 8152,\n",
       " 'SII': 9066,\n",
       " 'XT1570': 2171,\n",
       " 'SMARTEGO': 5934,\n",
       " '25252525252BY83': 6433,\n",
       " 't861bt': 6622,\n",
       " 'AL00X': 109,\n",
       " '20sonny': 8524,\n",
       " 'Coolpad7295': 5993,\n",
       " 'G920I': 3504,\n",
       " 'V889D': 3627,\n",
       " 'aum': 5414,\n",
       " '2522m1': 6478,\n",
       " 'L36h': 2650,\n",
       " '2525252BY13iL': 9306,\n",
       " 'B770S': 2662,\n",
       " 'Q787': 9086,\n",
       " '2525252BX6L': 5377,\n",
       " '2525252B2S': 3382,\n",
       " 'GN152': 721,\n",
       " 'RIO': 157,\n",
       " '20E77M': 6892,\n",
       " 'GN9010L': 1615,\n",
       " 'ba611t': 5085,\n",
       " '20T6V': 6920,\n",
       " 'D910': 7659,\n",
       " 'MG10': 9587,\n",
       " 'Find': 8080,\n",
       " '252525252BA37m': 1832,\n",
       " 'A00': 1314,\n",
       " '2525252BX6Plus': 2149,\n",
       " 'a9100': 3415,\n",
       " '2525252BS10B': 4775,\n",
       " '20A320e': 5662,\n",
       " '2525252BXplay6L': 4802,\n",
       " 'P631M': 7222,\n",
       " 'I939': 5139,\n",
       " '25252BX21UD': 2784,\n",
       " 'A690e': 7870,\n",
       " '20eH880': 8697,\n",
       " 'xxBEL7A': 8899,\n",
       " 'X16': 7212,\n",
       " 'BAOFA': 6849,\n",
       " '5086D': 8268,\n",
       " '20K51c78': 3799,\n",
       " '252525252525252B4X': 1760,\n",
       " 'HWWAS': 4664,\n",
       " '252525252525252BM6': 4898,\n",
       " '252BX7': 905,\n",
       " 'X17.0': 5655,\n",
       " 'LeBest': 3919,\n",
       " 'n970': 7688,\n",
       " 'G79': 8807,\n",
       " 'T19': 884,\n",
       " '2BNote': 1328,\n",
       " 'huaweic8818': 7493,\n",
       " '2BX9L': 5533,\n",
       " 'G955N': 1935,\n",
       " 'n9008v': 2137,\n",
       " 'Y79': 245,\n",
       " 'A01w': 3052,\n",
       " 'ZTEQ302C': 9396,\n",
       " 'volte': 5950,\n",
       " '2525252525252BA5010': 3988,\n",
       " 'G50': 4224,\n",
       " '7.1': 2200,\n",
       " 'H6C3': 7178,\n",
       " 'Moretel': 8279,\n",
       " 'bla': 3710,\n",
       " '25252BR7sm': 2730,\n",
       " 'E250K': 6079,\n",
       " 'ATH': 241,\n",
       " 'S7t': 4012,\n",
       " 'j3110': 4706,\n",
       " 'Z012DA': 5321,\n",
       " 'MediaPad': 2882,\n",
       " '8297w': 8521,\n",
       " 'V210101': 2615,\n",
       " 'cm': 6188,\n",
       " '252525252BV3M': 2424,\n",
       " '25252B5C': 6577,\n",
       " 'koomii': 7524,\n",
       " 'r8207': 1655,\n",
       " '2525252BD816d': 7766,\n",
       " 'LCH88': 9378,\n",
       " '7.1.2': 4469,\n",
       " 'I659': 7625,\n",
       " '252525252BA33t': 5513,\n",
       " '2016': 5640,\n",
       " 'x86': 4978,\n",
       " 'IPH': 3091,\n",
       " 'vivoX21UDA': 6242,\n",
       " 'AL30': 167,\n",
       " 'R820': 6839,\n",
       " 'A860e': 5169,\n",
       " 'MT2': 1073,\n",
       " 'xplay': 8421,\n",
       " 'MX4': 308,\n",
       " '252525252BY66i': 3114,\n",
       " '20Plusk': 1778,\n",
       " 'Core4': 8349,\n",
       " 'I639T': 6183,\n",
       " 'T0024545S': 7566,\n",
       " 'P360W': 4966,\n",
       " 'D10u': 7749,\n",
       " 'A1593': 1319,\n",
       " '20T9': 5130,\n",
       " 'E910': 4572,\n",
       " 'w': 3993,\n",
       " '252525252525252BCAZ': 2560,\n",
       " '4c': 254,\n",
       " 'T10A': 2568,\n",
       " '821w': 2599,\n",
       " 'A398t': 8703,\n",
       " '2017X82A': 3018,\n",
       " '252525252BA3010': 7826,\n",
       " '25252B2': 1271,\n",
       " 'M6': 72,\n",
       " 's206m': 8601,\n",
       " '20C9': 5734,\n",
       " 'Y927': 706,\n",
       " 'KXT': 7605,\n",
       " '25252525252BY33': 8209,\n",
       " 'Q801L': 7599,\n",
       " '252525252525252BY27': 5823,\n",
       " 'GN5005': 261,\n",
       " 'R2017': 505,\n",
       " '252525252B4X': 1908,\n",
       " 'C8812': 7047,\n",
       " 'VT898': 9295,\n",
       " 'M654': 2978,\n",
       " 'IdeaTab': 9209,\n",
       " 'A380e': 6917,\n",
       " 'M210': 5877,\n",
       " 'MJ': 9033,\n",
       " '252525252525252BR7': 2675,\n",
       " 'Amazon': 4875,\n",
       " 'U01': 2685,\n",
       " 'M100': 3353,\n",
       " '2BA59s': 1790,\n",
       " 'R801': 7682,\n",
       " '2BA5000': 5426,\n",
       " 'f103': 2179,\n",
       " 'OONEO1': 8204,\n",
       " 'T708S': 3750,\n",
       " 'I50JGSH': 7743,\n",
       " '6.1': 4540,\n",
       " 'Y51n': 1003,\n",
       " 'HHT6D': 1094,\n",
       " 'T35L': 7311,\n",
       " 'HUAWEIGRA': 2221,\n",
       " 'al00a': 1963,\n",
       " '1LTETD': 727,\n",
       " '703l': 2387,\n",
       " '25252BV3Max': 1358,\n",
       " '8D': 8832,\n",
       " '20D728w': 7182,\n",
       " 'MI8': 2649,\n",
       " 'R850': 6763,\n",
       " 'G7106': 843,\n",
       " 'DOOVV5': 9291,\n",
       " '8702d': 8415,\n",
       " 'U63': 5016,\n",
       " 'NX512J': 1561,\n",
       " '2BE': 4677,\n",
       " 'D826t': 4379,\n",
       " 'sprint': 9347,\n",
       " 'G920L': 6026,\n",
       " 'P7000': 7057,\n",
       " 'BP': 4298,\n",
       " 'Z017DA': 7944,\n",
       " 'Y623': 1143,\n",
       " 'sagit': 9361,\n",
       " '25252BI7': 8841,\n",
       " '25252BD820ts': 7864,\n",
       " '252BM7L': 5224,\n",
       " '208297': 3741,\n",
       " 'ul10': 1989,\n",
       " 'gn9010l': 8925,\n",
       " 'x5m': 3458,\n",
       " 'MI5C': 5496,\n",
       " 'R2': 1705,\n",
       " 'EML': 209,\n",
       " 'BenWee': 2813,\n",
       " '252525252525252BX528': 6009,\n",
       " '9120': 7778,\n",
       " '2BCAZ': 4975,\n",
       " 'Z19': 7368,\n",
       " 'sintave': 6732,\n",
       " 'Y67A': 50,\n",
       " 'meizu': 3786,\n",
       " '252525252BSE': 4103,\n",
       " 'f999': 7474,\n",
       " '252525252525252B2SC': 8201,\n",
       " 'A700L': 6797,\n",
       " 'k33': 7522,\n",
       " 'A2M': 6134,\n",
       " 'knt': 2126,\n",
       " 'g500': 9053,\n",
       " '25252525252B750': 7508,\n",
       " '25252BY27': 4447,\n",
       " 'f100s': 2066,\n",
       " 'kenzo': 6251,\n",
       " 'CPH1707': 4054,\n",
       " 'vivoY51': 1307,\n",
       " 'S7572': 3451,\n",
       " 'A630': 5103,\n",
       " '25252525252BP590L': 7723,\n",
       " '2BPlustm': 4890,\n",
       " 'T06': 3296,\n",
       " 'ZTEC880A': 8889,\n",
       " '25252525252B8712': 8072,\n",
       " 'RX030': 8126,\n",
       " 'c8816d': 5884,\n",
       " 'I1': 2610,\n",
       " '205890': 5065,\n",
       " 'Z9T': 7394,\n",
       " 'K18': 4671,\n",
       " 'PD1731': 4739,\n",
       " 'W017': 7670,\n",
       " 'S860': 4123,\n",
       " 'G928A': 5881,\n",
       " '2525252B5270': 7429,\n",
       " 'A766': 2968,\n",
       " 'N910L': 5207,\n",
       " '20C8813DQ': 8787,\n",
       " 'G666': 4773,\n",
       " '20Plus': 318,\n",
       " '252525252525252BS10': 3823,\n",
       " '25252BR11': 779,\n",
       " '252525252B8P': 2746,\n",
       " 'nx612j': 8397,\n",
       " 'xt1662': 5841,\n",
       " 'E6683': 2196,\n",
       " '2BMAX': 3455,\n",
       " 'S9Q': 2018,\n",
       " '308': 4289,\n",
       " '5951': 3236,\n",
       " 'A710K': 6207,\n",
       " '2BV3M': 4858,\n",
       " 'C1950': 5223,\n",
       " 'HMNOTE1LTE': 1774,\n",
       " '28A1549': 6647,\n",
       " '2525252525252B1TD': 8436,\n",
       " 'KT108': 7514,\n",
       " 'NEM': 121,\n",
       " 'OPPOA77t': 3626,\n",
       " '20C2017': 5309,\n",
       " 'FIG': 163,\n",
       " '2525252BF6': 4914,\n",
       " '8.1.2': 4003,\n",
       " 'mp': 7503,\n",
       " '252B5263': 8894,\n",
       " 'deeg1': 3096,\n",
       " 'C70': 3177,\n",
       " 'H860': 4220,\n",
       " 'l5': 6984,\n",
       " 'G610L': 7636,\n",
       " 'HWLLD': 2905,\n",
       " 'RS988': 9591,\n",
       " 'Y51A': 36,\n",
       " 'M6Note': 2064,\n",
       " 'i6310': 4013,\n",
       " 'T110': 5200,\n",
       " '25252BBA910T': 9521,\n",
       " 'Y330': 4008,\n",
       " '08': 1337,\n",
       " 'A378t': 5507,\n",
       " 'F109L': 1105,\n",
       " '703lt': 7842,\n",
       " '2525252BE': 1838,\n",
       " 'AUM': 279,\n",
       " 'UOOUOG': 8913,\n",
       " '2525252525252BL': 1360,\n",
       " 'N5117': 521,\n",
       " 'a5800': 4458,\n",
       " '25252525252B3W': 7728,\n",
       " 'D516w': 5345,\n",
       " 'oppor9plustma': 1702,\n",
       " '25252525252BM6': 5350,\n",
       " 'l3': 6475,\n",
       " '101A': 8062,\n",
       " 'gn3001': 2348,\n",
       " 'l02': 4229,\n",
       " '7251': 4405,\n",
       " '20T3': 8389,\n",
       " 'E8': 3622,\n",
       " 'built': 4977,\n",
       " 'Y105': 3425,\n",
       " 'A820t': 1932,\n",
       " 'A2001': 1626,\n",
       " '20M6': 2774,\n",
       " 'Y09': 3163,\n",
       " 'T8620': 8961,\n",
       " 'ZUK': 354,\n",
       " 'QK1505': 4394,\n",
       " '25252BY53': 7069,\n",
       " 'E717': 9581,\n",
       " '5270': 1376,\n",
       " 'N9109W': 979,\n",
       " '25252B5': 1035,\n",
       " 'a708t': 7007,\n",
       " 'A77t': 202,\n",
       " 'mla': 837,\n",
       " 'a628t': 5547,\n",
       " 'MD298ZP': 9345,\n",
       " '20J738M': 4959,\n",
       " '20U807': 7933,\n",
       " '20F11': 8798,\n",
       " '252525252525252BY83A': 2890,\n",
       " 'q7': 4471,\n",
       " 'A628t': 3463,\n",
       " '252BS109M': 6059,\n",
       " '2525252525252Bch3': 9146,\n",
       " 'E3T': 4368,\n",
       " 'M521': 9082,\n",
       " 'hwH30': 8918,\n",
       " 'ALP': 134,\n",
       " 'Y76': 2659,\n",
       " 'N900U': 6532,\n",
       " 'A1699': 859,\n",
       " '4.4': 2166,\n",
       " '2525252BX9Plus': 948,\n",
       " 'S33': 2102,\n",
       " 'LND': 172,\n",
       " '20L3C': 5693,\n",
       " '206': 703,\n",
       " 'Q505T': 3662,\n",
       " '252BX9': 778,\n",
       " 'i9508': 6164,\n",
       " 'm560': 7378,\n",
       " 'JingGang': 6825,\n",
       " 'u1555': 8513,\n",
       " '25252525252BY75s': 6467,\n",
       " 'G532G': 5113,\n",
       " '208670': 7156,\n",
       " 'Skyhon': 5121,\n",
       " 'S7278': 3119,\n",
       " 'oxygen': 4467,\n",
       " '20A79t': 2136,\n",
       " '7269': 6098,\n",
       " 'ATU': 324,\n",
       " 'A777': 8978,\n",
       " 'U59GT': 4730,\n",
       " '20R7st': 2984,\n",
       " '252525252BA53t': 7383,\n",
       " '252525252BY55A': 2904,\n",
       " 'ATMAN': 4496,\n",
       " 'C7108': 2947,\n",
       " 'Coolpad8729': 7934,\n",
       " '252525252BNOTE': 2617,\n",
       " '207.0': 9088,\n",
       " '252525252BR11st': 2829,\n",
       " '205721': 8702,\n",
       " 'V7': 2199,\n",
       " 'MP1605': 2152,\n",
       " 'i9100g': 9275,\n",
       " 'Nibiru': 6401,\n",
       " 'redminote5a': 8253,\n",
       " '2BX6SPlus': 6074,\n",
       " 'GM7': 8837,\n",
       " 'L6735': 2980,\n",
       " '252525252525252BX21i': 3518,\n",
       " 'A31c': 274,\n",
       " 'onea2001': 8227,\n",
       " '2525252525252BR9skt': 5184,\n",
       " '20A37t': 766,\n",
       " 'S810t': 1800,\n",
       " 'L35h': 8802,\n",
       " 'LT556': 5067,\n",
       " 's9': 1279,\n",
       " 'PI': 5133,\n",
       " 'TOOKY': 6156,\n",
       " '1011T': 5700,\n",
       " 'BA602T': 2177,\n",
       " 'cun': 1075,\n",
       " 'UNC': 1249,\n",
       " 'A500YZ': 8846,\n",
       " 'Non': 8335,\n",
       " 'M821': 646,\n",
       " 'CPH1819': 6123,\n",
       " 'A858': 9544,\n",
       " 'mi8se': 5801,\n",
       " 'x20': 2446,\n",
       " '11': 604,\n",
       " 'Pioneer': 2020,\n",
       " 'G9287': 1787,\n",
       " '25252BNEX': 3221,\n",
       " 'Coolpad5380CA': 6871,\n",
       " 'ZAW618': 8822,\n",
       " 'PGN610': 7482,\n",
       " 'vivox5v': 5945,\n",
       " 'G532M': 7765,\n",
       " 'A3': 869,\n",
       " 'm621c': 5145,\n",
       " 'a03l': 7177,\n",
       " 'B770': 711,\n",
       " 'B906': 5650,\n",
       " 'Q72901C': 8225,\n",
       " 'A1458': 4589,\n",
       " 'UKooo': 9624,\n",
       " 'Ben7': 3216,\n",
       " 'P98': 4156,\n",
       " '252BE': 2140,\n",
       " 'T2556': 8749,\n",
       " 'T950S': 6926,\n",
       " '2525252BY75s': 6579,\n",
       " 'P650A31': 5793,\n",
       " 'G900A': 8219,\n",
       " 'gn9007': 6072,\n",
       " '25252525252BY23L': 4998,\n",
       " 'yq603': 9513,\n",
       " 'M518': 9037,\n",
       " 'F1': 784,\n",
       " '2525252BA77t': 2769,\n",
       " '20Y55': 917,\n",
       " 'OPPOA77': 2579,\n",
       " 'NX573J': 746,\n",
       " '20L78011': 5689,\n",
       " 'L5M': 1226,\n",
       " 'm20': 5013,\n",
       " 'N2': 1696,\n",
       " 'natrium': 5791,\n",
       " 'PD1524B': 6111,\n",
       " 'vivox7plus': 2534,\n",
       " 'A7Pro': 3650,\n",
       " 'LEX728': 5826,\n",
       " 'r2': 6118,\n",
       " '252525252BS11L': 7860,\n",
       " 'C00': 797,\n",
       " '2525252BBA603': 7003,\n",
       " 'T001': 4881,\n",
       " 'E9000': 6045,\n",
       " 'HLJ6P': 3532,\n",
       " 'huaweigra': 3482,\n",
       " 'G928T': 5195,\n",
       " '252525252525252BA': 775,\n",
       " '4GVOLTE': 3411,\n",
       " 'Y685Q': 3004,\n",
       " 'M550': 5730,\n",
       " 'CoolpadA8': 7272,\n",
       " 'InFocusM5503D': 7741,\n",
       " '252525252BR11': 1356,\n",
       " 'XT1085': 2512,\n",
       " 's6': 3602,\n",
       " 'z2151': 3867,\n",
       " 'V410': 5709,\n",
       " 'H101': 6323,\n",
       " '208675': 1645,\n",
       " '2525252525252BPhone': 5917,\n",
       " 'A1530': 2090,\n",
       " 'honorh30': 7539,\n",
       " 'yeechui': 3859,\n",
       " 'V989': 4916,\n",
       " 'X7': 39,\n",
       " 'MZ4': 6229,\n",
       " '25252BD820u': 7309,\n",
       " 'YNBD': 6369,\n",
       " 'Z12': 6545,\n",
       " '252BTAG': 2290,\n",
       " '2525252BY83A': 2767,\n",
       " 'MX5': 218,\n",
       " '25252525252BX520': 7236,\n",
       " 'D9': 3129,\n",
       " 'a01': 358,\n",
       " 'G7105': 4361,\n",
       " '20D10w': 5660,\n",
       " 'e5': 7123,\n",
       " 'E6': 3363,\n",
       " '25252525252BT21': 7964,\n",
       " '701w': 4240,\n",
       " '2525252525252BPlustm': 2998,\n",
       " 'm16': 7472,\n",
       " 'N920F': 8117,\n",
       " 'xm50t': 8508,\n",
       " '25252525252BF6L': 7216,\n",
       " '2BTIT': 7692,\n",
       " 'V889M': 4045,\n",
       " 'V970': 3884,\n",
       " '2525252BY67': 1100,\n",
       " 'HLTE100M': 3861,\n",
       " 'PLK': 164,\n",
       " '252BY85A': 1777,\n",
       " '91': 5520,\n",
       " 'DOOVL5M': 9511,\n",
       " 'ali6737t': 9434,\n",
       " 'i8552': 5795,\n",
       " 'KOOMII': 1155,\n",
       " 'D8': 3954,\n",
       " 'vivi': 2524,\n",
       " 'N910V': 7238,\n",
       " '20Y11': 7872,\n",
       " '20MLA': 413,\n",
       " '20Y51t': 1523,\n",
       " 'H60': 233,\n",
       " 'NX611J': 1682,\n",
       " 'HILARY': 2134,\n",
       " 'LGM': 3915,\n",
       " 'Lezhou': 1700,\n",
       " 'E621T': 4701,\n",
       " 'C880S': 1573,\n",
       " 'S5': 1191,\n",
       " 'N888': 6116,\n",
       " '252525252525252BX7': 1061,\n",
       " 'htcd816t': 7708,\n",
       " 'OPPOA79kt': 4436,\n",
       " 'G900': 8168,\n",
       " '4S': 493,\n",
       " 'nex': 7244,\n",
       " 'SUPERJO': 8191,\n",
       " '252525252BRIO': 4164,\n",
       " 'T328w': 8685,\n",
       " 'PD1801': 7803,\n",
       " 'C6502': 8559,\n",
       " 'ztebv0730': 7437,\n",
       " '25252525252BX9Plus': 1748,\n",
       " '252525252BY53': 5158,\n",
       " 'Sofia': 6799,\n",
       " 'LenovoA3910e70': 6845,\n",
       " '2525252525252BY79': 4433,\n",
       " 'SUBOR': 2469,\n",
       " 'E602M': 4031,\n",
       " 'V411': 7347,\n",
       " 'CA100': 8522,\n",
       " '2525252BY67A': 1040,\n",
       " 'eH880': 4174,\n",
       " '9300': 3577,\n",
       " 'ZWX': 7119,\n",
       " 'k5': 4864,\n",
       " 'ztebv0710': 6883,\n",
       " 'G7108V': 824,\n",
       " 'f1': 9245,\n",
       " '252525252BX501': 6143,\n",
       " 'S820e': 5040,\n",
       " 'JSN': 6563,\n",
       " '252525252B3S': 3738,\n",
       " 'GN5007': 432,\n",
       " 'x520': 3916,\n",
       " 'A820e': 7038,\n",
       " 'HUAWEIC8817E': 9562,\n",
       " 'F2': 708,\n",
       " 'E620M': 5722,\n",
       " 'G600': 4639,\n",
       " 'TB2': 6307,\n",
       " 'S10L': 441,\n",
       " 'X909': 2912,\n",
       " 'pmi': 8139,\n",
       " 'DRA': 717,\n",
       " 'VCR': 1510,\n",
       " 'F6L': 576,\n",
       " 'HM1SW': 9548,\n",
       " 'MD': 6519,\n",
       " '96': 8834,\n",
       " 'QK1801': 9176,\n",
       " 'g6plus': 6794,\n",
       " '20Q529T': 3014,\n",
       " 'r11': 2190,\n",
       " 'Y3': 3869,\n",
       " 'RedmiNote5': 2367,\n",
       " 'g20': 6932,\n",
       " 'Q806T': 2739,\n",
       " '25252BK520': 7220,\n",
       " 'NX': 3064,\n",
       " '25252525252B1LTETD': 8766,\n",
       " '8076D': 7649,\n",
       " 'N916S': 5172,\n",
       " '20K1': 6817,\n",
       " '25252525252BY27': 5888,\n",
       " '252525252Bnote': 3676,\n",
       " 'LEX651': 1165,\n",
       " 'X60': 2182,\n",
       " 'T331C': 4308,\n",
       " '252525252525252BV': 5539,\n",
       " 'C902': 8784,\n",
       " 'mix2s': 5500,\n",
       " 'GN8002S': 277,\n",
       " 'BA601': 509,\n",
       " 'M199': 3320,\n",
       " 'HisenseF31': 5586,\n",
       " 'PD1401CL': 4396,\n",
       " 'giza': 7251,\n",
       " 'NX510J': 1794,\n",
       " 'f108': 7495,\n",
       " '252BT5': 9339,\n",
       " 'm2note': 1904,\n",
       " 'L1C': 5653,\n",
       " '20L5Pro': 7114,\n",
       " 'LZ6': 1826,\n",
       " '20151209T': 1019,\n",
       " 'P4': 2901,\n",
       " 'U11': 1936,\n",
       " 'vivox710l': 4839,\n",
       " 'HON': 4668,\n",
       " '2525252525252BS11': 6962,\n",
       " '2BX7': 2606,\n",
       " 'A31': 79,\n",
       " 'HWFRD': 3856,\n",
       " 'mi6': 2549,\n",
       " '252525252525252BA6000': 4736,\n",
       " 'InFocus': 2094,\n",
       " 'cl00': 1119,\n",
       " 'k2': 4482,\n",
       " 'I869': 2805,\n",
       " 'S1': 543,\n",
       " 'jd': 7598,\n",
       " 'R11': 28,\n",
       " 'M8St': 2619,\n",
       " 'F109N': 3462,\n",
       " 'gionees10': 8015,\n",
       " 's302m': 7393,\n",
       " '2BY75': 7902,\n",
       " 'xiaoR9': 6852,\n",
       " 'L5': 1329,\n",
       " 'Xplay5L': 7127,\n",
       " 'M652YN': 1042,\n",
       " '252525252525252BM9': 6741,\n",
       " 'LenovoS898t': 8515,\n",
       " 'LC': 3566,\n",
       " 'OZZO': 8897,\n",
       " 'mx6': 4853,\n",
       " 'SL600': 5476,\n",
       " '25252BX528': 5039,\n",
       " 'FS8016': 3743,\n",
       " 'X905Q': 2062,\n",
       " '20Y891': 6536,\n",
       " '20S301T': 5118,\n",
       " 'eg939': 8272,\n",
       " 'MX4Pro': 5155,\n",
       " 'lephoneT7': 5808,\n",
       " '252525252B7': 1906,\n",
       " '2525252BA37m': 908,\n",
       " 'Iris': 5082,\n",
       " 'DAXIAN': 5199,\n",
       " 'GN3002': 650,\n",
       " 'M7': 384,\n",
       " '20C70': 7227,\n",
       " 'PD1610': 7575,\n",
       " '20Y321': 4850,\n",
       " 'H930': 4388,\n",
       " '25252BX621': 5456,\n",
       " '2525252B4C': 6832,\n",
       " 'SMART': 7832,\n",
       " 'hol': 8705,\n",
       " 'Y67': 62,\n",
       " 'I9158V': 986,\n",
       " '20BA610T': 4011,\n",
       " 'vivoY51A': 1231,\n",
       " '2525252BX9i': 1653,\n",
       " 'eton': 7037,\n",
       " 'S19': 5594,\n",
       " 'L05': 2281,\n",
       " 'Forme': 2752,\n",
       " '25252525252BSE': 5643,\n",
       " '5892': 2823,\n",
       " 'LH801S': 9560,\n",
       " 'A18': 6281,\n",
       " 'KT107H': 3528,\n",
       " 'AXS': 6689,\n",
       " '������4G������': 8507,\n",
       " 'f106': 4200,\n",
       " 'OPPOR7sm': 2332,\n",
       " '2522OD103': 5706,\n",
       " 'M5Note': 1604,\n",
       " 'X6L': 320,\n",
       " 'J3110': 531,\n",
       " 'ONEPLUSA5000': 5809,\n",
       " 'lephoneW7': 7341,\n",
       " 'l100': 8133,\n",
       " '252BGN5007': 7017,\n",
       " 'K88L': 5869,\n",
       " 'LA5': 6202,\n",
       " 'NX505J': 2243,\n",
       " 'A8000': 402,\n",
       " 'U960S3': 8637,\n",
       " 'MUCH': 7515,\n",
       " 'P2': 1095,\n",
       " 'G71': 8332,\n",
       " '2525252525252BT11': 9375,\n",
       " 'A708t': 2400,\n",
       " '25252525252528GT20': 7946,\n",
       " 'H6': 1726,\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build NN model only use model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.41479"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T16:05:30.176698Z",
     "start_time": "2018-10-07T16:05:30.167231Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nn_model(cols,doc_cols=[],numu_cols=[]):\n",
    "    \"\"\"\n",
    "    cols, used to do ebd and dense layers\n",
    "    doc_cols: used to do rnn\n",
    "    there can be overlaps\n",
    "    \"\"\"\n",
    "    input_list = []\n",
    "    concat_list = []\n",
    "    numu_list = []\n",
    "    for col in cols:\n",
    "        max_feature = len(info_dict[prefix_input_nonDoc+col]['tok'].index_word)\n",
    "#         max_feature = len(set(info_dict[prefix_input_nonDoc+col]['tok'].index_word.values()))\n",
    "        embed_size = int(np.log2(max_feature)/np.log2(1.5))\n",
    "        if embed_size< 2:\n",
    "            embed_size = 2\n",
    "        cur_input = Input(shape=(1, ),name = prefix_input_nonDoc+col)\n",
    "        \n",
    "       \n",
    "        embed_layer = Embedding(max_feature,\n",
    "                            embed_size,\n",
    "                            input_length=1,\n",
    "                            trainable=True,\n",
    "                            embeddings_regularizer=l2(0.0005),\n",
    "                            name='ebd_'+col)(cur_input)\n",
    "        embed_layer = SpatialDropout1D(0.5)(embed_layer)\n",
    "        x = Flatten()(embed_layer)\n",
    "        input_list.append(cur_input)\n",
    "        concat_list.append(x)\n",
    "    for col in doc_cols:\n",
    "       \n",
    "        max_feature = len(info_dict[prefix_input_Doc+col]['tok'].index_word)\n",
    "#         max_feature = len(set(info_dict[prefix_input_Doc+col]['tok'].index_word.values()))\n",
    "        embed_size = int(np.log2(max_feature)/np.log2(1.5))\n",
    "        if embed_size< 2:\n",
    "            embed_size = 2\n",
    "        input_shape = sequence_size_dict[col]\n",
    "        cur_input = Input(shape=(input_shape, ),name = prefix_input_Doc+col)\n",
    "        if col == 'model':\n",
    "            embed_layer = Embedding(max_feature,\n",
    "                            embed_size,\n",
    "                            input_length=input_shape,\n",
    "                            trainable=True,\n",
    "                            embeddings_regularizer=l2(0.0005),\n",
    "                            name='ebd_rnn_'+col)(cur_input)\n",
    "            x = SpatialDropout1D(0.5)(embed_layer)\n",
    "            x = Bidirectional(CuDNNGRU(25, return_sequences=True))(x)\n",
    "            x = Conv1D(25, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "#             x2 = Conv1D(99, kernel_size = 2, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "            x_aveP = GlobalAveragePooling1D()(x)\n",
    "            x_maxP = GlobalMaxPooling1D()(x)\n",
    "#             x_aveP2 = GlobalAveragePooling1D()(x2)\n",
    "#             x_maxP2 = GlobalMaxPooling1D()(x2)\n",
    "            x = concatenate([x_aveP,x_maxP])\n",
    "        else:\n",
    "            embed_layer = Embedding(max_feature,\n",
    "                            embed_size,\n",
    "                            input_length=input_shape,\n",
    "                            trainable=True,\n",
    "                            embeddings_regularizer=l2(0.0005),\n",
    "                            name='ebd_rnn_'+col)(cur_input)\n",
    "            x = SpatialDropout1D(0.5)(embed_layer)\n",
    "            x = Bidirectional(CuDNNGRU(25, return_sequences=True))(x)\n",
    "            x = Conv1D(25, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "            x_aveP = GlobalAveragePooling1D()(x)\n",
    "            x_maxP = GlobalMaxPooling1D()(x)\n",
    "            x = concatenate([x_aveP,x_maxP])\n",
    "        input_list.append(cur_input)\n",
    "        concat_list.append(x)\n",
    "\n",
    "    if len(numu_cols) > 0:\n",
    "        print('add numu...')\n",
    "        nu_shape = len(numu_cols)\n",
    "        cur_input = Input(shape=(nu_shape, ),name = prefix_input_nu)\n",
    "        x = BatchNormalization()(cur_input)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        input_list.append(cur_input)\n",
    "        numu_list.append(x)\n",
    "       \n",
    "    if len(concat_list) > 1:\n",
    "        x = concatenate(concat_list)\n",
    "#     x = BatchNormalization()(x)\n",
    "    \n",
    "\n",
    "    if len(numu_list)>0:\n",
    "        x = concatenate([x]+numu_list)\n",
    "#         x = BatchNormalization()(x)\n",
    "        \n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    preds = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(input_list, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T16:05:30.185794Z",
     "start_time": "2018-10-07T16:05:30.178188Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# def get_nn_model(cols,doc_cols=[],numu_cols=[]):\n",
    "#     \"\"\"\n",
    "#     cols, used to do ebd and dense layers\n",
    "#     doc_cols: used to do rnn\n",
    "#     there can be overlaps\n",
    "#     \"\"\"\n",
    "#     input_list = []\n",
    "#     concat_list = []\n",
    "#     numu_list = []\n",
    "#     for col in cols:\n",
    "#         max_feature = len(info_dict[prefix_input_nonDoc+col]['tok'].index_word)\n",
    "#         embed_size = int(np.log2(max_feature)/np.log2(1.5))\n",
    "#         if embed_size< 2:\n",
    "#             embed_size = 2\n",
    "#         cur_input = Input(shape=(1, ),name = prefix_input_nonDoc+col)\n",
    "        \n",
    "       \n",
    "#         embed_layer = Embedding(max_feature,\n",
    "#                             embed_size,\n",
    "#                             input_length=1,\n",
    "#                             trainable=True,\n",
    "#                             embeddings_regularizer=l2(0.0005),\n",
    "#                             name='ebd_'+col)(cur_input)\n",
    "#         embed_layer = SpatialDropout1D(0.5)(embed_layer)\n",
    "#         x = Flatten()(embed_layer)\n",
    "#         input_list.append(cur_input)\n",
    "#         concat_list.append(x)\n",
    "#     for col in doc_cols:\n",
    "#         max_feature = len(info_dict[prefix_input_Doc+col]['tok'].index_word)\n",
    "#         embed_size = int(np.log2(max_feature)/np.log2(1.5))\n",
    "#         if embed_size< 2:\n",
    "#             embed_size = 2\n",
    "#         input_shape = sequence_size_dict[col]\n",
    "#         cur_input = Input(shape=(input_shape, ),name = prefix_input_Doc+col)\n",
    "#         embed_layer = Embedding(max_feature,\n",
    "#                             embed_size,\n",
    "#                             input_length=input_shape,\n",
    "#                             trainable=True,\n",
    "#                             embeddings_regularizer=l2(0.0005),\n",
    "#                             name='ebd_rnn_'+col)(cur_input)\n",
    "#         x = SpatialDropout1D(0.5)(embed_layer)\n",
    "#         x = Bidirectional(CuDNNGRU(256, return_sequences=True))(x)\n",
    "#         x = Conv1D(128, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "#         x_aveP = GlobalAveragePooling1D()(x)\n",
    "#         x_maxP = GlobalMaxPooling1D()(x)\n",
    "#         x = concatenate([x_aveP, x_maxP]) \n",
    "#         input_list.append(cur_input)\n",
    "#         concat_list.append(x)\n",
    "# #         concat_list.append(x)\n",
    "#     if len(numu_cols) > 0:\n",
    "#         print('add numu...')\n",
    "#         nu_shape = len(numu_cols)\n",
    "#         cur_input = Input(shape=(nu_shape, ),name = prefix_input_nu)\n",
    "#         x = BatchNormalization()(cur_input)\n",
    "#         x = Dense(128, activation='relu')(x)\n",
    "#         x = Dropout(0.2)(x)\n",
    "#         input_list.append(cur_input)\n",
    "#         numu_list.append(x)\n",
    "       \n",
    "#     if len(concat_list) > 1:\n",
    "#         x = concatenate(concat_list)\n",
    "# #     x = BatchNormalization()(x)\n",
    "#     x = Dense(512, activation='relu')(x)\n",
    "#     x = Dropout(0.2)(x)\n",
    "\n",
    "#     if len(numu_list)>0:\n",
    "#         x = concatenate([x]+numu_list)\n",
    "#     x = Dense(128, activation='relu')(x)\n",
    "#     x = Dropout(0.2)(x)\n",
    "\n",
    "#     preds = Dense(1, activation=\"sigmoid\")(x)\n",
    "#     model = Model(input_list, preds)\n",
    "#     model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T16:05:30.195923Z",
     "start_time": "2018-10-07T16:05:30.187349Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_each_fold(input_train_dict,input_val_dict,y_train,y_val,cols,doc_col=[],tolerance=30,train_batch_size=5000):\n",
    "    model = get_nn_model(cols,doc_col)\n",
    "    cur_to = 0\n",
    "    best_logloss = None\n",
    "    best_weights = None\n",
    "    count = 0\n",
    "    base_lr = 0.001\n",
    "    while True:\n",
    "        model.fit(input_train_dict, y_train, \n",
    "                  batch_size=train_batch_size, \n",
    "                  epochs=1,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  )\n",
    "        preds = model.predict(input_val_dict,5000,verbose=1)\n",
    "        logloss = log_loss(y_val,preds)\n",
    "        roc = roc_auc_score(y_val,preds)\n",
    "        print(logloss)\n",
    "        print(roc)\n",
    "        if best_logloss is None:\n",
    "            best_logloss = logloss\n",
    "            best_weights = model.get_weights()\n",
    "        else:\n",
    "            if best_logloss > logloss:\n",
    "                best_logloss = logloss\n",
    "                best_weights = model.get_weights()\n",
    "                cur_to = 0\n",
    "            else:\n",
    "                cur_to +=1\n",
    "        if cur_to == tolerance:\n",
    "            break\n",
    "        print('best logloss is: {}'.format(best_logloss))\n",
    "        print('remainning trial is: {}/{}'.format(cur_to,tolerance))\n",
    "        print('total epoch trained: {}'.format(count))\n",
    "        count+=1\n",
    "    model.set_weights(best_weights)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T16:05:30.213435Z",
     "start_time": "2018-10-07T16:05:30.197712Z"
    }
   },
   "outputs": [],
   "source": [
    "def nn_K_fold(train_fold_dict,\n",
    "              val_fold_dict,\n",
    "              train_fold_y,\n",
    "              val_fold_y,\n",
    "              test_input_dict,\n",
    "              val_index_list,\n",
    "              train_df,\n",
    "              test_df,\n",
    "              pred_col_name = 'predicted_score',\n",
    "              holdout_input_dict=None,\n",
    "              holdout_y=None,\n",
    "              holdout_index_list=None,\n",
    "              nondoc_cols=[],\n",
    "              doc_cols=[],\n",
    "              tolerance=30,\n",
    "              train_batch_size=5000,\n",
    "              preds_batch=5000):\n",
    "    \"\"\"\n",
    "    train_fold_dict: format, key - foldNum, value - nn input \n",
    "    train_fold_y: label for train fold, fotmat, key -foldNum, value - label\n",
    "    val_fold_dict: format, key - foldNum, value - nn input \n",
    "    val_fold_y: label for val fold, fotmat, key -foldNum, value - label\n",
    "    test_input_dict: format, key - foldNum, value - nn input \n",
    "    holdout_input_dict: Noneable, if none, not using holdout\n",
    "    holdout_y: Noneable, label for holdout\n",
    "    cols: all cols used to do ebd\n",
    "    doc_cols, cols that are used to do rnn\n",
    "    val_index_list: cv, going to predict and generate oof\n",
    "    holdout_index_list: going to predict on each fold\n",
    "    train_df: 'dataframe which only has id columns, which will be used to store oof prediction'\n",
    "    test_df: 'dataframe which only has id columns, which will be used to store test prediction'\n",
    "    \"\"\"\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    n_fold = len(train_fold_dict)\n",
    "    test_preds_list = []\n",
    "    val_score_list = []\n",
    "    hold_out_preds_list = []\n",
    "    holdout_score_list = []\n",
    "    train_score_list = []\n",
    "    cv_score_list = []\n",
    "    train_df[pred_col_name] = np.nan\n",
    "    test_df[pred_col_name] = np.nan\n",
    "    \n",
    "    for fold in range(n_fold):\n",
    "        print('start fold {}...'.format(fold))\n",
    "        model = train_each_fold(train_fold_dict[fold],\n",
    "                                val_fold_dict[fold],\n",
    "                                train_fold_y[fold],\n",
    "                                val_fold_y[fold],\n",
    "                                nondoc_cols,\n",
    "                                doc_cols,\n",
    "                                tolerance=tolerance,\n",
    "                                train_batch_size=train_batch_size)\n",
    "        train_preds =  model.predict(train_fold_dict[fold],preds_batch,verbose=1)\n",
    "        val_preds = model.predict(val_fold_dict[fold],preds_batch,verbose=1)\n",
    "        train_loss = log_loss(train_fold_y[fold],train_preds)\n",
    "        val_loss = log_loss(val_fold_y[fold],val_preds)\n",
    "        train_df.loc[val_index_list[fold],pred_col_name] = val_preds\n",
    "        test_preds = model.predict(test_input_dict,preds_batch,verbose=1)\n",
    "        test_preds_list.append(test_preds)\n",
    "        val_score_list.append(val_loss)\n",
    "        train_score_list.append(train_loss)\n",
    "        print('Fold {} finish! val loss: {}.'.format(fold,val_loss))\n",
    "        if holdout_index_list is not None:\n",
    "            ho_preds = model.predict(holdout_input_dict,preds_batch,verbose=1)\n",
    "            ho_loss = log_loss(holdout_y,ho_preds)\n",
    "            ho_roc = roc_auc_score(holdout_y,ho_preds)\n",
    "            holdout_score_list.append(ho_loss)\n",
    "            hold_out_preds_list.append(ho_preds)\n",
    "            print('hold out loss: {}'.format(ho_loss))\n",
    "        del model\n",
    "        gc.collect()\n",
    "        time.sleep(5)\n",
    "            \n",
    "    print('finish training... calculating evl matrix')\n",
    "    test_preds_list = np.array(test_preds_list)\n",
    "    hold_out_preds_list = np.array(hold_out_preds_list)\n",
    "    test_preds_final = np.mean(test_preds_list,axis=0)\n",
    "    cv_score_mean = np.mean(val_score_list)\n",
    "    train_score_mean = np.mean(train_score_list)\n",
    "    test_df[pred_col_name] = test_preds_final\n",
    "    print('cv mean is: {}'.format(cv_score_mean))\n",
    "    if holdout_index_list is not None:\n",
    "        ho_preds_final = np.mean(hold_out_preds_list,axis=0)\n",
    "        ho_loss_overall = log_loss(holdout_y,ho_preds_final)\n",
    "        ho_roc_overall = roc_auc_score(holdout_y,ho_preds_final)\n",
    "        train_df.loc[holdout_index_list,pred_col_name] = ho_preds_final\n",
    "        print('holdout loss overall is: {}'.format(ho_loss_overall))\n",
    "        print('holdout roc overall is: {}'.format(ho_roc_overall))\n",
    "        return train_df,test_df,cv_score_mean,ho_loss_overall,train_score_mean\n",
    "    else:\n",
    "        return train_df,test_df,cv_score_mean,0.0,train_score_mean\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-07T15:58:37.834Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fold 0...\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 18s 22us/step - loss: 0.4576 - acc: 0.8008\n",
      "200331/200331 [==============================] - 2s 9us/step\n",
      "0.42288232047659496\n",
      "0.7567416312670407\n",
      "best logloss is: 0.42288232047659496\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 0\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4263 - acc: 0.8041\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4204427296863615\n",
      "0.7622690396288728\n",
      "best logloss is: 0.4204427296863615\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 1\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4245 - acc: 0.8052\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41885247837939293\n",
      "0.7642076799163784\n",
      "best logloss is: 0.41885247837939293\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 2\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4238 - acc: 0.8054\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4179401273063227\n",
      "0.764901354172986\n",
      "best logloss is: 0.4179401273063227\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 3\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4232 - acc: 0.8054\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4178372056226309\n",
      "0.7657034102987207\n",
      "best logloss is: 0.4178372056226309\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 4\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4230 - acc: 0.8056\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.41705756690870766\n",
      "0.7657140017539813\n",
      "best logloss is: 0.41705756690870766\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 5\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4228 - acc: 0.8055\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41685288613177973\n",
      "0.7660212162209781\n",
      "best logloss is: 0.41685288613177973\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 6\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4226 - acc: 0.8056\n",
      "200331/200331 [==============================] - 2s 7us/step\n",
      "0.41681272063101116\n",
      "0.7661843904834891\n",
      "best logloss is: 0.41681272063101116\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 7\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4224 - acc: 0.8056\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.416773331102907\n",
      "0.7668381071039378\n",
      "best logloss is: 0.416773331102907\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 8\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4221 - acc: 0.8057\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41716376529940774\n",
      "0.7667852431851339\n",
      "best logloss is: 0.416773331102907\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 9\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4219 - acc: 0.8059\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4164050300692248\n",
      "0.767105060836184\n",
      "best logloss is: 0.4164050300692248\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 10\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4220 - acc: 0.8057\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41675373999946863\n",
      "0.7669683484904939\n",
      "best logloss is: 0.4164050300692248\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 11\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4219 - acc: 0.8057\n",
      "200331/200331 [==============================] - 2s 7us/step\n",
      "0.4164170925232457\n",
      "0.7678872171687692\n",
      "best logloss is: 0.4164050300692248\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 12\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4214 - acc: 0.8060\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.41695140736249175\n",
      "0.7671735645640023\n",
      "best logloss is: 0.4164050300692248\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 13\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4215 - acc: 0.8057\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4165234289725822\n",
      "0.7672464519433141\n",
      "best logloss is: 0.4164050300692248\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 14\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4213 - acc: 0.8058\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4164299800646373\n",
      "0.7676896173284792\n",
      "best logloss is: 0.4164050300692248\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 15\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4213 - acc: 0.8058\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4160868091472972\n",
      "0.7676179013038287\n",
      "best logloss is: 0.4160868091472972\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 16\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4212 - acc: 0.8061\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41639281434144654\n",
      "0.7674910232151602\n",
      "best logloss is: 0.4160868091472972\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 17\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4213 - acc: 0.8059\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4162030902707591\n",
      "0.7673145797348946\n",
      "best logloss is: 0.4160868091472972\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 18\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4212 - acc: 0.8061\n",
      "200331/200331 [==============================] - 2s 7us/step\n",
      "0.41595856569271067\n",
      "0.7674677199459375\n",
      "best logloss is: 0.41595856569271067\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 19\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4211 - acc: 0.8060\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41643749601828195\n",
      "0.7682191803798231\n",
      "best logloss is: 0.41595856569271067\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 20\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4211 - acc: 0.8062\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4160057207056831\n",
      "0.7682611759036713\n",
      "best logloss is: 0.41595856569271067\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 21\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4209 - acc: 0.8061\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4160533691586027\n",
      "0.7683366265415645\n",
      "best logloss is: 0.41595856569271067\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 22\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4210 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4160979285929748\n",
      "0.7682564014351307\n",
      "best logloss is: 0.41595856569271067\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 23\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4209 - acc: 0.8063\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4157044253927038\n",
      "0.7685417835617555\n",
      "best logloss is: 0.4157044253927038\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 24\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4207 - acc: 0.8064\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.415711448349728\n",
      "0.768701244044157\n",
      "best logloss is: 0.4157044253927038\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 25\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4208 - acc: 0.8064\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.41569665022954416\n",
      "0.7684268441115198\n",
      "best logloss is: 0.41569665022954416\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 26\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4208 - acc: 0.8062\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.415663453286662\n",
      "0.7685662075908872\n",
      "best logloss is: 0.415663453286662\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 27\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4204 - acc: 0.8065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4156544814857647\n",
      "0.7687897198545304\n",
      "best logloss is: 0.4156544814857647\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 28\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4205 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41555533642452036\n",
      "0.7689749232756979\n",
      "best logloss is: 0.41555533642452036\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 29\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4205 - acc: 0.8065\n",
      "200331/200331 [==============================] - 2s 7us/step\n",
      "0.41576672048786567\n",
      "0.7685199108082379\n",
      "best logloss is: 0.41555533642452036\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 30\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4205 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4158680669955739\n",
      "0.7685493834260835\n",
      "best logloss is: 0.41555533642452036\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 31\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4203 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.415234813785679\n",
      "0.7690170746564446\n",
      "best logloss is: 0.415234813785679\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 32\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4204 - acc: 0.8065\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4154164549165923\n",
      "0.7681980140731524\n",
      "best logloss is: 0.415234813785679\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 33\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4203 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41582865187588475\n",
      "0.7691507907119991\n",
      "best logloss is: 0.415234813785679\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 34\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4201 - acc: 0.8067\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4154354629937873\n",
      "0.7692764369046083\n",
      "best logloss is: 0.415234813785679\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 35\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4203 - acc: 0.8066\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.41527402074144515\n",
      "0.7686898614780758\n",
      "best logloss is: 0.415234813785679\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 36\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4202 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4156529830390956\n",
      "0.769136008037607\n",
      "best logloss is: 0.415234813785679\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 37\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4201 - acc: 0.8068\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41530339732840055\n",
      "0.7692665197066543\n",
      "best logloss is: 0.415234813785679\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 38\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4201 - acc: 0.8067\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.41528782391463265\n",
      "0.7689292344132732\n",
      "best logloss is: 0.415234813785679\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 39\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4201 - acc: 0.8066\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.41555576954505674\n",
      "0.7691686718058715\n",
      "best logloss is: 0.415234813785679\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 40\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4200 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41590018356116476\n",
      "0.7688703643648296\n",
      "best logloss is: 0.415234813785679\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 41\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4201 - acc: 0.8067\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41542748990507805\n",
      "0.7689552401941256\n",
      "best logloss is: 0.415234813785679\n",
      "remainning trial is: 10/30\n",
      "total epoch trained: 42\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4200 - acc: 0.8068\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41497630463338897\n",
      "0.7693301392395425\n",
      "best logloss is: 0.41497630463338897\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 43\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4199 - acc: 0.8069\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.415310070847026\n",
      "0.7687633457344851\n",
      "best logloss is: 0.41497630463338897\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 44\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4199 - acc: 0.8067\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41515108779794824\n",
      "0.769251562926792\n",
      "best logloss is: 0.41497630463338897\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 45\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4198 - acc: 0.8066\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4150628668518265\n",
      "0.7690438810248436\n",
      "best logloss is: 0.41497630463338897\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 46\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4199 - acc: 0.8068\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4151727501462376\n",
      "0.7696105347286126\n",
      "best logloss is: 0.41497630463338897\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 47\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4198 - acc: 0.8069\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.415048380598899\n",
      "0.7693379465736818\n",
      "best logloss is: 0.41497630463338897\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 48\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4198 - acc: 0.8068\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4154620866541737\n",
      "0.7694252084232874\n",
      "best logloss is: 0.41497630463338897\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 49\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4196 - acc: 0.8069\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4150269756882722\n",
      "0.7692439549171788\n",
      "best logloss is: 0.41497630463338897\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 50\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4198 - acc: 0.8069\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.41529048839348903\n",
      "0.7689988184095351\n",
      "best logloss is: 0.41497630463338897\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 51\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4197 - acc: 0.8070\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4153156513650941\n",
      "0.7691256627422622\n",
      "best logloss is: 0.41497630463338897\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 52\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4195 - acc: 0.8069\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.415373309373948\n",
      "0.7692460557741878\n",
      "best logloss is: 0.41497630463338897\n",
      "remainning trial is: 10/30\n",
      "total epoch trained: 53\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4196 - acc: 0.8068\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4152188641428808\n",
      "0.769378054819296\n",
      "best logloss is: 0.41497630463338897\n",
      "remainning trial is: 11/30\n",
      "total epoch trained: 54\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4196 - acc: 0.8070\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4148918328301579\n",
      "0.7696677701593022\n",
      "best logloss is: 0.4148918328301579\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 55\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4196 - acc: 0.8069\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4150858219883521\n",
      "0.7694259467463944\n",
      "best logloss is: 0.4148918328301579\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 56\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4196 - acc: 0.8069\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4154945334712687\n",
      "0.7692848570148152\n",
      "best logloss is: 0.4148918328301579\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 57\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4194 - acc: 0.8067\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4149831201263897\n",
      "0.7695237792181274\n",
      "best logloss is: 0.4148918328301579\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 58\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4194 - acc: 0.8070\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4150787385570713\n",
      "0.7697496594101487\n",
      "best logloss is: 0.4148918328301579\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 59\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4194 - acc: 0.8070\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4153239939515792\n",
      "0.7693779880122937\n",
      "best logloss is: 0.4148918328301579\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 60\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4195 - acc: 0.8071\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41521213087537745\n",
      "0.7695573811039453\n",
      "best logloss is: 0.4148918328301579\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 61\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4193 - acc: 0.8071\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4150479517142765\n",
      "0.7694900605571156\n",
      "best logloss is: 0.4148918328301579\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 62\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4194 - acc: 0.8072\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4150750738021944\n",
      "0.7695473776756169\n",
      "best logloss is: 0.4148918328301579\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 63\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4194 - acc: 0.8071\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.41541694639382154\n",
      "0.7691813996754436\n",
      "best logloss is: 0.4148918328301579\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 64\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4193 - acc: 0.8070\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4147729287110368\n",
      "0.7699080812903731\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 65\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4192 - acc: 0.8069\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4148883829234044\n",
      "0.7698111299975177\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 66\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4193 - acc: 0.8074\n",
      "200331/200331 [==============================] - 2s 7us/step\n",
      "0.41497980389183464\n",
      "0.769554578029372\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 67\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8069\n",
      "200331/200331 [==============================] - 2s 7us/step\n",
      "0.4149616237124078\n",
      "0.7695348701203439\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 68\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8069\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4151243070177458\n",
      "0.7692756110196858\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 69\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8071\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4148924141366357\n",
      "0.7695658287044898\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 70\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4190 - acc: 0.8072\n",
      "200331/200331 [==============================] - 2s 7us/step\n",
      "0.4151740612693722\n",
      "0.7693645032577627\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 71\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8071\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.41505980455329633\n",
      "0.7696321591292532\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 72\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4192 - acc: 0.8070\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41533691812151735\n",
      "0.769421760461426\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 73\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4190 - acc: 0.8073\n",
      "200331/200331 [==============================] - 2s 7us/step\n",
      "0.4149151463990752\n",
      "0.76955370538738\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 74\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8071\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41499192504993154\n",
      "0.7698556825143434\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 10/30\n",
      "total epoch trained: 75\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4190 - acc: 0.8071\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41495602768592454\n",
      "0.769367092831881\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 11/30\n",
      "total epoch trained: 76\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4190 - acc: 0.8071\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4155394882807566\n",
      "0.7691326268827479\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 12/30\n",
      "total epoch trained: 77\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4189 - acc: 0.8072\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4149978975601778\n",
      "0.7692952482840292\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 13/30\n",
      "total epoch trained: 78\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4190 - acc: 0.8072\n",
      "200331/200331 [==============================] - 2s 7us/step\n",
      "0.41506178475492544\n",
      "0.7695831121380639\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 14/30\n",
      "total epoch trained: 79\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8073\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4155819742847187\n",
      "0.7693215944438169\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 15/30\n",
      "total epoch trained: 80\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4190 - acc: 0.8073\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4155703033241418\n",
      "0.7694010411272781\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 16/30\n",
      "total epoch trained: 81\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4190 - acc: 0.8072\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4153486176907342\n",
      "0.7695110205687758\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 17/30\n",
      "total epoch trained: 82\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4187 - acc: 0.8073\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41508072570713783\n",
      "0.7693081142318486\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 18/30\n",
      "total epoch trained: 83\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4187 - acc: 0.8073\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.415276611480425\n",
      "0.7694095897606864\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 19/30\n",
      "total epoch trained: 84\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4186 - acc: 0.8073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200331/200331 [==============================] - 2s 7us/step\n",
      "0.4151502359314743\n",
      "0.7691822999644132\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 20/30\n",
      "total epoch trained: 85\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4188 - acc: 0.8073\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4150034297151405\n",
      "0.7694738273170857\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 21/30\n",
      "total epoch trained: 86\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4187 - acc: 0.8074\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4150508543450646\n",
      "0.7696737994716765\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 22/30\n",
      "total epoch trained: 87\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4189 - acc: 0.8073\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4150572583281296\n",
      "0.7696432255956143\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 23/30\n",
      "total epoch trained: 88\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4187 - acc: 0.8074\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41521612094616384\n",
      "0.7692187414219476\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 24/30\n",
      "total epoch trained: 89\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4185 - acc: 0.8074\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41506312125960754\n",
      "0.7694613742725674\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 25/30\n",
      "total epoch trained: 90\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4187 - acc: 0.8074\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41507904531807194\n",
      "0.7694231676377563\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 26/30\n",
      "total epoch trained: 91\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4187 - acc: 0.8075\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4151119212765296\n",
      "0.7695944561706806\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 27/30\n",
      "total epoch trained: 92\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4187 - acc: 0.8075\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4152310126388912\n",
      "0.7692801018130069\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 28/30\n",
      "total epoch trained: 93\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4187 - acc: 0.8072\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41515188521629676\n",
      "0.7694057848160392\n",
      "best logloss is: 0.4147729287110368\n",
      "remainning trial is: 29/30\n",
      "total epoch trained: 94\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4187 - acc: 0.8074\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4152098474764245\n",
      "0.7694947990767533\n",
      "801319/801319 [==============================] - 6s 7us/step\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "40024/40024 [==============================] - 0s 5us/step\n",
      "Fold 0 finish! val loss: 0.4147729287110368.\n",
      "start fold 1...\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 17s 21us/step - loss: 0.4599 - acc: 0.7998\n",
      "200331/200331 [==============================] - 2s 10us/step\n",
      "0.42125378211070263\n",
      "0.7606753675183284\n",
      "best logloss is: 0.42125378211070263\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 0\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4266 - acc: 0.8042\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.418380344007057\n",
      "0.764504478913266\n",
      "best logloss is: 0.418380344007057\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 1\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4249 - acc: 0.8048\n",
      "200331/200331 [==============================] - 2s 7us/step\n",
      "0.41838297123244145\n",
      "0.7661209660458441\n",
      "best logloss is: 0.418380344007057\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 2\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4240 - acc: 0.8050\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4179052097225436\n",
      "0.7676240448068352\n",
      "best logloss is: 0.4179052097225436\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 3\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4237 - acc: 0.8051\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41824181801727744\n",
      "0.7678392975946933\n",
      "best logloss is: 0.4179052097225436\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 4\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4233 - acc: 0.8054\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4162271844455735\n",
      "0.7681180829801846\n",
      "best logloss is: 0.4162271844455735\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 5\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4230 - acc: 0.8054\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41649976573973824\n",
      "0.7683857363454334\n",
      "best logloss is: 0.4162271844455735\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 6\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4229 - acc: 0.8054\n",
      "200331/200331 [==============================] - 2s 7us/step\n",
      "0.41602810044820593\n",
      "0.7685764671257778\n",
      "best logloss is: 0.41602810044820593\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 7\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4226 - acc: 0.8053\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41636512990367264\n",
      "0.7684451564355852\n",
      "best logloss is: 0.41602810044820593\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 8\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4225 - acc: 0.8056\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4157368891705098\n",
      "0.7691712833879236\n",
      "best logloss is: 0.4157368891705098\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 9\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4224 - acc: 0.8056\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4160087963480072\n",
      "0.7693312129291018\n",
      "best logloss is: 0.4157368891705098\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 10\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4222 - acc: 0.8056\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4164482265685888\n",
      "0.7688090385129123\n",
      "best logloss is: 0.4157368891705098\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 11\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4220 - acc: 0.8058\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41580819603573294\n",
      "0.7697331786004422\n",
      "best logloss is: 0.4157368891705098\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 12\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4219 - acc: 0.8059\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41516732610289586\n",
      "0.7703329807169425\n",
      "best logloss is: 0.41516732610289586\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 13\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4219 - acc: 0.8058\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4160056306481288\n",
      "0.7697148264114719\n",
      "best logloss is: 0.41516732610289586\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 14\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4218 - acc: 0.8058\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4157250129144923\n",
      "0.7696506559753551\n",
      "best logloss is: 0.41516732610289586\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 15\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4216 - acc: 0.8060\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.41563917943336715\n",
      "0.770039139085126\n",
      "best logloss is: 0.41516732610289586\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 16\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4217 - acc: 0.8059\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4150337142273582\n",
      "0.7704050383736492\n",
      "best logloss is: 0.4150337142273582\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 17\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4215 - acc: 0.8062\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4152266443965976\n",
      "0.7701898444824684\n",
      "best logloss is: 0.4150337142273582\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 18\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4216 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4148163671741365\n",
      "0.7706054726165323\n",
      "best logloss is: 0.4148163671741365\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 19\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4214 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41480642144099283\n",
      "0.7704931333919418\n",
      "best logloss is: 0.41480642144099283\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 20\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4213 - acc: 0.8063\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4149799105107754\n",
      "0.7706223006973385\n",
      "best logloss is: 0.41480642144099283\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 21\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4212 - acc: 0.8060\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4152438237349322\n",
      "0.7704130454239148\n",
      "best logloss is: 0.41480642144099283\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 22\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4211 - acc: 0.8062\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4148733111776474\n",
      "0.7703636526497069\n",
      "best logloss is: 0.41480642144099283\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 23\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4211 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41502623170540676\n",
      "0.7707252286714952\n",
      "best logloss is: 0.41480642144099283\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 24\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4209 - acc: 0.8062\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.41462350113389124\n",
      "0.7709337359101255\n",
      "best logloss is: 0.41462350113389124\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 25\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4211 - acc: 0.8063\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4151591615725211\n",
      "0.7708415840699064\n",
      "best logloss is: 0.41462350113389124\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 26\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4210 - acc: 0.8064\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.41477945432710106\n",
      "0.7710740362539255\n",
      "best logloss is: 0.41462350113389124\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 27\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4210 - acc: 0.8062\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.414791519001826\n",
      "0.7708637495054491\n",
      "best logloss is: 0.41462350113389124\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 28\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4208 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4146931313513073\n",
      "0.7708415775693424\n",
      "best logloss is: 0.41462350113389124\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 29\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4207 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4146579546490562\n",
      "0.7708188183111839\n",
      "best logloss is: 0.41462350113389124\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 30\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4205 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41495069636374854\n",
      "0.7705456491781255\n",
      "best logloss is: 0.41462350113389124\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 31\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4205 - acc: 0.8064\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.41462550268399684\n",
      "0.7714317427953458\n",
      "best logloss is: 0.41462350113389124\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 32\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4206 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41451807391263146\n",
      "0.7711704464335587\n",
      "best logloss is: 0.41451807391263146\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 33\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4204 - acc: 0.8064\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.415175137456858\n",
      "0.770804979863309\n",
      "best logloss is: 0.41451807391263146\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 34\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4205 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41498249613472526\n",
      "0.7702544401181344\n",
      "best logloss is: 0.41451807391263146\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 35\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4203 - acc: 0.8068\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4148146524456588\n",
      "0.771243241708492\n",
      "best logloss is: 0.41451807391263146\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 36\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4203 - acc: 0.8066\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.41482093049010077\n",
      "0.7709793827930036\n",
      "best logloss is: 0.41451807391263146\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 37\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4203 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41464935314072027\n",
      "0.7709113371591552\n",
      "best logloss is: 0.41451807391263146\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 38\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4203 - acc: 0.8065\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.414744272375787\n",
      "0.7714538133069884\n",
      "best logloss is: 0.41451807391263146\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 39\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4202 - acc: 0.8067\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4149120176166554\n",
      "0.7706276317864711\n",
      "best logloss is: 0.41451807391263146\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 40\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4204 - acc: 0.8065\n",
      "200331/200331 [==============================] - 2s 7us/step\n",
      "0.41433638668305567\n",
      "0.7714226045686046\n",
      "best logloss is: 0.41433638668305567\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 41\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4201 - acc: 0.8065\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4144327270967371\n",
      "0.7712542551521797\n",
      "best logloss is: 0.41433638668305567\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 42\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4200 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4145479331387943\n",
      "0.7710387601987283\n",
      "best logloss is: 0.41433638668305567\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 43\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4200 - acc: 0.8068\n",
      "200331/200331 [==============================] - 2s 7us/step\n",
      "0.4146633879488742\n",
      "0.7710466593239658\n",
      "best logloss is: 0.41433638668305567\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 44\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4200 - acc: 0.8066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41454432794658475\n",
      "0.7714170483093294\n",
      "best logloss is: 0.41433638668305567\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 45\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4199 - acc: 0.8066\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41455918379866885\n",
      "0.7712747235488888\n",
      "best logloss is: 0.41433638668305567\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 46\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4200 - acc: 0.8068\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4142972286849565\n",
      "0.7714678698762013\n",
      "best logloss is: 0.4142972286849565\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 47\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4200 - acc: 0.8068\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41461275789831875\n",
      "0.7709319279701018\n",
      "best logloss is: 0.4142972286849565\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 48\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4198 - acc: 0.8068\n",
      "200331/200331 [==============================] - 2s 7us/step\n",
      "0.4143891137662101\n",
      "0.7714476550798126\n",
      "best logloss is: 0.4142972286849565\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 49\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4199 - acc: 0.8069\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4147897265481436\n",
      "0.7710258300284683\n",
      "best logloss is: 0.4142972286849565\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 50\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4199 - acc: 0.8068\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4144693304383582\n",
      "0.7714407300993515\n",
      "best logloss is: 0.4142972286849565\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 51\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4198 - acc: 0.8067\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.41471399808984794\n",
      "0.7712061783114441\n",
      "best logloss is: 0.4142972286849565\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 52\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4200 - acc: 0.8067\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4147858764055145\n",
      "0.7713090840427067\n",
      "best logloss is: 0.4142972286849565\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 53\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4197 - acc: 0.8070\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41457766617918346\n",
      "0.7711861039429042\n",
      "best logloss is: 0.4142972286849565\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 54\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4196 - acc: 0.8069\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41451367060327426\n",
      "0.7709841098779142\n",
      "best logloss is: 0.4142972286849565\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 55\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4197 - acc: 0.8069\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41469614651608033\n",
      "0.7712406336508421\n",
      "best logloss is: 0.4142972286849565\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 56\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4197 - acc: 0.8070\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41450199746177824\n",
      "0.7708845626670165\n",
      "best logloss is: 0.4142972286849565\n",
      "remainning trial is: 10/30\n",
      "total epoch trained: 57\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4197 - acc: 0.8070\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4143904284478815\n",
      "0.7711115886813628\n",
      "best logloss is: 0.4142972286849565\n",
      "remainning trial is: 11/30\n",
      "total epoch trained: 58\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4198 - acc: 0.8068\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4144138988675199\n",
      "0.7713262130291432\n",
      "best logloss is: 0.4142972286849565\n",
      "remainning trial is: 12/30\n",
      "total epoch trained: 59\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4196 - acc: 0.8068\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41419013165911656\n",
      "0.7714839977757646\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 60\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4197 - acc: 0.8067\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4147549985241614\n",
      "0.7710435115628543\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 61\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4195 - acc: 0.8069\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4142492849338053\n",
      "0.7715359246734956\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 62\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4195 - acc: 0.8069\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4146112556536714\n",
      "0.7714836428292999\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 63\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4195 - acc: 0.8070\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41500623361577466\n",
      "0.770902006343412\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 64\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4195 - acc: 0.8070\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4146560414863862\n",
      "0.7710771381194879\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 65\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4193 - acc: 0.8069\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4143721266508974\n",
      "0.7713006790482693\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 66\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4195 - acc: 0.8070\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4145204299851917\n",
      "0.7711485038968768\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 67\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4193 - acc: 0.8069\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4145095336205183\n",
      "0.7711457936315622\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 68\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4193 - acc: 0.8069\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4143109723600713\n",
      "0.7711294325732115\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 69\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4192 - acc: 0.8070\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41458584823679234\n",
      "0.7712548504628762\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 10/30\n",
      "total epoch trained: 70\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4194 - acc: 0.8070\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4142478281959958\n",
      "0.7714369115270157\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 11/30\n",
      "total epoch trained: 71\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4192 - acc: 0.8071\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4147352207701969\n",
      "0.7708791132362889\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 12/30\n",
      "total epoch trained: 72\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4193 - acc: 0.8070\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41435411236677355\n",
      "0.7713767340036648\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total epoch trained: 73\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8072\n",
      "200331/200331 [==============================] - 2s 7us/step\n",
      "0.41493982440746224\n",
      "0.7702735733146389\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 14/30\n",
      "total epoch trained: 74\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4194 - acc: 0.8072\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4142946090078716\n",
      "0.7716178447824418\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 15/30\n",
      "total epoch trained: 75\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4192 - acc: 0.8072\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4143245511927012\n",
      "0.7714627524441637\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 16/30\n",
      "total epoch trained: 76\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8070\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.41465132986702624\n",
      "0.7712543767832165\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 17/30\n",
      "total epoch trained: 77\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4193 - acc: 0.8071\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4142626781493929\n",
      "0.7714778043045661\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 18/30\n",
      "total epoch trained: 78\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8070\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4147125959020064\n",
      "0.7710021378218794\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 19/30\n",
      "total epoch trained: 79\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8071\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.41469559853993315\n",
      "0.7711005118767551\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 20/30\n",
      "total epoch trained: 80\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4192 - acc: 0.8071\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4145112663271887\n",
      "0.7708639821943164\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 21/30\n",
      "total epoch trained: 81\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4190 - acc: 0.8072\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4144350647902982\n",
      "0.7712210322570814\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 22/30\n",
      "total epoch trained: 82\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4190 - acc: 0.8072\n",
      "200331/200331 [==============================] - 2s 7us/step\n",
      "0.4147385942594591\n",
      "0.7707992448776808\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 23/30\n",
      "total epoch trained: 83\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4189 - acc: 0.8072\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.4144618957340191\n",
      "0.7713430770588521\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 24/30\n",
      "total epoch trained: 84\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4190 - acc: 0.8071\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.41471599168984646\n",
      "0.7711047357553473\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 25/30\n",
      "total epoch trained: 85\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4190 - acc: 0.8073\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4147236472242059\n",
      "0.7710729935007858\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 26/30\n",
      "total epoch trained: 86\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8071\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4144408810486638\n",
      "0.7714254165716635\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 27/30\n",
      "total epoch trained: 87\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4189 - acc: 0.8074\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4145248065319084\n",
      "0.7711523977347803\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 28/30\n",
      "total epoch trained: 88\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4192 - acc: 0.8074\n",
      "200331/200331 [==============================] - 2s 8us/step\n",
      "0.41455319421452724\n",
      "0.7712004572667848\n",
      "best logloss is: 0.41419013165911656\n",
      "remainning trial is: 29/30\n",
      "total epoch trained: 89\n",
      "Epoch 1/1\n",
      "801319/801319 [==============================] - 15s 19us/step - loss: 0.4189 - acc: 0.8071\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "0.4144762110842548\n",
      "0.7713964462986157\n",
      "801319/801319 [==============================] - 6s 7us/step\n",
      "200331/200331 [==============================] - 1s 7us/step\n",
      "40024/40024 [==============================] - 0s 6us/step\n",
      "Fold 1 finish! val loss: 0.41419013165911656.\n",
      "start fold 2...\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 18s 22us/step - loss: 0.4605 - acc: 0.7994\n",
      "200330/200330 [==============================] - 2s 11us/step\n",
      "0.4231708821177587\n",
      "0.7568923834184271\n",
      "best logloss is: 0.4231708821177587\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 0\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4261 - acc: 0.8043\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.42109116700869853\n",
      "0.7605896623053723\n",
      "best logloss is: 0.42109116700869853\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 1\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4243 - acc: 0.8050\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4232549847877623\n",
      "0.7621306019077072\n",
      "best logloss is: 0.42109116700869853\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 2\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4235 - acc: 0.8052\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41982771288667237\n",
      "0.7619339603797454\n",
      "best logloss is: 0.41982771288667237\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 3\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4229 - acc: 0.8055\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41998993153890196\n",
      "0.7625488089099699\n",
      "best logloss is: 0.41982771288667237\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 4\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4226 - acc: 0.8055\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.41863126218685126\n",
      "0.7637637106276439\n",
      "best logloss is: 0.41863126218685126\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 5\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4223 - acc: 0.8056\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4187299338095344\n",
      "0.7639833437410791\n",
      "best logloss is: 0.41863126218685126\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 6\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4221 - acc: 0.8059\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41988904752195055\n",
      "0.7642740097569634\n",
      "best logloss is: 0.41863126218685126\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 7\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4220 - acc: 0.8057\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4191936392150568\n",
      "0.765079790061605\n",
      "best logloss is: 0.41863126218685126\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 8\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4219 - acc: 0.8059\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4182122453073947\n",
      "0.764673375849177\n",
      "best logloss is: 0.4182122453073947\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 9\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4217 - acc: 0.8058\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.41838915890271766\n",
      "0.7650217864187128\n",
      "best logloss is: 0.4182122453073947\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 10\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4215 - acc: 0.8060\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4181487551740135\n",
      "0.7650379063434817\n",
      "best logloss is: 0.4181487551740135\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 11\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4214 - acc: 0.8059\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4184128921030285\n",
      "0.7646411874572059\n",
      "best logloss is: 0.4181487551740135\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 12\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4213 - acc: 0.8060\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4178842558637949\n",
      "0.7649106002448399\n",
      "best logloss is: 0.4178842558637949\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 13\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4212 - acc: 0.8062\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41792463406195185\n",
      "0.7650586492254527\n",
      "best logloss is: 0.4178842558637949\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 14\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4212 - acc: 0.8061\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4180827828691874\n",
      "0.7651642373329808\n",
      "best logloss is: 0.4178842558637949\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 15\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4210 - acc: 0.8061\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4187876467219774\n",
      "0.7653405596227751\n",
      "best logloss is: 0.4178842558637949\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 16\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4208 - acc: 0.8063\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.41765684393282443\n",
      "0.7654644631779232\n",
      "best logloss is: 0.41765684393282443\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 17\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4209 - acc: 0.8064\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4176015634833632\n",
      "0.7661483789628729\n",
      "best logloss is: 0.4176015634833632\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 18\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4208 - acc: 0.8065\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4178578979693714\n",
      "0.7652257113462357\n",
      "best logloss is: 0.4176015634833632\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 19\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4207 - acc: 0.8063\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4178246130090801\n",
      "0.7655473664090062\n",
      "best logloss is: 0.4176015634833632\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 20\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4208 - acc: 0.8065\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41763948864406275\n",
      "0.7659890816087829\n",
      "best logloss is: 0.4176015634833632\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 21\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4205 - acc: 0.8066\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41805371894447924\n",
      "0.7656581342649728\n",
      "best logloss is: 0.4176015634833632\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 22\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4205 - acc: 0.8066\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4173934433203101\n",
      "0.7664351887941261\n",
      "best logloss is: 0.4173934433203101\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 23\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4204 - acc: 0.8064\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4174810378367841\n",
      "0.7662400745455448\n",
      "best logloss is: 0.4173934433203101\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 24\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4203 - acc: 0.8066\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.41740198287107394\n",
      "0.7660954316329166\n",
      "best logloss is: 0.4173934433203101\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 25\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4203 - acc: 0.8066\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41767601260611487\n",
      "0.7662551850561262\n",
      "best logloss is: 0.4173934433203101\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 26\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4203 - acc: 0.8066\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41769702680017506\n",
      "0.7664159857230587\n",
      "best logloss is: 0.4173934433203101\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 27\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4203 - acc: 0.8065\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.41744848039061205\n",
      "0.7664692591075821\n",
      "best logloss is: 0.4173934433203101\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 28\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4201 - acc: 0.8068\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.41710698382697664\n",
      "0.7666992806187201\n",
      "best logloss is: 0.41710698382697664\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 29\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4200 - acc: 0.8068\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41702186302715344\n",
      "0.7671136314363518\n",
      "best logloss is: 0.41702186302715344\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 30\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4201 - acc: 0.8066\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4171598592304619\n",
      "0.7665209634670886\n",
      "best logloss is: 0.41702186302715344\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 31\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4200 - acc: 0.8064\n",
      "200330/200330 [==============================] - 2s 7us/step\n",
      "0.41746698184900227\n",
      "0.7665594153492825\n",
      "best logloss is: 0.41702186302715344\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 32\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4199 - acc: 0.8069\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4172338942599836\n",
      "0.7665408346249823\n",
      "best logloss is: 0.41702186302715344\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 33\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4201 - acc: 0.8068\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4169482283792608\n",
      "0.7667120619890851\n",
      "best logloss is: 0.4169482283792608\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 34\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4199 - acc: 0.8067\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41728095316032127\n",
      "0.7667520763955685\n",
      "best logloss is: 0.4169482283792608\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 35\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4198 - acc: 0.8069\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.416949285094055\n",
      "0.7669658489855713\n",
      "best logloss is: 0.4169482283792608\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 36\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4198 - acc: 0.8071\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4170970001628552\n",
      "0.7666510909598396\n",
      "best logloss is: 0.4169482283792608\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 37\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4198 - acc: 0.8069\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.417490138910964\n",
      "0.7667434536111519\n",
      "best logloss is: 0.4169482283792608\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 38\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4197 - acc: 0.8070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41742549854246247\n",
      "0.7665072353401516\n",
      "best logloss is: 0.4169482283792608\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 39\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4197 - acc: 0.8070\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.416806440405669\n",
      "0.7672205436559416\n",
      "best logloss is: 0.416806440405669\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 40\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4196 - acc: 0.8071\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41705703435103064\n",
      "0.7669066500703393\n",
      "best logloss is: 0.416806440405669\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 41\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4195 - acc: 0.8068\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.41717249769302883\n",
      "0.7668476495447827\n",
      "best logloss is: 0.416806440405669\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 42\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4196 - acc: 0.8070\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41695119805212316\n",
      "0.7672893916090122\n",
      "best logloss is: 0.416806440405669\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 43\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4197 - acc: 0.8069\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41682228918291964\n",
      "0.7671491373141037\n",
      "best logloss is: 0.416806440405669\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 44\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4195 - acc: 0.8071\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4166231160824659\n",
      "0.7676964803306723\n",
      "best logloss is: 0.4166231160824659\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 45\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4195 - acc: 0.8071\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4170399465429125\n",
      "0.7669682187749158\n",
      "best logloss is: 0.4166231160824659\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 46\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4195 - acc: 0.8071\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.41714432744009605\n",
      "0.7669025925197963\n",
      "best logloss is: 0.4166231160824659\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 47\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4194 - acc: 0.8071\n",
      "200330/200330 [==============================] - 2s 7us/step\n",
      "0.4169571897066695\n",
      "0.76668731387568\n",
      "best logloss is: 0.4166231160824659\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 48\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4193 - acc: 0.8073\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4172498355719399\n",
      "0.7664145771015378\n",
      "best logloss is: 0.4166231160824659\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 49\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4193 - acc: 0.8072\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4174863010023412\n",
      "0.7673103649143647\n",
      "best logloss is: 0.4166231160824659\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 50\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4193 - acc: 0.8071\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41719665132850736\n",
      "0.7669807445022092\n",
      "best logloss is: 0.4166231160824659\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 51\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4192 - acc: 0.8073\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41725967148206117\n",
      "0.7668535084234527\n",
      "best logloss is: 0.4166231160824659\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 52\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4194 - acc: 0.8070\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.41692839406017934\n",
      "0.7672500341676286\n",
      "best logloss is: 0.4166231160824659\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 53\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4192 - acc: 0.8071\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41669209813775193\n",
      "0.7671241894795489\n",
      "best logloss is: 0.4166231160824659\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 54\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4192 - acc: 0.8072\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4171258768061647\n",
      "0.7665299873385674\n",
      "best logloss is: 0.4166231160824659\n",
      "remainning trial is: 10/30\n",
      "total epoch trained: 55\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4192 - acc: 0.8071\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4169757418872471\n",
      "0.7672000695751418\n",
      "best logloss is: 0.4166231160824659\n",
      "remainning trial is: 11/30\n",
      "total epoch trained: 56\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4190 - acc: 0.8072\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4169817266888297\n",
      "0.7672009004934468\n",
      "best logloss is: 0.4166231160824659\n",
      "remainning trial is: 12/30\n",
      "total epoch trained: 57\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8072\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4176690876160502\n",
      "0.7664913230470564\n",
      "best logloss is: 0.4166231160824659\n",
      "remainning trial is: 13/30\n",
      "total epoch trained: 58\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8070\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4168202016212255\n",
      "0.767747945338087\n",
      "best logloss is: 0.4166231160824659\n",
      "remainning trial is: 14/30\n",
      "total epoch trained: 59\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4190 - acc: 0.8074\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.41706196169396914\n",
      "0.766980670957833\n",
      "best logloss is: 0.4166231160824659\n",
      "remainning trial is: 15/30\n",
      "total epoch trained: 60\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8072\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4172398548659662\n",
      "0.7672991483396276\n",
      "best logloss is: 0.4166231160824659\n",
      "remainning trial is: 16/30\n",
      "total epoch trained: 61\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4189 - acc: 0.8072\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.416603736598936\n",
      "0.7674469182588848\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 62\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4190 - acc: 0.8072\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41726000604653596\n",
      "0.7669021207079507\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 63\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4190 - acc: 0.8074\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4167896656061291\n",
      "0.7670722543833729\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 64\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4189 - acc: 0.8072\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4170189080034674\n",
      "0.7671306487748204\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 65\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4188 - acc: 0.8071\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4169447207935858\n",
      "0.7670789979092534\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 66\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4188 - acc: 0.8072\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41696476921671155\n",
      "0.7672360432703161\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 67\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4187 - acc: 0.8074\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4171362181699272\n",
      "0.7669828673422262\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 68\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4189 - acc: 0.8074\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41707943664626934\n",
      "0.7671113055073382\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 69\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4189 - acc: 0.8073\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41679210854640003\n",
      "0.767418106172531\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 70\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4188 - acc: 0.8074\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4172273293423065\n",
      "0.7671556147800839\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 71\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4186 - acc: 0.8077\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41690200723052034\n",
      "0.7672849841156859\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 10/30\n",
      "total epoch trained: 72\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4186 - acc: 0.8075\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4171964340129232\n",
      "0.7671912321883916\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 11/30\n",
      "total epoch trained: 73\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4186 - acc: 0.8074\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.41681027235321816\n",
      "0.7673247521994179\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 12/30\n",
      "total epoch trained: 74\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4186 - acc: 0.8076\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4171226214604118\n",
      "0.7671167358862394\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 13/30\n",
      "total epoch trained: 75\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4186 - acc: 0.8074\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41674908375957875\n",
      "0.7673711218112089\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 14/30\n",
      "total epoch trained: 76\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4185 - acc: 0.8076\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41693854639386235\n",
      "0.7668395618564035\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 15/30\n",
      "total epoch trained: 77\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4186 - acc: 0.8073\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.41694526951789107\n",
      "0.7672642021510272\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 16/30\n",
      "total epoch trained: 78\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4185 - acc: 0.8075\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.41675705487753784\n",
      "0.7674270420883819\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 17/30\n",
      "total epoch trained: 79\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4186 - acc: 0.8072\n",
      "200330/200330 [==============================] - 2s 7us/step\n",
      "0.4170290162062317\n",
      "0.767283226882856\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 18/30\n",
      "total epoch trained: 80\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4184 - acc: 0.8074\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41720420292161164\n",
      "0.766769141275503\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 19/30\n",
      "total epoch trained: 81\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4185 - acc: 0.8076\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4169305087449478\n",
      "0.7672217881145098\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 20/30\n",
      "total epoch trained: 82\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4183 - acc: 0.8075\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4171412756454178\n",
      "0.7672426750306747\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 21/30\n",
      "total epoch trained: 83\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4184 - acc: 0.8075\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41752957307466276\n",
      "0.7667613307687506\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 22/30\n",
      "total epoch trained: 84\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4184 - acc: 0.8074\n",
      "200330/200330 [==============================] - 2s 7us/step\n",
      "0.41714368512306704\n",
      "0.7667988463112013\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 23/30\n",
      "total epoch trained: 85\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4183 - acc: 0.8077\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4169894176720631\n",
      "0.7672801936276842\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 24/30\n",
      "total epoch trained: 86\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4183 - acc: 0.8075\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.4174626422238822\n",
      "0.7668799522085803\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 25/30\n",
      "total epoch trained: 87\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4182 - acc: 0.8076\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41720988024693395\n",
      "0.7667474133374925\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 26/30\n",
      "total epoch trained: 88\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4183 - acc: 0.8077\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41736832063945056\n",
      "0.7667944714781572\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 27/30\n",
      "total epoch trained: 89\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4182 - acc: 0.8077\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "0.41768385924452567\n",
      "0.7667036885819787\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 28/30\n",
      "total epoch trained: 90\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4182 - acc: 0.8077\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.41743236031089836\n",
      "0.7669142746406173\n",
      "best logloss is: 0.416603736598936\n",
      "remainning trial is: 29/30\n",
      "total epoch trained: 91\n",
      "Epoch 1/1\n",
      "801320/801320 [==============================] - 15s 19us/step - loss: 0.4182 - acc: 0.8076\n",
      "200330/200330 [==============================] - 2s 8us/step\n",
      "0.4170752692961035\n",
      "0.7670183635080476\n",
      "801320/801320 [==============================] - 6s 7us/step\n",
      "200330/200330 [==============================] - 1s 7us/step\n",
      "40024/40024 [==============================] - 0s 6us/step\n",
      "Fold 2 finish! val loss: 0.416603736598936.\n",
      "start fold 3...\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 18s 22us/step - loss: 0.4584 - acc: 0.7990\n",
      "200329/200329 [==============================] - 2s 12us/step\n",
      "0.4228435792994773\n",
      "0.7571552927720981\n",
      "best logloss is: 0.4228435792994773\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 0\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4261 - acc: 0.8043\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4196401289363322\n",
      "0.7623311410525191\n",
      "best logloss is: 0.4196401289363322\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 1\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4243 - acc: 0.8048\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41948367048944607\n",
      "0.7627219613567889\n",
      "best logloss is: 0.41948367048944607\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 2\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4236 - acc: 0.8051\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4182703198641363\n",
      "0.7641188749165316\n",
      "best logloss is: 0.4182703198641363\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 3\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4231 - acc: 0.8054\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4197131881660482\n",
      "0.7639289549763513\n",
      "best logloss is: 0.4182703198641363\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 4\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4230 - acc: 0.8054\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41798184685409856\n",
      "0.7652305780841842\n",
      "best logloss is: 0.41798184685409856\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 5\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4226 - acc: 0.8056\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41822147400363263\n",
      "0.7652595217751391\n",
      "best logloss is: 0.41798184685409856\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 6\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4223 - acc: 0.8056\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.418111040202696\n",
      "0.7655045769226463\n",
      "best logloss is: 0.41798184685409856\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 7\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4221 - acc: 0.8058\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4173991390442119\n",
      "0.7658237265718513\n",
      "best logloss is: 0.4173991390442119\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 8\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4220 - acc: 0.8058\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4176208773138875\n",
      "0.765742027581276\n",
      "best logloss is: 0.4173991390442119\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 9\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4218 - acc: 0.8058\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41750367034349845\n",
      "0.7657789566434614\n",
      "best logloss is: 0.4173991390442119\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 10\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4218 - acc: 0.8060\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.41794680964211334\n",
      "0.7662911227131723\n",
      "best logloss is: 0.4173991390442119\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 11\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4217 - acc: 0.8057\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.41721454330460633\n",
      "0.7661707857416251\n",
      "best logloss is: 0.41721454330460633\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 12\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4216 - acc: 0.8062\n",
      "200329/200329 [==============================] - 2s 7us/step\n",
      "0.41725945261651093\n",
      "0.7665297332498124\n",
      "best logloss is: 0.41721454330460633\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 13\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4215 - acc: 0.8060\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41679871327652557\n",
      "0.767013675647378\n",
      "best logloss is: 0.41679871327652557\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 14\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4215 - acc: 0.8060\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41730677349259326\n",
      "0.7669967716139203\n",
      "best logloss is: 0.41679871327652557\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 15\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4215 - acc: 0.8061\n",
      "200329/200329 [==============================] - 2s 7us/step\n",
      "0.41701258827764964\n",
      "0.766436358723076\n",
      "best logloss is: 0.41679871327652557\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 16\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4215 - acc: 0.8061\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41662472806345074\n",
      "0.7673729974588275\n",
      "best logloss is: 0.41662472806345074\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 17\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4210 - acc: 0.8062\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4168514086506707\n",
      "0.7669900025912282\n",
      "best logloss is: 0.41662472806345074\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 18\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4211 - acc: 0.8063\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41667572664932245\n",
      "0.7670738472268905\n",
      "best logloss is: 0.41662472806345074\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 19\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4209 - acc: 0.8063\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41753304657646295\n",
      "0.7669803615605162\n",
      "best logloss is: 0.41662472806345074\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 20\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4210 - acc: 0.8063\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4167368171923467\n",
      "0.7674556634974004\n",
      "best logloss is: 0.41662472806345074\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 21\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4209 - acc: 0.8062\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4168951838792222\n",
      "0.7671072917207269\n",
      "best logloss is: 0.41662472806345074\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 22\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4208 - acc: 0.8064\n",
      "200329/200329 [==============================] - 2s 7us/step\n",
      "0.4169502927340337\n",
      "0.7675557888760072\n",
      "best logloss is: 0.41662472806345074\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 23\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4207 - acc: 0.8063\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4166290496610452\n",
      "0.7675186488913097\n",
      "best logloss is: 0.41662472806345074\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 24\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4207 - acc: 0.8064\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4170718435499548\n",
      "0.7675434837052101\n",
      "best logloss is: 0.41662472806345074\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 25\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4207 - acc: 0.8063\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4167619351103864\n",
      "0.767652084284008\n",
      "best logloss is: 0.41662472806345074\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 26\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4206 - acc: 0.8065\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4169821327243795\n",
      "0.7681473202388664\n",
      "best logloss is: 0.41662472806345074\n",
      "remainning trial is: 10/30\n",
      "total epoch trained: 27\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 18us/step - loss: 0.4204 - acc: 0.8064\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.41700653210431277\n",
      "0.7681100446779088\n",
      "best logloss is: 0.41662472806345074\n",
      "remainning trial is: 11/30\n",
      "total epoch trained: 28\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4204 - acc: 0.8065\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4165034924046486\n",
      "0.7681464589262539\n",
      "best logloss is: 0.4165034924046486\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 29\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4206 - acc: 0.8066\n",
      "200329/200329 [==============================] - 1s 5us/step\n",
      "0.41644053845924034\n",
      "0.7683171823331885\n",
      "best logloss is: 0.41644053845924034\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 30\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4202 - acc: 0.8065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4164480255804906\n",
      "0.7679789157264052\n",
      "best logloss is: 0.41644053845924034\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 31\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4202 - acc: 0.8066\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.41595570128285775\n",
      "0.7684148071091055\n",
      "best logloss is: 0.41595570128285775\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 32\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4202 - acc: 0.8065\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41621153426565294\n",
      "0.7680074285652476\n",
      "best logloss is: 0.41595570128285775\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 33\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4204 - acc: 0.8066\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4163761762724937\n",
      "0.7682617134719809\n",
      "best logloss is: 0.41595570128285775\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 34\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4202 - acc: 0.8066\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4161689394481789\n",
      "0.7680674565796044\n",
      "best logloss is: 0.41595570128285775\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 35\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4200 - acc: 0.8066\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.41688386083820955\n",
      "0.7681084348370941\n",
      "best logloss is: 0.41595570128285775\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 36\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4200 - acc: 0.8068\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4166094472144501\n",
      "0.7681054160526958\n",
      "best logloss is: 0.41595570128285775\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 37\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4199 - acc: 0.8065\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.41630212817362644\n",
      "0.7682972832470523\n",
      "best logloss is: 0.41595570128285775\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 38\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4199 - acc: 0.8066\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41623192210133797\n",
      "0.7682838720058072\n",
      "best logloss is: 0.41595570128285775\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 39\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4200 - acc: 0.8067\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4160957098186203\n",
      "0.7681819449745652\n",
      "best logloss is: 0.41595570128285775\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 40\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4198 - acc: 0.8069\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.416588187979649\n",
      "0.7676526303485288\n",
      "best logloss is: 0.41595570128285775\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 41\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4199 - acc: 0.8066\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4164590605440465\n",
      "0.768323985974429\n",
      "best logloss is: 0.41595570128285775\n",
      "remainning trial is: 10/30\n",
      "total epoch trained: 42\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4198 - acc: 0.8068\n",
      "200329/200329 [==============================] - 1s 5us/step\n",
      "0.41638078734126627\n",
      "0.7683981006833337\n",
      "best logloss is: 0.41595570128285775\n",
      "remainning trial is: 11/30\n",
      "total epoch trained: 43\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4198 - acc: 0.8070\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4163257244116108\n",
      "0.7681714928359678\n",
      "best logloss is: 0.41595570128285775\n",
      "remainning trial is: 12/30\n",
      "total epoch trained: 44\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4199 - acc: 0.8068\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4161803596225014\n",
      "0.7686148398947816\n",
      "best logloss is: 0.41595570128285775\n",
      "remainning trial is: 13/30\n",
      "total epoch trained: 45\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4198 - acc: 0.8067\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4161984459124998\n",
      "0.7683815859127373\n",
      "best logloss is: 0.41595570128285775\n",
      "remainning trial is: 14/30\n",
      "total epoch trained: 46\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4199 - acc: 0.8066\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4162839734456943\n",
      "0.7681333646561959\n",
      "best logloss is: 0.41595570128285775\n",
      "remainning trial is: 15/30\n",
      "total epoch trained: 47\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4196 - acc: 0.8067\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.41618271378309835\n",
      "0.7679518487993695\n",
      "best logloss is: 0.41595570128285775\n",
      "remainning trial is: 16/30\n",
      "total epoch trained: 48\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4196 - acc: 0.8068\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4161687145809024\n",
      "0.768358325772846\n",
      "best logloss is: 0.41595570128285775\n",
      "remainning trial is: 17/30\n",
      "total epoch trained: 49\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4195 - acc: 0.8069\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.415802252571591\n",
      "0.7686721291277037\n",
      "best logloss is: 0.415802252571591\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 50\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4195 - acc: 0.8067\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4166036574854304\n",
      "0.7683209795649867\n",
      "best logloss is: 0.415802252571591\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 51\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4196 - acc: 0.8071\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41618055264237697\n",
      "0.7684018617301336\n",
      "best logloss is: 0.415802252571591\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 52\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4196 - acc: 0.8070\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4159222532706046\n",
      "0.768636358612046\n",
      "best logloss is: 0.415802252571591\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 53\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4194 - acc: 0.8068\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4157488031994191\n",
      "0.7688893688199732\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 54\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4194 - acc: 0.8069\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4165180106032689\n",
      "0.7679044361129389\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 55\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4194 - acc: 0.8070\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4159837474966933\n",
      "0.7684929099217587\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 56\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4194 - acc: 0.8069\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4163053901295357\n",
      "0.7685390889503771\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 57\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4193 - acc: 0.8068\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4160713215459915\n",
      "0.7682935829785181\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 58\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4193 - acc: 0.8071\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4159933159234873\n",
      "0.7685908493975098\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 59\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4193 - acc: 0.8068\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4162420304569117\n",
      "0.7683400780384535\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 60\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4193 - acc: 0.8071\n",
      "200329/200329 [==============================] - 2s 7us/step\n",
      "0.41618308299547524\n",
      "0.7685680447813209\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 61\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4193 - acc: 0.8070\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.41612478090180494\n",
      "0.7685016966888836\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 62\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8071\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.41583024603536556\n",
      "0.769058751658873\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 63\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4192 - acc: 0.8069\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4161213243595312\n",
      "0.7687720957465884\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 10/30\n",
      "total epoch trained: 64\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4192 - acc: 0.8070\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.41625580161548054\n",
      "0.7685507585340277\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 11/30\n",
      "total epoch trained: 65\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4192 - acc: 0.8070\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41592045836955227\n",
      "0.7684676806757117\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 12/30\n",
      "total epoch trained: 66\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8071\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41606728072830723\n",
      "0.7685982495429657\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 13/30\n",
      "total epoch trained: 67\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8073\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41617693728688276\n",
      "0.7686535712361823\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 14/30\n",
      "total epoch trained: 68\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4192 - acc: 0.8071\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4158187090136262\n",
      "0.7688764636204476\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 15/30\n",
      "total epoch trained: 69\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8073\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4162245115409305\n",
      "0.7681025038652298\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 16/30\n",
      "total epoch trained: 70\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4192 - acc: 0.8071\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41630649505627954\n",
      "0.7691361624921744\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 17/30\n",
      "total epoch trained: 71\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4189 - acc: 0.8071\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4160185692965003\n",
      "0.7685152003506874\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 18/30\n",
      "total epoch trained: 72\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8069\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4158703319016629\n",
      "0.7687645074704699\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 19/30\n",
      "total epoch trained: 73\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8070\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41663976840213923\n",
      "0.768486419805515\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 20/30\n",
      "total epoch trained: 74\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4191 - acc: 0.8070\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.415870488223919\n",
      "0.7688725811737614\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 21/30\n",
      "total epoch trained: 75\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4189 - acc: 0.8072\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4160614611820335\n",
      "0.7684911595703658\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 22/30\n",
      "total epoch trained: 76\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4190 - acc: 0.8072\n",
      "200329/200329 [==============================] - 2s 7us/step\n",
      "0.41635648836667094\n",
      "0.7684098255626746\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 23/30\n",
      "total epoch trained: 77\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4190 - acc: 0.8073\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4160180960743576\n",
      "0.7686291061045163\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 24/30\n",
      "total epoch trained: 78\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4188 - acc: 0.8072\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.41619159039157916\n",
      "0.7685971365018147\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 25/30\n",
      "total epoch trained: 79\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4187 - acc: 0.8073\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41601123277907154\n",
      "0.768903394814578\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 26/30\n",
      "total epoch trained: 80\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4189 - acc: 0.8071\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4161360738341617\n",
      "0.7680212321239318\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 27/30\n",
      "total epoch trained: 81\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4188 - acc: 0.8073\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41589907309486285\n",
      "0.7688365879872154\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 28/30\n",
      "total epoch trained: 82\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4187 - acc: 0.8075\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.41598482018563504\n",
      "0.7687082227237039\n",
      "best logloss is: 0.4157488031994191\n",
      "remainning trial is: 29/30\n",
      "total epoch trained: 83\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4186 - acc: 0.8070\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4161749984980485\n",
      "0.7680293363104057\n",
      "801321/801321 [==============================] - 6s 7us/step\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "40024/40024 [==============================] - 0s 6us/step\n",
      "Fold 3 finish! val loss: 0.4157488031994191.\n",
      "start fold 4...\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 18s 23us/step - loss: 0.4585 - acc: 0.8006\n",
      "200329/200329 [==============================] - 3s 13us/step\n",
      "0.42213929035519693\n",
      "0.7574395856943379\n",
      "best logloss is: 0.42213929035519693\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 0\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4260 - acc: 0.8044\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4198383993131673\n",
      "0.761718665859599\n",
      "best logloss is: 0.4198383993131673\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 1\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4244 - acc: 0.8050\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.42070484670254327\n",
      "0.763169071498788\n",
      "best logloss is: 0.4198383993131673\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 2\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4237 - acc: 0.8052\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41906854590046366\n",
      "0.7634572877520628\n",
      "best logloss is: 0.41906854590046366\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 3\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4231 - acc: 0.8053\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.41830303145985814\n",
      "0.7641824767085734\n",
      "best logloss is: 0.41830303145985814\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 4\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4228 - acc: 0.8054\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41802284766006576\n",
      "0.7644272035281299\n",
      "best logloss is: 0.41802284766006576\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 5\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4226 - acc: 0.8054\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4178148892996198\n",
      "0.7652097458636962\n",
      "best logloss is: 0.4178148892996198\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 6\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4224 - acc: 0.8054\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4179616539167564\n",
      "0.7651761708062402\n",
      "best logloss is: 0.4178148892996198\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 7\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4223 - acc: 0.8054\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4181047133010016\n",
      "0.7656724877683443\n",
      "best logloss is: 0.4178148892996198\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 8\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4220 - acc: 0.8055\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.41757711795276287\n",
      "0.7655731554051235\n",
      "best logloss is: 0.41757711795276287\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 9\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4219 - acc: 0.8055\n",
      "200329/200329 [==============================] - 2s 7us/step\n",
      "0.4176309402321992\n",
      "0.7661429680150348\n",
      "best logloss is: 0.41757711795276287\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 10\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4217 - acc: 0.8057\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41715709112796123\n",
      "0.7663878695721051\n",
      "best logloss is: 0.41715709112796123\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 11\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4215 - acc: 0.8059\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4179537953349439\n",
      "0.766870263925314\n",
      "best logloss is: 0.41715709112796123\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 12\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4216 - acc: 0.8057\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4173766730687936\n",
      "0.7658916559957107\n",
      "best logloss is: 0.41715709112796123\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 13\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4213 - acc: 0.8059\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4172480547317573\n",
      "0.7666198600554618\n",
      "best logloss is: 0.41715709112796123\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 14\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4213 - acc: 0.8060\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.41739908489790245\n",
      "0.7669432263534915\n",
      "best logloss is: 0.41715709112796123\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 15\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4213 - acc: 0.8060\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41692946013750076\n",
      "0.767299454817516\n",
      "best logloss is: 0.41692946013750076\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 16\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4212 - acc: 0.8060\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41722018669017735\n",
      "0.7667844812775313\n",
      "best logloss is: 0.41692946013750076\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 17\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4211 - acc: 0.8061\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.417491559991968\n",
      "0.7672808976308971\n",
      "best logloss is: 0.41692946013750076\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 18\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4213 - acc: 0.8061\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4171431938686554\n",
      "0.7672706722359584\n",
      "best logloss is: 0.41692946013750076\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 19\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4211 - acc: 0.8062\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41667417934412754\n",
      "0.767544176467787\n",
      "best logloss is: 0.41667417934412754\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 20\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4211 - acc: 0.8061\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4170754393547798\n",
      "0.7672614219562353\n",
      "best logloss is: 0.41667417934412754\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 21\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4209 - acc: 0.8062\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41661991225313155\n",
      "0.7678972598132088\n",
      "best logloss is: 0.41661991225313155\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 22\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4207 - acc: 0.8063\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4166193889827022\n",
      "0.768009937391801\n",
      "best logloss is: 0.4166193889827022\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 23\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4209 - acc: 0.8063\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41663471511424494\n",
      "0.7679312287545539\n",
      "best logloss is: 0.4166193889827022\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 24\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4209 - acc: 0.8061\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4166271455867545\n",
      "0.7674814945735934\n",
      "best logloss is: 0.4166193889827022\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 25\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4206 - acc: 0.8063\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.417338534177971\n",
      "0.7673745111979258\n",
      "best logloss is: 0.4166193889827022\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 26\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4206 - acc: 0.8063\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41737467082189156\n",
      "0.767387498166149\n",
      "best logloss is: 0.4166193889827022\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 27\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4206 - acc: 0.8063\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41698979929884245\n",
      "0.7680943206513461\n",
      "best logloss is: 0.4166193889827022\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 28\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4205 - acc: 0.8064\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4164853712954762\n",
      "0.7678865275150029\n",
      "best logloss is: 0.4164853712954762\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 29\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4202 - acc: 0.8066\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.41625489837576923\n",
      "0.7680965306775341\n",
      "best logloss is: 0.41625489837576923\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 30\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4204 - acc: 0.8066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200329/200329 [==============================] - 2s 7us/step\n",
      "0.41624165804234114\n",
      "0.7684920062366696\n",
      "best logloss is: 0.41624165804234114\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 31\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4203 - acc: 0.8065\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.416443542432226\n",
      "0.7679244581654115\n",
      "best logloss is: 0.41624165804234114\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 32\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4204 - acc: 0.8064\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41644725979710606\n",
      "0.767979973628511\n",
      "best logloss is: 0.41624165804234114\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 33\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4204 - acc: 0.8066\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4169289580057979\n",
      "0.7679928897931876\n",
      "best logloss is: 0.41624165804234114\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 34\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4203 - acc: 0.8067\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4166950061862912\n",
      "0.7679743016691785\n",
      "best logloss is: 0.41624165804234114\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 35\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4203 - acc: 0.8065\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.416725257193651\n",
      "0.7673637973055091\n",
      "best logloss is: 0.41624165804234114\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 36\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4200 - acc: 0.8065\n",
      "200329/200329 [==============================] - 2s 7us/step\n",
      "0.41665075736487045\n",
      "0.7682896212694545\n",
      "best logloss is: 0.41624165804234114\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 37\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4201 - acc: 0.8066\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4163396378853446\n",
      "0.7680570230817638\n",
      "best logloss is: 0.41624165804234114\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 38\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4202 - acc: 0.8066\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.41611483284930256\n",
      "0.7685674390350494\n",
      "best logloss is: 0.41611483284930256\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 39\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4199 - acc: 0.8068\n",
      "200329/200329 [==============================] - 2s 7us/step\n",
      "0.4162320262810047\n",
      "0.7680471614948714\n",
      "best logloss is: 0.41611483284930256\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 40\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4201 - acc: 0.8069\n",
      "200329/200329 [==============================] - 2s 7us/step\n",
      "0.41640663593463645\n",
      "0.7684952388415128\n",
      "best logloss is: 0.41611483284930256\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 41\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4198 - acc: 0.8067\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.41631249265296366\n",
      "0.7682193609677783\n",
      "best logloss is: 0.41611483284930256\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 42\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4201 - acc: 0.8068\n",
      "200329/200329 [==============================] - 2s 8us/step\n",
      "0.4165647690926568\n",
      "0.7683133648941833\n",
      "best logloss is: 0.41611483284930256\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 43\n",
      "Epoch 1/1\n",
      "801321/801321 [==============================] - 15s 19us/step - loss: 0.4198 - acc: 0.8068\n",
      "200329/200329 [==============================] - 1s 7us/step\n",
      "0.4164235431535572\n",
      "0.7686570740536756\n",
      "best logloss is: 0.41611483284930256\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 44\n",
      "Epoch 1/1\n",
      "515000/801321 [==================>...........] - ETA: 5s - loss: 0.4204 - acc: 0.8059"
     ]
    }
   ],
   "source": [
    "train_df = train[['instance_id']].copy()\n",
    "test_df = test[['instance_id']].copy()\n",
    "\n",
    "train_save,test_save,cv_,ho_,ta_ = nn_K_fold(train_fold_dict,\n",
    "                                      val_fold_dict,\n",
    "                                      train_fold_y,\n",
    "                                      val_fold_y,\n",
    "                                      test_input_dict,\n",
    "                                      val_index_list,\n",
    "                                      train_df,\n",
    "                                      test_df,\n",
    "                                      pred_col_name = 'predicted_score',\n",
    "                                      holdout_input_dict=holdout_input_dict,\n",
    "                                      holdout_y=holdout_y,\n",
    "                                      holdout_index_list=holdout_index,\n",
    "                                      nondoc_cols=non_doc_col,\n",
    "                                      doc_cols=doc_col,\n",
    "                                      tolerance=30,\n",
    "                                      preds_batch=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-07T15:58:37.836Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_oof(train_df,test_df,cv,ta,ho=None,file_name='',path='../../data/nn_ebd/'):\n",
    "    try:\n",
    "        report = pd.read_csv(path+'report.csv')\n",
    "    except:\n",
    "        print('no report found! generate a new one!')\n",
    "        report = pd.DataFrame()\n",
    "    new_record = pd.DataFrame({'ho':[ho],'cv':[cv],'train_mean':[ta],'file':[file_name]})\n",
    "    report = pd.concat([report,new_record],sort=False)\n",
    "    train_df.to_pickle(path+'train/'+file_name+'.pkl')\n",
    "    print(train_df.shape)\n",
    "    test_df.to_csv(path+'test/'+file_name+'.csv',index=False)\n",
    "    print(test_df.shape)\n",
    "    report.to_csv(path+'report.csv',index=False)\n",
    "    print('done!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-07T15:58:37.839Z"
    }
   },
   "outputs": [],
   "source": [
    "save_oof(train_save,test_save,cv_,ta_,ho_,file_name='oldModel_RNNModelTag_ebdNANExclude_changeModelFilter_noholdout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-07T15:58:37.842Z"
    }
   },
   "outputs": [],
   "source": [
    "name_dict = {'new_modelMakeOsOSV_rnnEBD':\n",
    "             [['model','make','os','osv']],\n",
    "             'new_modelMakeOsOSV+modelAppidInnerslotCreateWCreativeH_rnnEBD':\n",
    "             [['model','make','os','osv'],['model','app_id','inner_slot_id','creative_width', 'creative_height']],\n",
    "             'new_modelOSV':[['model', 'osv']],'new_model_onlyuTagRNN_100patience':[],}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-07T15:58:37.844Z"
    }
   },
   "outputs": [],
   "source": [
    "tok=text.Tokenizer(num_words=100,lower=False)\n",
    "tok.fit_on_texts(list(['9.2.1 Iphone']))\n",
    "tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-07T15:58:37.847Z"
    }
   },
   "outputs": [],
   "source": [
    "tok=text.Tokenizer(num_words=100,lower=False,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tok.fit_on_texts(list(['9.2.1 Iphone']))\n",
    "tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-07T15:58:37.848Z"
    }
   },
   "outputs": [],
   "source": [
    "tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
