{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:13:12.391452Z",
     "start_time": "2018-09-27T14:13:11.324491Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "__file__=''\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../LIB/'))\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../../../../automl/automl_libs/'))\n",
    "from env import FILE\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csc_matrix, csr_matrix, hstack\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU,CuDNNGRU,Flatten,BatchNormalization,CuDNNLSTM,Activation,BatchNormalization\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import log_loss,roc_auc_score\n",
    "from keras.regularizers import l1\n",
    "from keras.regularizers import l2\n",
    "from keras.regularizers import l1_l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import re\n",
    "from keras import backend as K\n",
    "# To get learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:13:16.302862Z",
     "start_time": "2018-09-27T14:13:12.392974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: (1001650, 35)\n",
      "test shape is: (40024, 34)\n",
      "(1041674, 35)\n",
      "(1041674, 45)\n",
      "(1041674, 36)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_pickle(FILE.train_ori.value)\n",
    "print('train shape is: {}'.format(train.shape))\n",
    "test = pd.read_pickle(FILE.test_ori.value)\n",
    "print('test shape is: {}'.format(test.shape))\n",
    "\n",
    "# X = pd.concat([train.drop(['click'],axis=1),test])\n",
    "X = pd.concat([train,test],sort=False)\n",
    "print(X.shape)\n",
    "\n",
    "X_clean = pd.read_csv('../../data/original/cleaned_data_final.csv')\n",
    "\n",
    "X_shiyi = pd.read_pickle(FILE.shiyi_fillna_ori.value)\n",
    "print(X_shiyi.shape)\n",
    "\n",
    "X = X.merge(X_shiyi[['time_hour','instance_id']],how='inner',on='instance_id')\n",
    "print(X.shape)\n",
    "assert np.sum(X_clean['instance_id'].values != X['instance_id'].values) == 0\n",
    "X['model'] = X_clean['model_new'].values\n",
    "# X['make'] = X_clean['make_new'].values\n",
    "\n",
    "ignore_columns = ['instance_id','time','click'] + ['creative_is_js', 'creative_is_voicead', 'app_paid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:13:16.313052Z",
     "start_time": "2018-09-27T14:13:16.304542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['instance_id', 'time', 'city', 'province', 'user_tags', 'carrier',\n",
       "       'devtype', 'make', 'model', 'nnt', 'os', 'osv', 'os_name', 'adid',\n",
       "       'advert_id', 'orderid', 'advert_industry_inner', 'campaign_id',\n",
       "       'creative_id', 'creative_tp_dnf', 'app_cate_id', 'f_channel', 'app_id',\n",
       "       'inner_slot_id', 'creative_type', 'creative_width', 'creative_height',\n",
       "       'creative_is_jump', 'creative_is_download', 'creative_is_js',\n",
       "       'creative_is_voicead', 'creative_has_deeplink', 'app_paid',\n",
       "       'advert_name', 'click', 'time_hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:13:16.325673Z",
     "start_time": "2018-09-27T14:13:16.314393Z"
    }
   },
   "outputs": [],
   "source": [
    "# def comb_fe(X,cols,sep=' '):\n",
    "#     ret = X[cols[0]].astype(str).copy()\n",
    "#     for col in cols[1:]:\n",
    "#         ret = ret + sep + X[col].astype(str)\n",
    "#     return ret.values\n",
    "\n",
    "# processed_col = []\n",
    "# cob_col = [['model','make','os','osv']]\n",
    "# doc_col=['user_tags']\n",
    "# X_doc = X[doc_col].copy()\n",
    "# processed_col.extend(doc_col)\n",
    "# for each in cob_col:\n",
    "#     feature_name = '_'.join(each)\n",
    "#     processed_col.extend(each)\n",
    "#     doc_col.append(feature_name)\n",
    "#     X_doc[feature_name] = comb_fe(X,each)\n",
    "    \n",
    "# non_doc_col = list(set(X.columns) - set(ignore_columns) - set(processed_col))\n",
    "# X_ = X[non_doc_col].copy()\n",
    "\n",
    "# for col in tqdm(non_doc_col):\n",
    "#     X_[col] = le.fit_transform(X_[col].astype(str))\n",
    "#     X_[col] = col + '_'+X_[col].astype(str)\n",
    "    \n",
    "# for col in tqdm(doc_col):\n",
    "#     X_doc[col] = X_doc[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:13:57.387735Z",
     "start_time": "2018-09-27T14:13:16.327306Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.65it/s]\n",
      "100%|██████████| 24/24 [00:35<00:00,  1.48s/it]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# # def comb_fe(X,cols,sep=' '):\n",
    "# #     ret = X[cols[0]].astype(str).copy()\n",
    "# #     for col in cols[1:]:\n",
    "# #         ret = ret + sep + X[col].astype(str)\n",
    "# #     return ret.values\n",
    "\n",
    "# def process_slot(x):\n",
    "#     x = re.sub(r'[-_]',' ',x)\n",
    "#     x = x.split(' ')\n",
    "#     ret = ['p{}_'.format(c)+x[c] for c in range(len(x))]\n",
    "#     ret = ' '.join(ret)\n",
    "#     return ret\n",
    "        \n",
    "    \n",
    "\n",
    "# def comb_fe(X,cols,sep=' '):\n",
    "#     def add_col_name(x,colName,splitter=' ' ,sep=' '):\n",
    "#         ret = [colName+'_'+each for each in x.split(splitter)]\n",
    "#         return sep.join(ret)\n",
    "#     ret = pd.DataFrame()\n",
    "#     if cols[0] == 'user_tags':\n",
    "#         ret['comb'] = X['user_tags'].astype(str).apply(add_col_name,colName='userTags',splitter=',',sep=sep)\n",
    "#     else:\n",
    "#         if cols[0] in original_name_col:\n",
    "#             ret['comb'] = X[cols[0]].astype(str).copy()\n",
    "#         else:\n",
    "#             ret['comb'] = X[cols[0]].astype(str).apply(add_col_name,colName=cols[0],splitter=' ',sep=sep)\n",
    "\n",
    "#     for col in tqdm(cols[1:]):\n",
    "#         if col == 'user_tags':\n",
    "#             ret['new_feature'] = X['user_tags'].astype(str).apply(add_col_name,colName='userTags',splitter=',',sep=sep)\n",
    "#         else:\n",
    "#             if col in original_name_col:\n",
    "#                 ret['new_feature'] = X[col].astype(str).copy()\n",
    "#             else:\n",
    "#                 ret['new_feature'] = X[col].astype(str).apply(add_col_name,colName=col,splitter=' ',sep=sep)\n",
    "#         ret['comb'] =( ret['comb'] + sep + ret['new_feature']).values\n",
    "#     return ret['comb'].values\n",
    "\n",
    "# processed_col = []\n",
    "# original_name_col = ['model','make','os','osv']\n",
    "# #41481\n",
    "# # cob_col = [['model','make','os','osv'],['model','app_id','inner_slot_id','creative_width', 'creative_height']]\n",
    "# #4179\n",
    "# # cob_col = [['model','osv']]\n",
    "# #41479 with duplicate in non-doc\n",
    "# # cob_col = [['model','make','os','osv']]\n",
    "# #41480 + inner slot id\n",
    "# # cob_col = [['model','make','os','osv']]\n",
    "# #41480\n",
    "# doc_col=['user_tags','inner_slot_id']\n",
    "# X_doc = X[doc_col].copy()\n",
    "# processed_col.extend(doc_col)\n",
    "# for each in cob_col:\n",
    "#     feature_name = '_'.join(each)\n",
    "#     processed_col.extend(each)\n",
    "#     doc_col.append(feature_name)\n",
    "#     X_doc[feature_name] = comb_fe(X,each)\n",
    "    \n",
    "# non_doc_col = list(set(X.columns) - set(ignore_columns) - set(processed_col))\n",
    "# #!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# # 0.414797\n",
    "# # non_doc_col = list(set(X.columns) - set(ignore_columns)- set(doc_col))\n",
    "# X_ = X[non_doc_col].copy()\n",
    "\n",
    "# for col in tqdm(non_doc_col):\n",
    "#     X_[col] = le.fit_transform(X_[col].astype(str))\n",
    "#     X_[col] = col + '_'+X_[col].astype(str)\n",
    "    \n",
    "# for col in tqdm(doc_col):\n",
    "#     if col=='inner_slot_id':\n",
    "#         X_doc[col] = X_doc[col].astype(str).apply(process_slot)\n",
    "#     else:\n",
    "#         X_doc[col] = X_doc[col].astype(str)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rnn all except user_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:13:57.391685Z",
     "start_time": "2018-09-27T14:13:57.389216Z"
    }
   },
   "outputs": [],
   "source": [
    "# cobdoc = list(set(X.columns) - set(ignore_columns)-set(['user_tags']))\n",
    "# doc_col = ['user_tags']\n",
    "# non_doc_col = []\n",
    "# def comb_fe(X,cols,sep=' '):\n",
    "#     ret = pd.DataFrame()\n",
    "#     le = LabelEncoder()\n",
    "#     ret['new_feature'] = le.fit_transform(X[cols[0]].astype(str))\n",
    "#     ret['new_feature'] = cols[0]+'_'+ ret['new_feature'].astype(str)\n",
    "#     ret['comb'] = ret['new_feature'].values\n",
    "\n",
    "#     for col in tqdm(cols[1:]):\n",
    "#         ret['new_feature'] = le.fit_transform(X[col].astype(str))\n",
    "#         ret['new_feature'] = col+'_'+ ret['new_feature'].astype(str)\n",
    "#         ret['comb'] =( ret['comb'] + sep + ret['new_feature']).values\n",
    "#     return ret['comb'].values\n",
    "# X_doc = X[doc_col].copy()\n",
    "# X_doc['combinesALL'] = comb_fe(X,cobdoc)\n",
    "# doc_col.append('combinesALL')\n",
    "\n",
    "# for col in tqdm(doc_col):\n",
    "#     X_doc[col] = X_doc[col].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rnn all including user tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:13:57.405648Z",
     "start_time": "2018-09-27T14:13:57.392957Z"
    }
   },
   "outputs": [],
   "source": [
    "# def comb_fe(X,cols,sep=' '):\n",
    "#     ret = pd.DataFrame()\n",
    "#     if cols[0] == 'user_tags':\n",
    "#         ret['comb'] = X['user_tags'].astype(str).apply(add_col_name,colName='userTags',splitter=',',sep=sep)\n",
    "#     else:\n",
    "#         ret['comb'] = X[cols[0]].astype(str).apply(add_col_name,colName=cols[0],splitter=' ',sep=sep)\n",
    "\n",
    "#     for col in tqdm(cols[1:]):\n",
    "#         if col == 'user_tags':\n",
    "#             ret['new_feature'] = X['user_tags'].astype(str).apply(add_col_name,colName='userTags',splitter=',',sep=sep)\n",
    "#         else:\n",
    "#             ret['new_feature'] = X[col].astype(str).apply(add_col_name,colName=col,splitter=' ',sep=sep)\n",
    "#         ret['comb'] =( ret['comb'] + sep + ret['new_feature']).values\n",
    "#     return ret['comb'].values\n",
    "\n",
    "\n",
    "# def add_col_name(x,colName,splitter=' ' ,sep=' '):\n",
    "#     ret = [colName+'_'+each for each in x.split(splitter)]\n",
    "#     return sep.join(ret)\n",
    "\n",
    "# non_doc_col = []\n",
    "# needsProcessCols = list(set(X.columns) - set(ignore_columns))\n",
    "# doc_col = ['combineALL']\n",
    "# X_doc = pd.DataFrame()\n",
    "# X_doc['combineALL'] = comb_fe(X,needsProcessCols,sep=' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:13:57.416893Z",
     "start_time": "2018-09-27T14:13:57.407048Z"
    }
   },
   "outputs": [],
   "source": [
    "need_process_col = list(set(X.columns) - set(ignore_columns))\n",
    "X_ = X[need_process_col].copy()\n",
    "\n",
    "\n",
    "doc_col = ['user_tags','model']\n",
    "non_doc_col = [f for f in need_process_col if f not in doc_col]\n",
    "# doc_col = doc_col + []\n",
    "# non_doc_col = non_doc_col+['user_tags']\n",
    "counter = 0\n",
    "\n",
    "X_doc = X[doc_col].copy()\n",
    "for col in tqdm(non_doc_col):\n",
    "    X_[col] = le.fit_transform(X_[col].astype(str))\n",
    "    X_[col] = col + '_'+X_[col].astype(str)\n",
    "    \n",
    "for col in tqdm(doc_col):\n",
    "    X_doc[col] = X_doc[col].astype(str)\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:13:57.428702Z",
     "start_time": "2018-09-27T14:13:57.418787Z"
    }
   },
   "outputs": [],
   "source": [
    "train_index = pickle.load(open(FILE.train_index.value,'rb'))\n",
    "holdout_index = pickle.load(open(FILE.holdout_index.value,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:14:03.100168Z",
     "start_time": "2018-09-27T14:13:57.431338Z"
    }
   },
   "outputs": [],
   "source": [
    "num_folds = 7\n",
    "seed = 1001\n",
    "train_index_list = []\n",
    "val_index_list = []\n",
    "folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=seed)\n",
    "for t,v in folds.split(train.loc[train_index],train.loc[train_index,'click']):\n",
    "    train_index_list.append(train.loc[train_index].iloc[t].index)\n",
    "    val_index_list.append(train.loc[train_index].iloc[v].index)\n",
    "    \n",
    "check = []\n",
    "for i in val_index_list:\n",
    "    check.extend(list(i))\n",
    "assert len(set(check+list(holdout_index))) == len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:14:03.602589Z",
     "start_time": "2018-09-27T14:14:03.101982Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fold_y = {}\n",
    "val_fold_y = {}\n",
    "holdout_y = train.loc[holdout_index,'click'].values\n",
    "for fold in range(num_folds):\n",
    "    train_fold_y[fold] = train.loc[train_index_list[fold],'click'].values\n",
    "    val_fold_y[fold] = train.loc[val_index_list[fold],'click'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:19:07.118825Z",
     "start_time": "2018-09-27T14:14:03.605101Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [04:07<00:00, 10.30s/it]\n",
      "100%|██████████| 3/3 [00:56<00:00, 18.79s/it]\n"
     ]
    }
   ],
   "source": [
    "info_dict = {}\n",
    "train_all_dict = {}\n",
    "train_fold_dict = {}\n",
    "\n",
    "val_fold_dict = {}\n",
    "\n",
    "holdout_input_dict = {}\n",
    "test_input_dict = {}\n",
    "maxlen = 1\n",
    "prefix_input_nonDoc = 'input_'\n",
    "prefix_input_Doc = 'input_rnn_'\n",
    "for col in tqdm(non_doc_col):\n",
    "\n",
    "    maxlen = 1\n",
    "    tok=text.Tokenizer(num_words=X_[col].nunique(),lower=False,filters='@')\n",
    "    tok.fit_on_texts(list(X_[col]))\n",
    "    info_dict.update({prefix_input_nonDoc+col:{'tok':tok}})\n",
    "    t = tok.texts_to_sequences(list(X_[col].iloc[:1001650].values))\n",
    "    te = tok.texts_to_sequences(list(X_[col].iloc[1001650:].values))\n",
    "    train_all_dict[prefix_input_nonDoc+col] = sequence.pad_sequences(t,maxlen=maxlen)\n",
    "    test_input_dict[prefix_input_nonDoc+col] = sequence.pad_sequences(te,maxlen=maxlen)\n",
    "    holdout_input_dict[prefix_input_nonDoc+col] = train_all_dict[prefix_input_nonDoc+col][list(holdout_index)]\n",
    "    \n",
    "    for fold in range(num_folds):\n",
    "        if train_fold_dict.get(fold) is None:\n",
    "            train_fold_dict[fold] = {}\n",
    "            val_fold_dict[fold] = {}\n",
    "        train_fold_dict[fold].update({prefix_input_nonDoc+col: train_all_dict[prefix_input_nonDoc+col][list(train_index_list[fold])]})\n",
    "        val_fold_dict[fold].update({prefix_input_nonDoc+col:train_all_dict[prefix_input_nonDoc+col][list(val_index_list[fold])]})\n",
    "        \n",
    "sequence_size_dict = {}\n",
    "for col in tqdm(doc_col):\n",
    "    if col == 'user_tags':\n",
    "        maxlen = 50\n",
    "        tok=text.Tokenizer(num_words=X_doc[col].nunique(),lower=False,filters=',')\n",
    "    else:\n",
    "        maxlen = 20\n",
    "        tok=text.Tokenizer(num_words=X_doc[col].nunique(),lower=False,filters=' ')\n",
    "    tok.fit_on_texts(list(X_doc[col]))\n",
    "    info_dict.update({prefix_input_Doc+col:{'tok':tok}})\n",
    "    sequence_size_dict[col] = maxlen\n",
    "    t = tok.texts_to_sequences(list(X_doc[col].iloc[:1001650].values))\n",
    "    te = tok.texts_to_sequences(list(X_doc[col].iloc[1001650:].values))\n",
    "    train_all_dict[prefix_input_Doc+col] = sequence.pad_sequences(t,maxlen=maxlen)\n",
    "    test_input_dict[prefix_input_Doc+col] = sequence.pad_sequences(te,maxlen=maxlen)\n",
    "    holdout_input_dict[prefix_input_Doc+col] = train_all_dict[prefix_input_Doc+col][list(holdout_index)]\n",
    "    \n",
    "    for fold in range(num_folds):\n",
    "        if train_fold_dict.get(fold) is None:\n",
    "            train_fold_dict[fold] = {}\n",
    "            val_fold_dict[fold] = {}\n",
    "        train_fold_dict[fold].update({prefix_input_Doc+col: train_all_dict[prefix_input_Doc+col][list(train_index_list[fold])]})\n",
    "        val_fold_dict[fold].update({prefix_input_Doc+col:train_all_dict[prefix_input_Doc+col][list(val_index_list[fold])]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:19:07.122401Z",
     "start_time": "2018-09-27T14:19:07.120396Z"
    }
   },
   "outputs": [],
   "source": [
    "# tok = info_dict['input_rnn_inner_slot_id']['tok']\n",
    "# tok.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build NN model only use model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.41463 -- 71 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:19:07.137740Z",
     "start_time": "2018-09-27T14:19:07.123647Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_nn_model(cols,doc_cols=[]):\n",
    "    \"\"\"\n",
    "    cols, used to do ebd and dense layers\n",
    "    doc_cols: used to do rnn\n",
    "    there can be overlaps\n",
    "    \"\"\"\n",
    "    input_list = []\n",
    "    concat_list = []\n",
    "    for col in cols:\n",
    "        max_feature = len(info_dict[prefix_input_nonDoc+col]['tok'].index_word)\n",
    "        embed_size = int(np.log2(max_feature)/np.log2(1.5))\n",
    "        if embed_size< 2:\n",
    "            embed_size = 2\n",
    "        cur_input = Input(shape=(1, ),name = prefix_input_nonDoc+col)\n",
    "        \n",
    "       \n",
    "        embed_layer = Embedding(max_feature,\n",
    "                            embed_size,\n",
    "                            input_length=1,\n",
    "                            trainable=True,\n",
    "                            embeddings_regularizer=l2(0.0005),\n",
    "                            name='ebd_'+col)(cur_input)\n",
    "        embed_layer = SpatialDropout1D(0.5)(embed_layer)\n",
    "        x = Flatten()(embed_layer)\n",
    "        input_list.append(cur_input)\n",
    "        concat_list.append(x)\n",
    "    for col in doc_cols:\n",
    "        max_feature = len(info_dict[prefix_input_Doc+col]['tok'].index_word)\n",
    "        embed_size = int(np.log2(max_feature)/np.log2(1.5))\n",
    "        if embed_size< 2:\n",
    "            embed_size = 2\n",
    "        input_shape = sequence_size_dict[col]\n",
    "        cur_input = Input(shape=(input_shape, ),name = prefix_input_Doc+col)\n",
    "        embed_layer = Embedding(max_feature,\n",
    "                            embed_size,\n",
    "                            input_length=input_shape,\n",
    "                            trainable=True,\n",
    "                            embeddings_regularizer=l2(0.0005),\n",
    "                            name='ebd_rnn_'+col)(cur_input)\n",
    "        x = SpatialDropout1D(0.5)(embed_layer)\n",
    "        x = Bidirectional(CuDNNGRU(50, return_sequences=True))(x)\n",
    "        x = Conv1D(32, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "        x_aveP = GlobalAveragePooling1D()(x)\n",
    "        x_maxP = GlobalMaxPooling1D()(x)\n",
    "        x = concatenate([x_aveP, x_maxP]) \n",
    "        input_list.append(cur_input)\n",
    "        concat_list.append(x)\n",
    "#         concat_list.append(x)\n",
    "       \n",
    "    if len(concat_list) > 1:\n",
    "        x = concatenate(concat_list)\n",
    "#     x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "#     x = Dense(512)(x)\n",
    "\n",
    "#     x = Activation('relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    preds = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(input_list, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:19:07.149205Z",
     "start_time": "2018-09-27T14:19:07.139133Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# def get_nn_model(cols,doc_cols=[]):\n",
    "#     input_list = []\n",
    "#     concat_list = []\n",
    "#     reg = 0.0005\n",
    "#     for col in cols:\n",
    "#         max_feature = len(info_dict[col]['tok'].index_word)\n",
    "#         embed_size = int(np.log2(max_feature)/np.log2(1.5))\n",
    "#         if embed_size< 2:\n",
    "#             embed_size = 2\n",
    "#         if col in doc_cols:\n",
    "#             cur_input = Input(shape=(50, ),name = 'input_'+col)\n",
    "#             embed_layer = Embedding(max_feature,\n",
    "#                                 embed_size,\n",
    "#                                 input_length=50,\n",
    "#                                 trainable=True,\n",
    "#                                 embeddings_regularizer=l2(reg),\n",
    "#                                 name='ebd_'+col)(cur_input)\n",
    "#             x = SpatialDropout1D(0.5)(embed_layer)\n",
    "#             x = Bidirectional(CuDNNGRU(50, return_sequences=True))(x)\n",
    "#             x = Conv1D(32, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "#             x_aveP = GlobalAveragePooling1D()(x)\n",
    "#             x_maxP = GlobalMaxPooling1D()(x)\n",
    "#             x = concatenate([x_aveP, x_maxP]) \n",
    "# #             x = Flatten()(x)\n",
    "#             concat_list.append(x)\n",
    "            \n",
    "#         else:\n",
    "#             cur_input = Input(shape=(1, ),name = 'input_'+col)\n",
    "        \n",
    "       \n",
    "#             embed_layer = Embedding(max_feature,\n",
    "#                                 embed_size,\n",
    "#                                 input_length=1,\n",
    "#                                 trainable=True,\n",
    "#                                 embeddings_regularizer=l2(reg),\n",
    "#                                 name='ebd_'+col)(cur_input)\n",
    "#             embed_layer = SpatialDropout1D(0.5)(embed_layer)\n",
    "# #             x = Conv1D(embed_size*2, kernel_size = 1, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(embed_layer)\n",
    "# #             x_aveP = GlobalAveragePooling1D()(x)\n",
    "# #             x_maxP = GlobalMaxPooling1D()(x)\n",
    "# #             x = concatenate([x_aveP, x_maxP]) \n",
    "#             x = Flatten()(embed_layer)\n",
    "#         input_list.append(cur_input)\n",
    "#         concat_list.append(x)\n",
    "    \n",
    "#     x = concatenate(concat_list)\n",
    "# #     x = BatchNormalization()(x)\n",
    "#     x = Dense(512, activation='relu')(x)\n",
    "#     x = Dropout(0.2)(x)\n",
    "# #     x = Dense(512)(x)\n",
    "\n",
    "# #     x = Activation('relu')(x)\n",
    "#     x = Dense(128, activation='relu')(x)\n",
    "#     x = Dropout(0.2)(x)\n",
    "\n",
    "#     preds = Dense(1, activation=\"sigmoid\")(x)\n",
    "#     model = Model(input_list, preds)\n",
    "#     model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:19:07.161111Z",
     "start_time": "2018-09-27T14:19:07.151065Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_each_fold(input_train_dict,input_val_dict,y_train,y_val,cols,doc_col=[],tolerance=30,):\n",
    "    model = get_nn_model(cols,doc_col)\n",
    "    cur_to = 0\n",
    "    best_logloss = None\n",
    "    best_weights = None\n",
    "    count = 0\n",
    "    base_lr = 0.001\n",
    "    while True:\n",
    "#         decay = (80 - count)/80\n",
    "#         decay = max(decay,0.1)\n",
    "#         lr = base_lr * decay\n",
    "#         K.set_value(model.optimizer.lr, lr)\n",
    "        model.fit(input_train_dict, y_train, \n",
    "                  batch_size=5000, \n",
    "                  epochs=1,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  )\n",
    "        preds = model.predict(input_val_dict,5000,verbose=1)\n",
    "        logloss = log_loss(y_val,preds)\n",
    "        roc = roc_auc_score(y_val,preds)\n",
    "        print(logloss)\n",
    "        print(roc)\n",
    "        if best_logloss is None:\n",
    "            best_logloss = logloss\n",
    "            best_weights = model.get_weights()\n",
    "        else:\n",
    "            if best_logloss > logloss:\n",
    "                best_logloss = logloss\n",
    "                best_weights = model.get_weights()\n",
    "                cur_to = 0\n",
    "            else:\n",
    "                cur_to +=1\n",
    "        if cur_to == tolerance:\n",
    "            break\n",
    "        print('best logloss is: {}'.format(best_logloss))\n",
    "        print('remainning trial is: {}/{}'.format(cur_to,tolerance))\n",
    "        print('total epoch trained: {}'.format(count))\n",
    "        count+=1\n",
    "    model.set_weights(best_weights)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:19:07.169379Z",
     "start_time": "2018-09-27T14:19:07.162920Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = train_each_fold(t_notag,v_notag,train_fold_y[0],val_fold_y[0],col_notag,doc_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:58:44.960512Z",
     "start_time": "2018-09-27T14:19:07.171473Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 24s 29us/step - loss: 0.4597 - acc: 0.7986\n",
      "137376/137376 [==============================] - 2s 12us/step\n",
      "0.4206240229920487\n",
      "0.7599462012943433\n",
      "best logloss is: 0.4206240229920487\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 0\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 26us/step - loss: 0.4259 - acc: 0.8041\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41803811494233883\n",
      "0.7632496483493093\n",
      "best logloss is: 0.41803811494233883\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 1\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 26us/step - loss: 0.4244 - acc: 0.8048\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.418018104771168\n",
      "0.7636277416358157\n",
      "best logloss is: 0.418018104771168\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 2\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 26us/step - loss: 0.4236 - acc: 0.8052\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4175653872157127\n",
      "0.7643206129819407\n",
      "best logloss is: 0.4175653872157127\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 3\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4233 - acc: 0.8052\n",
      "137376/137376 [==============================] - 1s 9us/step\n",
      "0.4183466879432787\n",
      "0.7642007461661436\n",
      "best logloss is: 0.4175653872157127\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 4\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 26us/step - loss: 0.4233 - acc: 0.8050\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4174942541204015\n",
      "0.7651305065280923\n",
      "best logloss is: 0.4174942541204015\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 5\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4228 - acc: 0.8055\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41818026416363874\n",
      "0.7653059191054955\n",
      "best logloss is: 0.4174942541204015\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 6\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4227 - acc: 0.8053\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4170892656539661\n",
      "0.7655384294019942\n",
      "best logloss is: 0.4170892656539661\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 7\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4221 - acc: 0.8057\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41702406838804834\n",
      "0.7655615520835417\n",
      "best logloss is: 0.41702406838804834\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 8\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4220 - acc: 0.8055\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41659069300908547\n",
      "0.766055761024\n",
      "best logloss is: 0.41659069300908547\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 9\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4220 - acc: 0.8056\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4162092033370372\n",
      "0.7672498822685649\n",
      "best logloss is: 0.4162092033370372\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 10\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4218 - acc: 0.8058\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4162456486186665\n",
      "0.7666573643882084\n",
      "best logloss is: 0.4162092033370372\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 11\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4216 - acc: 0.8059\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41615563199829037\n",
      "0.7670041138400108\n",
      "best logloss is: 0.41615563199829037\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 12\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4214 - acc: 0.8060\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4162045130077808\n",
      "0.767238428385121\n",
      "best logloss is: 0.41615563199829037\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 13\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 26us/step - loss: 0.4212 - acc: 0.8060\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41623988229692704\n",
      "0.766734833614237\n",
      "best logloss is: 0.41615563199829037\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 14\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 26us/step - loss: 0.4212 - acc: 0.8060\n",
      "137376/137376 [==============================] - 1s 9us/step\n",
      "0.416053244893765\n",
      "0.7673135093553117\n",
      "best logloss is: 0.416053244893765\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 15\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 26us/step - loss: 0.4211 - acc: 0.8062\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41609058596672477\n",
      "0.7673056773178271\n",
      "best logloss is: 0.416053244893765\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 16\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4208 - acc: 0.8062\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4161526776840774\n",
      "0.7676189399621226\n",
      "best logloss is: 0.416053244893765\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 17\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 26us/step - loss: 0.4209 - acc: 0.8061\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41593760362957\n",
      "0.7676899415584069\n",
      "best logloss is: 0.41593760362957\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 18\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 21s 26us/step - loss: 0.4210 - acc: 0.8064\n",
      "137376/137376 [==============================] - 1s 9us/step\n",
      "0.4157007381973578\n",
      "0.7674115086045208\n",
      "best logloss is: 0.4157007381973578\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 19\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 21s 26us/step - loss: 0.4206 - acc: 0.8063\n",
      "137376/137376 [==============================] - 1s 8us/step\n",
      "0.4157823775551069\n",
      "0.7682604161694297\n",
      "best logloss is: 0.4157007381973578\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 20\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4205 - acc: 0.8063\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4160348619196906\n",
      "0.7678025875419967\n",
      "best logloss is: 0.4157007381973578\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 21\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4205 - acc: 0.8064\n",
      "137376/137376 [==============================] - 1s 9us/step\n",
      "0.41536934660617153\n",
      "0.7687072642929906\n",
      "best logloss is: 0.41536934660617153\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 22\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 26us/step - loss: 0.4205 - acc: 0.8061\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4158007649811645\n",
      "0.7675015284795648\n",
      "best logloss is: 0.41536934660617153\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 23\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 26us/step - loss: 0.4204 - acc: 0.8064\n",
      "137376/137376 [==============================] - 1s 9us/step\n",
      "0.4153922614423603\n",
      "0.7684724907182003\n",
      "best logloss is: 0.41536934660617153\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 24\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 21s 26us/step - loss: 0.4204 - acc: 0.8064\n",
      "137376/137376 [==============================] - 1s 9us/step\n",
      "0.4156788611655182\n",
      "0.7684666514802316\n",
      "best logloss is: 0.41536934660617153\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 25\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 21s 26us/step - loss: 0.4203 - acc: 0.8064\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4154309415100057\n",
      "0.7684344628373598\n",
      "best logloss is: 0.41536934660617153\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 26\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4203 - acc: 0.8062\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4158420888763395\n",
      "0.768261717003812\n",
      "best logloss is: 0.41536934660617153\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 27\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4201 - acc: 0.8063\n",
      "137376/137376 [==============================] - 1s 9us/step\n",
      "0.4156914688314206\n",
      "0.7682207120209898\n",
      "best logloss is: 0.41536934660617153\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 28\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4201 - acc: 0.8066\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41561091224496616\n",
      "0.7681715027377909\n",
      "best logloss is: 0.41536934660617153\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 29\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4202 - acc: 0.8066\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41566145777846925\n",
      "0.7685001157617456\n",
      "best logloss is: 0.41536934660617153\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 30\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4200 - acc: 0.8065\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4154565660126239\n",
      "0.7687154737660027\n",
      "best logloss is: 0.41536934660617153\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 31\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4201 - acc: 0.8065\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41556202629860295\n",
      "0.768129030728816\n",
      "best logloss is: 0.41536934660617153\n",
      "remainning trial is: 10/30\n",
      "total epoch trained: 32\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4200 - acc: 0.8065\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4152643860827589\n",
      "0.7683218043393294\n",
      "best logloss is: 0.4152643860827589\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 33\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4198 - acc: 0.8066\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41540956476094265\n",
      "0.7683996838721495\n",
      "best logloss is: 0.4152643860827589\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 34\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4198 - acc: 0.8066\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4154305059116105\n",
      "0.7683478021718824\n",
      "best logloss is: 0.4152643860827589\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 35\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4199 - acc: 0.8067\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41548078136837935\n",
      "0.7687203550652608\n",
      "best logloss is: 0.4152643860827589\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 36\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4198 - acc: 0.8065\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41561302154159285\n",
      "0.7683395881936717\n",
      "best logloss is: 0.4152643860827589\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 37\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4198 - acc: 0.8067\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4154556136255668\n",
      "0.7686983585165053\n",
      "best logloss is: 0.4152643860827589\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 38\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4195 - acc: 0.8067\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41569730161790025\n",
      "0.7691160957152776\n",
      "best logloss is: 0.4152643860827589\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 39\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4196 - acc: 0.8068\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4158946274021086\n",
      "0.768750366862911\n",
      "best logloss is: 0.4152643860827589\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 40\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4197 - acc: 0.8066\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41498779232772165\n",
      "0.7694331188655341\n",
      "best logloss is: 0.41498779232772165\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 41\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4195 - acc: 0.8067\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4154366325987006\n",
      "0.7683622263160802\n",
      "best logloss is: 0.41498779232772165\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 42\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4196 - acc: 0.8068\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41562425840447853\n",
      "0.7681521262123127\n",
      "best logloss is: 0.41498779232772165\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 43\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4197 - acc: 0.8069\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41512497197800213\n",
      "0.7691627375358497\n",
      "best logloss is: 0.41498779232772165\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 44\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4194 - acc: 0.8068\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41603702439842505\n",
      "0.7688056529915901\n",
      "best logloss is: 0.41498779232772165\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 45\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4195 - acc: 0.8067\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41526878347872165\n",
      "0.7691375276124748\n",
      "best logloss is: 0.41498779232772165\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 46\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4194 - acc: 0.8069\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41511124026156293\n",
      "0.7688290279642593\n",
      "best logloss is: 0.41498779232772165\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 47\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4194 - acc: 0.8067\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4151178400434056\n",
      "0.7690393393117837\n",
      "best logloss is: 0.41498779232772165\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 48\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4194 - acc: 0.8066\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4155727759398087\n",
      "0.7688835433702585\n",
      "best logloss is: 0.41498779232772165\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 49\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4193 - acc: 0.8067\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41512714149001534\n",
      "0.7691244904020103\n",
      "best logloss is: 0.41498779232772165\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 50\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4194 - acc: 0.8067\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41545864248638403\n",
      "0.7692563457181525\n",
      "best logloss is: 0.41498779232772165\n",
      "remainning trial is: 10/30\n",
      "total epoch trained: 51\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4191 - acc: 0.8068\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41495911786984535\n",
      "0.7693966441101974\n",
      "best logloss is: 0.41495911786984535\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 52\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4192 - acc: 0.8070\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4150833379632915\n",
      "0.7689383028912786\n",
      "best logloss is: 0.41495911786984535\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 53\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4192 - acc: 0.8072\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41499338161529115\n",
      "0.7695216593668439\n",
      "best logloss is: 0.41495911786984535\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 54\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4191 - acc: 0.8068\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4153652890724168\n",
      "0.7685166381604785\n",
      "best logloss is: 0.41495911786984535\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 55\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4193 - acc: 0.8069\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41518503487911196\n",
      "0.7690420626209105\n",
      "best logloss is: 0.41495911786984535\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 56\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4191 - acc: 0.8069\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4154537005621384\n",
      "0.7688781831850748\n",
      "best logloss is: 0.41495911786984535\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 57\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4191 - acc: 0.8070\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41532855017201914\n",
      "0.769000601611874\n",
      "best logloss is: 0.41495911786984535\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 58\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4190 - acc: 0.8072\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4150700882275774\n",
      "0.76964776121065\n",
      "best logloss is: 0.41495911786984535\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 59\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4190 - acc: 0.8068\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41536063075953095\n",
      "0.7689415232739847\n",
      "best logloss is: 0.41495911786984535\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 60\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4189 - acc: 0.8070\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41498214417477125\n",
      "0.7695221838053332\n",
      "best logloss is: 0.41495911786984535\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 61\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4192 - acc: 0.8069\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4155966312094713\n",
      "0.7692112500149977\n",
      "best logloss is: 0.41495911786984535\n",
      "remainning trial is: 10/30\n",
      "total epoch trained: 62\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4189 - acc: 0.8072\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4152846194416245\n",
      "0.7690114583062142\n",
      "best logloss is: 0.41495911786984535\n",
      "remainning trial is: 11/30\n",
      "total epoch trained: 63\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4190 - acc: 0.8071\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4154045506874749\n",
      "0.7686567344860231\n",
      "best logloss is: 0.41495911786984535\n",
      "remainning trial is: 12/30\n",
      "total epoch trained: 64\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4190 - acc: 0.8072\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4158894401449005\n",
      "0.768941757544312\n",
      "best logloss is: 0.41495911786984535\n",
      "remainning trial is: 13/30\n",
      "total epoch trained: 65\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4187 - acc: 0.8069\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4149476838024966\n",
      "0.7693057851001985\n",
      "best logloss is: 0.4149476838024966\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 66\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4190 - acc: 0.8071\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41524281493879023\n",
      "0.7688831809520599\n",
      "best logloss is: 0.4149476838024966\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 67\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4189 - acc: 0.8070\n",
      "137376/137376 [==============================] - 1s 9us/step\n",
      "0.4150232929479516\n",
      "0.7691720714730617\n",
      "best logloss is: 0.4149476838024966\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 68\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4187 - acc: 0.8070\n",
      "137376/137376 [==============================] - 1s 9us/step\n",
      "0.4155849588457271\n",
      "0.7690634406225835\n",
      "best logloss is: 0.4149476838024966\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 69\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4188 - acc: 0.8070\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41530521094833805\n",
      "0.7692002464845549\n",
      "best logloss is: 0.4149476838024966\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 70\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4188 - acc: 0.8070\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4152541842053373\n",
      "0.7694619955199951\n",
      "best logloss is: 0.4149476838024966\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 71\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4188 - acc: 0.8069\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41518547370926007\n",
      "0.7690661302261502\n",
      "best logloss is: 0.4149476838024966\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 72\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4187 - acc: 0.8070\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41524418501417004\n",
      "0.7692622036443524\n",
      "best logloss is: 0.4149476838024966\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 73\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4187 - acc: 0.8073\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4153531349328641\n",
      "0.7689762905590503\n",
      "best logloss is: 0.4149476838024966\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 74\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4185 - acc: 0.8071\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4154664517835463\n",
      "0.7688426186467154\n",
      "best logloss is: 0.4149476838024966\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 75\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4187 - acc: 0.8072\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4154217738257988\n",
      "0.7690743386980071\n",
      "best logloss is: 0.4149476838024966\n",
      "remainning trial is: 10/30\n",
      "total epoch trained: 76\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4187 - acc: 0.8072\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4149669930831881\n",
      "0.7692411486821051\n",
      "best logloss is: 0.4149476838024966\n",
      "remainning trial is: 11/30\n",
      "total epoch trained: 77\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4185 - acc: 0.8071\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4151818154565125\n",
      "0.7690585426374046\n",
      "best logloss is: 0.4149476838024966\n",
      "remainning trial is: 12/30\n",
      "total epoch trained: 78\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4186 - acc: 0.8074\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4149858310579735\n",
      "0.7692751248876677\n",
      "best logloss is: 0.4149476838024966\n",
      "remainning trial is: 13/30\n",
      "total epoch trained: 79\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4185 - acc: 0.8071\n",
      "137376/137376 [==============================] - 1s 9us/step\n",
      "0.4150490641746839\n",
      "0.7694165978016747\n",
      "best logloss is: 0.4149476838024966\n",
      "remainning trial is: 14/30\n",
      "total epoch trained: 80\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4187 - acc: 0.8073\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41489848697381065\n",
      "0.7695624696249708\n",
      "best logloss is: 0.41489848697381065\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 81\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4185 - acc: 0.8075\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4151350116307256\n",
      "0.7692105391947734\n",
      "best logloss is: 0.41489848697381065\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 82\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4185 - acc: 0.8072\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4152721500511834\n",
      "0.7693583572638666\n",
      "best logloss is: 0.41489848697381065\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 83\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4184 - acc: 0.8072\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4151981654118155\n",
      "0.7693664873118958\n",
      "best logloss is: 0.41489848697381065\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 84\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4183 - acc: 0.8074\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4151901455344485\n",
      "0.7689151945596231\n",
      "best logloss is: 0.41489848697381065\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 85\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4185 - acc: 0.8072\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4155091175450735\n",
      "0.769281799756548\n",
      "best logloss is: 0.41489848697381065\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 86\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4184 - acc: 0.8073\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4150689322549413\n",
      "0.7694354223568944\n",
      "best logloss is: 0.41489848697381065\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 87\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4183 - acc: 0.8074\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4154737169677017\n",
      "0.7689288144424404\n",
      "best logloss is: 0.41489848697381065\n",
      "remainning trial is: 7/30\n",
      "total epoch trained: 88\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4185 - acc: 0.8071\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41527046904818365\n",
      "0.7692751427416029\n",
      "best logloss is: 0.41489848697381065\n",
      "remainning trial is: 8/30\n",
      "total epoch trained: 89\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4184 - acc: 0.8076\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41511929923496343\n",
      "0.7691258049188474\n",
      "best logloss is: 0.41489848697381065\n",
      "remainning trial is: 9/30\n",
      "total epoch trained: 90\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4185 - acc: 0.8074\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4151022814029588\n",
      "0.7695012481474194\n",
      "best logloss is: 0.41489848697381065\n",
      "remainning trial is: 10/30\n",
      "total epoch trained: 91\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4182 - acc: 0.8075\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4155165852427249\n",
      "0.7685662609207937\n",
      "best logloss is: 0.41489848697381065\n",
      "remainning trial is: 11/30\n",
      "total epoch trained: 92\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4182 - acc: 0.8074\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41499020935862824\n",
      "0.7698151260016132\n",
      "best logloss is: 0.41489848697381065\n",
      "remainning trial is: 12/30\n",
      "total epoch trained: 93\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4183 - acc: 0.8074\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4148061409158921\n",
      "0.7696630710435217\n",
      "best logloss is: 0.4148061409158921\n",
      "remainning trial is: 0/30\n",
      "total epoch trained: 94\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4183 - acc: 0.8072\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4153722740707368\n",
      "0.7688578492214635\n",
      "best logloss is: 0.4148061409158921\n",
      "remainning trial is: 1/30\n",
      "total epoch trained: 95\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4182 - acc: 0.8073\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41500790990191133\n",
      "0.7694189194806887\n",
      "best logloss is: 0.4148061409158921\n",
      "remainning trial is: 2/30\n",
      "total epoch trained: 96\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4182 - acc: 0.8074\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4150579995432917\n",
      "0.7695038174454973\n",
      "best logloss is: 0.4148061409158921\n",
      "remainning trial is: 3/30\n",
      "total epoch trained: 97\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4181 - acc: 0.8072\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41560460565475055\n",
      "0.7683791461734457\n",
      "best logloss is: 0.4148061409158921\n",
      "remainning trial is: 4/30\n",
      "total epoch trained: 98\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4181 - acc: 0.8076\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.41529445363045653\n",
      "0.7690771529454019\n",
      "best logloss is: 0.4148061409158921\n",
      "remainning trial is: 5/30\n",
      "total epoch trained: 99\n",
      "Epoch 1/1\n",
      "824250/824250 [==============================] - 22s 27us/step - loss: 0.4181 - acc: 0.8076\n",
      "137376/137376 [==============================] - 1s 10us/step\n",
      "0.4148933017557706\n",
      "0.769390529387677\n",
      "best logloss is: 0.4148061409158921\n",
      "remainning trial is: 6/30\n",
      "total epoch trained: 100\n",
      "Epoch 1/1\n",
      "490000/824250 [================>.............] - ETA: 8s - loss: 0.4184 - acc: 0.8067"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m?\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_each_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fold_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_fold_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_fold_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_fold_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnon_doc_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdoc_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m?\u001b[0m in \u001b[0;36mtrain_each_fold\u001b[0;34m(input_train_dict, input_val_dict, y_train, y_val, cols, doc_col, tolerance)\u001b[0m\n\u001b[1;32m     15\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                   )\n\u001b[1;32m     19\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_val_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2670\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2672\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2652\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2654\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2655\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_each_fold(train_fold_dict[0],val_fold_dict[0],train_fold_y[0],val_fold_y[0],non_doc_col,doc_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:58:44.961160Z",
     "start_time": "2018-09-27T14:13:11.360Z"
    }
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:58:44.961867Z",
     "start_time": "2018-09-27T14:13:11.363Z"
    }
   },
   "outputs": [],
   "source": [
    "input_val_dict = val_fold_dict[0]\n",
    "y_val = val_fold_y[0]\n",
    "preds = model.predict(input_val_dict,5000,verbose=1)\n",
    "logloss = log_loss(y_val,preds)\n",
    "roc = roc_auc_score(y_val,preds)\n",
    "print(logloss)\n",
    "print(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:58:44.962486Z",
     "start_time": "2018-09-27T14:13:11.365Z"
    }
   },
   "outputs": [],
   "source": [
    "# predsub = model.predict(test_input_dict,5000,verbose=1)\n",
    "# test_save = test[['instance_id']].copy()\n",
    "# test_save['predicted_score'] = predsub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:58:44.963091Z",
     "start_time": "2018-09-27T14:13:11.368Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_save.to_csv('ebd_nn.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:58:44.963637Z",
     "start_time": "2018-09-27T14:13:11.370Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_nn_model(need_process_col,doc_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:58:44.964165Z",
     "start_time": "2018-09-27T14:13:11.373Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T14:58:44.964691Z",
     "start_time": "2018-09-27T14:13:11.376Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
