{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T08:20:22.048522Z",
     "start_time": "2018-09-21T08:20:22.042685Z"
    }
   },
   "outputs": [],
   "source": [
    "__file__=''\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../LIB/'))\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../../../../automl/automl_libs/'))\n",
    "from env import FILE\n",
    "from sklearn.metrics import log_loss,roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU,CuDNNGRU,Flatten,BatchNormalization\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T08:20:37.773946Z",
     "start_time": "2018-09-21T08:20:36.684442Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "train_doc = pd.read_pickle('../../data/fe_doc/train_doc.pkl')\n",
    "test_doc = pd.read_pickle('../../data/fe_doc/test_doc.pkl')\n",
    "\n",
    "\n",
    "\n",
    "holdout_index = pickle.load(open(FILE.holdout_index.value,'rb'))\n",
    "train_index = pickle.load(open(FILE.train_index.value,'rb'))\n",
    "train_cv = train_doc.loc[train_index].copy()\n",
    "holdout = train_doc.loc[holdout_index].copy()\n",
    "\n",
    "holdout_label = holdout.click.values\n",
    "label_name = FILE.Vectorize_label_format.value.format('holdout')\n",
    "# pickle.dump(holdout.click.values,open(label_name,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T08:21:40.345874Z",
     "start_time": "2018-09-21T08:20:52.807963Z"
    }
   },
   "outputs": [],
   "source": [
    "ct = CountVectorizer(token_pattern=r'(?u)\\b[\\w.-]+\\b',\n",
    "                max_features=500000)\n",
    "ctvector = ct.fit(pd.concat([train_doc['text'],test_doc['text']]))\n",
    "\n",
    "# train_cv_matrix = ctvector.transform(train_cv['text'])\n",
    "holdout_matrix = ctvector.transform(holdout['text'])\n",
    "test_matrix = ctvector.transform(test_doc['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T08:21:58.306091Z",
     "start_time": "2018-09-21T08:21:58.297652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40024, 25422)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T08:22:32.486903Z",
     "start_time": "2018-09-21T08:22:32.480353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 25422)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = np.sum(test_matrix,axis=0)\n",
    "s1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T08:26:00.336473Z",
     "start_time": "2018-09-21T08:26:00.329990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 8.37937837,  1.        ,  5.357552  , ...,  7.83920379,\n",
       "         13.69185255,  5.169925  ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(s1+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T08:24:17.502606Z",
     "start_time": "2018-09-21T08:24:17.467862Z"
    }
   },
   "outputs": [],
   "source": [
    "s2 = test_matrix.multiply(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T08:25:01.640451Z",
     "start_time": "2018-09-21T08:25:01.618442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[   110224,         1,      1600, ...,     51984, 175085824,\n",
       "              1225]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(s2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T19:11:21.288047Z",
     "start_time": "2018-09-20T19:11:21.285697Z"
    }
   },
   "outputs": [],
   "source": [
    "# # n_fold = 5\n",
    "# # sf = StratifiedKFold(n_splits=5,shuffle= True,random_state=20)\n",
    "# # train_index_list = []\n",
    "# # val_index_list = []\n",
    "# # fold = 0\n",
    "# # for t,v in sf.split(train_cv,train_cv.click):\n",
    "# #     print('here')\n",
    "# #     train_cur_index = train_cv.iloc[t].index\n",
    "# #     train_cur_label = train_cv.iloc[t].click.values\n",
    "# #     index_name = FILE.Vectorize_index_format.value.format('train_fold_{}'.format(fold))\n",
    "# #     label_name = FILE.Vectorize_label_format.value.format('train_fold_{}'.format(fold))\n",
    "# #     pickle.dump(train_cur_index,open(index_name,'wb'))\n",
    "# #     pickle.dump(train_cur_label,open(label_name,'wb'))\n",
    "# #     val_cur_index = train_cv.iloc[v].index\n",
    "# #     val_cur_label =  train_cv.iloc[v].click.values\n",
    "# #     index_name = FILE.Vectorize_index_format.value.format('val_fold_{}'.format(fold))\n",
    "# #     label_name = FILE.Vectorize_label_format.value.format('val_fold_{}'.format(fold))\n",
    "# #     pickle.dump(val_cur_index,open(index_name,'wb'))\n",
    "# #     pickle.dump(val_cur_label,open(label_name,'wb'))\n",
    "# #     fold += 1\n",
    "# label_name = FILE.Vectorize_label_format.value.format('holdout')\n",
    "# pickle.dump(holdout.click.values,open(label_name,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T20:02:34.469175Z",
     "start_time": "2018-09-20T19:57:56.448443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train index length:826135\n",
      "train label length:826135\n",
      "train matrix shape:(826135, 25422)\n",
      "val index length:135491\n",
      "val label length:135491\n",
      "val matrix shape:(135491, 25422)\n",
      "train index length:818892\n",
      "train label length:818892\n",
      "train matrix shape:(818892, 25422)\n",
      "val index length:142734\n",
      "val label length:142734\n",
      "val matrix shape:(142734, 25422)\n",
      "train index length:816463\n",
      "train label length:816463\n",
      "train matrix shape:(816463, 25422)\n",
      "val index length:145163\n",
      "val label length:145163\n",
      "val matrix shape:(145163, 25422)\n",
      "train index length:832563\n",
      "train label length:832563\n",
      "train matrix shape:(832563, 25422)\n",
      "val index length:129063\n",
      "val label length:129063\n",
      "val matrix shape:(129063, 25422)\n",
      "train index length:820939\n",
      "train label length:820939\n",
      "train matrix shape:(820939, 25422)\n",
      "val index length:140687\n",
      "val label length:140687\n",
      "val matrix shape:(140687, 25422)\n",
      "train index length:810325\n",
      "train label length:810325\n",
      "train matrix shape:(810325, 25422)\n",
      "val index length:151301\n",
      "val label length:151301\n",
      "val matrix shape:(151301, 25422)\n",
      "train index length:844439\n",
      "train label length:844439\n",
      "train matrix shape:(844439, 25422)\n",
      "val index length:117187\n",
      "val label length:117187\n",
      "val matrix shape:(117187, 25422)\n",
      "(40024, 25422)\n",
      "(40024, 25422)\n"
     ]
    }
   ],
   "source": [
    "day_index = pickle.load(open(FILE.train_7_fold_index.value,'rb'))\n",
    "\n",
    "for key,(v,t) in day_index.items():\n",
    "    index_name = FILE.Vectorize_index_format.value.format('train_fold_{}'.format(key))\n",
    "    print('train index length:{}'.format(len(t)))\n",
    "    t_labels = train_cv.loc[t,'click'].values\n",
    "    label_name = FILE.Vectorize_label_format.value.format('train_fold_{}'.format(key))\n",
    "    print('train label length:{}'.format(len(t_labels)))\n",
    "    matrix_train = ctvector.transform(train_cv.loc[t]['text'])\n",
    "    print('train matrix shape:{}'.format(matrix_train.shape))\n",
    "    name = 'train_fold_{}'.format(key)\n",
    "    save_name = FILE.tfidfVectorize_format.value.format(name)\n",
    "    pickle.dump(matrix_train,open(save_name,'wb'))\n",
    "    pickle.dump(t,open(index_name,'wb'))\n",
    "    pickle.dump(t_labels,open(label_name,'wb'))\n",
    "    \n",
    "    #### validation\n",
    "    index_name = FILE.Vectorize_index_format.value.format('val_fold_{}'.format(key))\n",
    "    print('val index length:{}'.format(len(v)))\n",
    "    v_labels = train_cv.loc[v,'click'].values\n",
    "    label_name = FILE.Vectorize_label_format.value.format('val_fold_{}'.format(key))\n",
    "    print('val label length:{}'.format(len(v_labels)))\n",
    "    matrix_val = ctvector.transform(train_cv.loc[v]['text'])\n",
    "    print('val matrix shape:{}'.format(matrix_val.shape))\n",
    "    name = 'val_fold_{}'.format(key)\n",
    "    save_name = FILE.tfidfVectorize_format.value.format(name)\n",
    "    pickle.dump(matrix_val,open(save_name,'wb'))\n",
    "    pickle.dump(v,open(index_name,'wb'))\n",
    "    pickle.dump(v_labels,open(label_name,'wb'))\n",
    "    \n",
    "    \n",
    "save_name = FILE.tfidfVectorize_format.value.format('holdout')\n",
    "pickle.dump(holdout_matrix,open(save_name,'wb'))\n",
    "print(holdout_matrix.shape)\n",
    "\n",
    "save_name = FILE.tfidfVectorize_format.value.format('test')\n",
    "pickle.dump(test_matrix,open(save_name,'wb'))\n",
    "print(test_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T19:51:24.030692Z",
     "start_time": "2018-09-20T19:51:23.952865Z"
    }
   },
   "outputs": [],
   "source": [
    "day_index = pickle.load(open(FILE.train_7_fold_index.value,'rb'))\n",
    "val_index_list = []\n",
    "\n",
    "for key,(v,t) in day_index.items():\n",
    "\n",
    "    val_index_list.extend(list(v))\n",
    "val_index_list.extend(list(holdout_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T19:51:35.191256Z",
     "start_time": "2018-09-20T19:51:35.124430Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001650"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(val_index_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T13:31:24.546196Z",
     "start_time": "2018-09-19T13:31:24.542676Z"
    }
   },
   "outputs": [],
   "source": [
    "# n_fold = 5\n",
    "# sf = StratifiedKFold(n_splits=5,shuffle= True,random_state=20)\n",
    "# train_index_list = []\n",
    "# val_index_list = []\n",
    "# for t,v in sf.split(train_cv,train_cv.click):\n",
    "#     train_index_list.append(t)\n",
    "#     val_index_list.append(v)\n",
    "    \n",
    "# for fold in range(len(train_index_list)):\n",
    "#     matrix_train = ctvector.transform(train_cv.iloc[train_index_list[fold]]['text'])\n",
    "#     print(matrix_train.shape)\n",
    "#     name = 'train_fold_{}'.format(fold)\n",
    "#     save_name = FILE.tfidfVectorize_format.value.format(name)\n",
    "#     pickle.dump(matrix_train,open(save_name,'wb'))\n",
    "#     matrix_val = ctvector.transform(train_cv.iloc[val_index_list[fold]]['text'])\n",
    "#     print(matrix_val.shape)\n",
    "#     name = 'val_fold_{}'.format(fold)\n",
    "#     save_name = FILE.tfidfVectorize_format.value.format(name)\n",
    "#     pickle.dump(matrix_val,open(save_name,'wb'))\n",
    "    \n",
    "# save_name = FILE.tfidfVectorize_format.value.format('holdout')\n",
    "# pickle.dump(holdout_matrix,open(save_name,'wb'))\n",
    "# print(holdout_matrix.shape)\n",
    "\n",
    "# save_name = FILE.tfidfVectorize_format.value.format('test')\n",
    "# pickle.dump(test_matrix,open(save_name,'wb'))\n",
    "# print(test_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(len(train_index_list)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T12:08:13.246600Z",
     "start_time": "2018-09-19T12:08:13.240016Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_nn_model(param):\n",
    "    layer1 = param.get('layer1')\n",
    "    layer2 = param.get('layer2')\n",
    "    layer3 = param.get('layer3')\n",
    "    input_shape = param.get('input_shape')\n",
    "    \n",
    "    sequence_input = Input(shape=(input_shape, ))\n",
    "    x = Dense(layer1, activation='relu')(sequence_input)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    if layer2 is not None:\n",
    "        x = Dense(layer2, activation='relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "    \n",
    "    if layer2 is not None and layer3 is not None:\n",
    "        x = Dense(layer3, activation='relu')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        \n",
    "    preds = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])    \n",
    "    return model\n",
    "\n",
    "def train_each_epoch(x,y,batch_size,model):\n",
    "    x,y = shuffle(x,y)\n",
    "    model.fit(x, y, \n",
    "              batch_size=batch_size, \n",
    "              epochs=1,\n",
    "              verbose=1,\n",
    "              )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T12:18:59.824418Z",
     "start_time": "2018-09-19T12:18:59.819079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "param_list = []\n",
    "for layer1 in [512,1024,2048]:\n",
    "    for layer2 in [None,32,512,1024]:\n",
    "        for layer3 in [None,16,32,128]:\n",
    "            if layer2 is None and layer3 is not None:\n",
    "                continue\n",
    "            param = {'input_shape':train_cv_matrix.shape[1],\n",
    "                     'layer1':layer1,\n",
    "                     'layer2':layer2,\n",
    "                     'layer3':layer3}\n",
    "            param_list.append(param)\n",
    "print(len(param_list))\n",
    "\n",
    "def get_file_name(param,vectorize='countV'):\n",
    "    layer1 = param.get('layer1')\n",
    "    layer2 = param.get('layer2')\n",
    "    layer3 = param.get('layer3')\n",
    "    return '{}_layer1_{}_layer2_{}_layer3_{}.pkl'.format(vectorize,layer1,layer2,layer3)\n",
    "\n",
    "def save_report(params,report_path):\n",
    "    \"\"\"\n",
    "    params. dataframe report params.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        saved_report = pd.read_csv(report_path)\n",
    "    except:\n",
    "        saved_report = pd.DataFrame()\n",
    "    new_report = pd.DataFrame({'fileName':params['file_Name'],\n",
    "                               'cv_mean':params['cv_mean'],\n",
    "                               'holdout':params['holdout']})\n",
    "    saved_report = pd.concat([saved_report,new_report]).reset_index(drop=True)\n",
    "    saved_report.to_csv(report_path)\n",
    "    print('saved report to path {}'.format(report_path))\n",
    "    \n",
    "def save_preds(param,preds_params,path='../../data/nn/{}/{}'):\n",
    "    \"\"\"\n",
    "    param: nn layer info.\n",
    "    preds_params:key should be train/test/holdout and values should be coresponding predict dataframe\n",
    "    \"\"\"\n",
    "    file_name = get_file_name(param)\n",
    "    for key in preds_params:\n",
    "        saving_path = path.format(key,file_name)\n",
    "        preds_params[key].to_pickle(saving_path)\n",
    "        print('saving preds {} done!'.format(key))\n",
    "        \n",
    "def train_each_fold(param,fold,mode='countVectorize'):\n",
    "    if mode != 'tfidf':\n",
    "        train_name = FILE.countVectorize_format.value.format('train_fold_{}'.format(fold))\n",
    "        val_name = FILE.countVectorize_format.value.format('val_fold_{}'.format(fold))\n",
    "        holdout_name = FILE.countVectorize_format.value.format('holdout')\n",
    "        test_name = FILE.countVectorize_format.value.format('test')\n",
    "        \n",
    "    else:\n",
    "        train_name = FILE.tfidfVectorize_format.value.format('train_fold_{}'.format(fold))\n",
    "        val_name = FILE.tfidfVectorize_format.value.format('val_fold_{}'.format(fold))\n",
    "        holdout_name = FILE.tfidfVectorize_format.value.format('holdout')\n",
    "        test_name = FILE.tfidfVectorize_format.value.format('test')\n",
    "        \n",
    "    y_train_name = FILE.Vectorize_label_format.value.format('train_fold_{}'.format(fold))\n",
    "    y_val_name = FILE.Vectorize_label_format.value.format('val_fold_{}'.format(fold))\n",
    "    y_holdout_name = FILE.Vectorize_label_format.value.format('holdout')\n",
    "        \n",
    "    x_train = pickle.load(open(train_name,'rb'))\n",
    "    x_val = pickle.load(open(val_name,'rb'))\n",
    "    x_holdout = pickle.load(open(holdout_name,'rb'))\n",
    "    x_test = pickle.load(open(test_name,'rb'))\n",
    "    y_train =  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One fold Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T05:31:04.146729Z",
     "start_time": "2018-09-19T05:31:04.069363Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_nn_model(train_cv_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T05:34:29.299570Z",
     "start_time": "2018-09-19T05:31:04.765864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "961626/961626 [==============================] - 203s 211us/step - loss: 0.4238 - acc: 0.8043\n"
     ]
    }
   ],
   "source": [
    "model = train_each_epoch(train_cv_matrix,train_cv.click.values,2048,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T05:34:36.520774Z",
     "start_time": "2018-09-19T05:34:30.265186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40024/40024 [==============================] - 6s 156us/step\n",
      "0.43059639321782484\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(holdout_matrix,batch_size=3000,verbose=1)\n",
    "score = log_loss(holdout.click.values,y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T05:38:03.562764Z",
     "start_time": "2018-09-19T05:34:44.757319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "961626/961626 [==============================] - 197s 205us/step - loss: 0.4160 - acc: 0.8067\n"
     ]
    }
   ],
   "source": [
    "model = train_each_epoch(train_cv_matrix,train_cv.click.values,2048,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T05:38:10.254851Z",
     "start_time": "2018-09-19T05:38:03.956790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40024/40024 [==============================] - 6s 157us/step\n",
      "0.4276796885171615\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(holdout_matrix,batch_size=6000,verbose=1)\n",
    "score = log_loss(holdout.click.values,y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T05:24:03.527596Z",
     "start_time": "2018-09-19T05:23:31.058164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "217088/961626 [=====>........................] - ETA: 1:45 - loss: 0.4105 - acc: 0.8074"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m?\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_each_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cv_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m?\u001b[0m in \u001b[0;36mtrain_each_epoch\u001b[0;34m(x, y, batch_size, model)\u001b[0m\n\u001b[1;32m     19\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m               )\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2670\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2672\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2652\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2654\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2655\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_each_epoch(train_cv_matrix,train_cv.click.values,2048,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T05:15:01.549248Z",
     "start_time": "2018-09-19T05:14:56.516367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40024/40024 [==============================] - 5s 125us/step\n",
      "0.4267815168378254\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(holdout_matrix,batch_size=6000,verbose=1)\n",
    "score = log_loss(holdout.click.values,y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T05:17:36.654894Z",
     "start_time": "2018-09-19T05:15:17.810106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "961626/961626 [==============================] - 137s 143us/step - loss: 0.4108 - acc: 0.8079\n"
     ]
    }
   ],
   "source": [
    "model = train_each_epoch(train_cv_matrix,train_cv.click.values,2048,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T05:17:45.797193Z",
     "start_time": "2018-09-19T05:17:40.626811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40024/40024 [==============================] - 5s 128us/step\n",
      "0.42833751105049117\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(holdout_matrix,batch_size=6000,verbose=1)\n",
    "score = log_loss(holdout.click.values,y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
