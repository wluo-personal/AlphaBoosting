{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T05:17:01.171302Z",
     "start_time": "2018-09-27T05:17:00.195157Z"
    }
   },
   "outputs": [],
   "source": [
    "__file__=''\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../LIB/'))\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../../../../automl/automl_libs/'))\n",
    "from env import FILE\n",
    "import utils\n",
    "from itertools import combinations\n",
    "from feature_engineering import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "from sklearn.metrics import log_loss\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T05:17:01.639459Z",
     "start_time": "2018-09-27T05:17:01.175081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: (1001650, 72)\n",
      "test shape is: (40024, 72)\n"
     ]
    }
   ],
   "source": [
    "# train = pd.read_pickle(FILE.train_lgb_base.value)\n",
    "train = pd.read_pickle('../../data/lgb_feature/train_lgb_agg2.pickle')\n",
    "print('train shape is: {}'.format(train.shape))\n",
    "# test = pd.read_pickle(FILE.test_lgb_base.value)\n",
    "test = pd.read_pickle('../../data/lgb_feature/test_lgb_agg2.pickle')\n",
    "print('test shape is: {}'.format(test.shape))\n",
    "test['click'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T05:17:07.541364Z",
     "start_time": "2018-09-27T05:17:01.641147Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m?\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mroun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mappend_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_fe_agg_count_formater\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappend_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mappend_timeNext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_fe_agg_time_next_formater\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappend_timeNext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreset_index\u001b[0;34m(self, level, drop, inplace, col_level, col_fill)\u001b[0m\n\u001b[1;32m   4068\u001b[0m             \u001b[0mnew_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4069\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4070\u001b[0;31m             \u001b[0mnew_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4072\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_casted_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m   5108\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5109\u001b[0m         \"\"\"\n\u001b[0;32m-> 5110\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep, mgr)\u001b[0m\n\u001b[1;32m   3918\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3919\u001b[0m         return self.apply('copy', axes=new_axes, deep=deep,\n\u001b[0;32m-> 3920\u001b[0;31m                           do_integrity_check=False)\n\u001b[0m\u001b[1;32m   3921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3922\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mas_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[1;32m   3579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3580\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mgr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3581\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3582\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep, mgr)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "roun=2\n",
    "append_count = pd.read_pickle(FILE.X_fe_agg_count_formater.value.format(roun)).reset_index(drop=True)\n",
    "print(append_count.shape)\n",
    "append_timeNext = pd.read_pickle(FILE.X_fe_agg_time_next_formater.value.format(roun)).reset_index(drop=True)\n",
    "print(append_timeNext.shape)\n",
    "append_timeCount = pd.read_pickle(FILE.X_fe_agg_time_count_formater.value.format(roun)).reset_index(drop=True)\n",
    "print(append_timeCount.shape)\n",
    "\n",
    "# X_fe = append_count.merge(append_timeNext,on='instance_id',how='inner')\n",
    "# X_fe = X_fe.merge(append_timeCount,on='instance_id',how='inner')\n",
    "\n",
    "# X_fe = append_count.copy()\n",
    "# assert np.sum(X_fe['instance_id'].values != append_timeNext['instance_id'].values) == 0\n",
    "# assert np.sum(X_fe['instance_id'].values != append_timeCount['instance_id'].values) == 0\n",
    "# for col in append_timeNext.columns:\n",
    "#     X_fe[col] = append_timeNext[col].values\n",
    "# for col in append_timeCount.columns:\n",
    "#     X_fe[col] = append_timeCount[col].values\n",
    "\n",
    "# X_fe_train_libfm = pd.read_pickle(FILE.X_fe_train_libfm.value)\n",
    "# X_fe_test_libfm = pd.read_pickle(FILE.X_fe_test_libfm.value)\n",
    "# X_fe = pd.concat([X_fe_train_libfm,X_fe_test_libfm])\n",
    "# print(X_fe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T05:17:07.542221Z",
     "start_time": "2018-09-27T05:17:00.161Z"
    }
   },
   "outputs": [],
   "source": [
    "init_feature = list(test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T05:17:07.542970Z",
     "start_time": "2018-09-27T05:17:00.163Z"
    }
   },
   "outputs": [],
   "source": [
    "for X_fe in [append_count,append_timeCount,append_timeNext]:\n",
    "# for X_fe in [append_timeNext]:\n",
    "    train_order = train[['instance_id']].copy()\n",
    "    train_order['ori_order'] = train_order.index\n",
    "    test_order = test[['instance_id']].copy()\n",
    "    test_order['ori_order'] = test_order.index\n",
    "\n",
    "    X_fe_order = X_fe[['instance_id']].copy()\n",
    "    X_fe_order['fe_order'] = X_fe_order.index\n",
    "    X_fe_train = X_fe_order.merge(train_order,on='instance_id',how='inner')\n",
    "    X_fe_train = X_fe_train.sort_values('ori_order',ascending=True)\n",
    "    X_fe_test = X_fe_order.merge(test_order,on='instance_id',how='inner')\n",
    "    X_fe_test = X_fe_test.sort_values('ori_order',ascending=True)\n",
    "    index_order_train = X_fe_train.fe_order.values\n",
    "    index_order_test = X_fe_test.fe_order.values\n",
    "\n",
    "    X_fe_train = X_fe.iloc[index_order_train].copy()\n",
    "    print(X_fe_train.shape)\n",
    "\n",
    "    X_fe_test = X_fe.iloc[index_order_test].copy()\n",
    "    print(X_fe_test.shape)\n",
    "\n",
    "    assert np.sum(X_fe_train.instance_id.values != train.instance_id.values) == 0\n",
    "    assert np.sum(X_fe_test.instance_id.values != test.instance_id.values) == 0\n",
    "\n",
    "    for col in X_fe_train.columns:\n",
    "        if col == 'instance_id':\n",
    "            continue\n",
    "        else:\n",
    "            train[col] = X_fe_train[col].values\n",
    "            test[col] = X_fe_test[col].values\n",
    "    print(train.shape)\n",
    "    print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T05:17:07.543608Z",
     "start_time": "2018-09-27T05:17:00.166Z"
    }
   },
   "outputs": [],
   "source": [
    "search_feature = list(set(test.columns) - set(init_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T05:17:07.544423Z",
     "start_time": "2018-09-27T05:17:00.170Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print_to_file = False \n",
    "test_run = False \n",
    "\n",
    "def get_time(timezone='America/New_York', time_format='%Y-%m-%d %H:%M:%S'):\n",
    "    from datetime import datetime\n",
    "    from dateutil import tz\n",
    "\n",
    "    # METHOD 1: Hardcode zones:\n",
    "    from_zone = tz.gettz('UTC')\n",
    "    to_zone = tz.gettz(timezone)\n",
    "\n",
    "    utc = datetime.utcnow()\n",
    "\n",
    "    # Tell the datetime object that it's in UTC time zone since \n",
    "    # datetime objects are 'naive' by default\n",
    "    utc = utc.replace(tzinfo=from_zone)\n",
    "\n",
    "    # Convert time zone\n",
    "    est = utc.astimezone(to_zone)\n",
    "\n",
    "    return est.strftime(time_format)\n",
    "\n",
    "import sys, time\n",
    "class Logger(object):\n",
    "    def __init__(self, logtofile=True, logfilename='log'):\n",
    "        self.terminal = sys.stdout\n",
    "        self.logfile = \"{}_{}.log\".format(logfilename, int(time.time()))\n",
    "        self.logtofile = logtofile\n",
    "\n",
    "    def write(self, message):\n",
    "        #         self.terminal.write(message)\n",
    "        if self.logtofile:\n",
    "            self.log = open(self.logfile, \"a\")\n",
    "            self.log.write('[' + get_time() + '] ' + message)\n",
    "            self.log.close()\n",
    "\n",
    "    def flush(self):\n",
    "        # this flush method is needed for python 3 compatibility.\n",
    "        # this handles the flush command by doing nothing.\n",
    "        # you might want to specify some extra behavior here.\n",
    "        pass\n",
    "\n",
    "\n",
    "def divert_printout_to_file():\n",
    "    sys.stdout = Logger(logfilename='logfile')\n",
    "\n",
    "if print_to_file:\n",
    "    divert_printout_to_file()  # note: comment this to use pdb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "def kfold_lightgbm(df, train_df, test_df, holdout, num_folds, submission_file_name, fe_img_name, stratified = False, debug= False, colsample=0.67, max_depth=8, num_leaves=31, min_child_samples=20, subsample=0.7, reg_lambda=0.3, lr=0.04, seed=1001, verbose=100, rounds=None, target='click'):\n",
    "    print(train_df.shape, test_df.shape, holdout.shape)\n",
    "    print('MEAN: train({}) vs holdout({}): '.format(len(train_df), len(holdout)), train_df[target].mean(), holdout[target].mean())\n",
    "    # Divide in training/validation and test data\n",
    "    if df is not None:\n",
    "        train_df = df[df[target].notnull()]\n",
    "        test_df = df[df[target].isnull()]\n",
    "        print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "        del df\n",
    "        gc.collect()\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=seed)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=seed)\n",
    "        \n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    holdout_final_preds = np.zeros(holdout.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feature_importance_gain_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in [target,'time','instance_id','index']]\n",
    "    train_scores = []\n",
    "    holdout_scores = []\n",
    "    scores = []\n",
    "    diff_val_holdout = []\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df[target])):\n",
    "#         print('valid index : ',list(valid_idx)[:5])\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df[target].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df[target].iloc[valid_idx]\n",
    "#         print('MEAN: train({}) vs valid({}): '.format(len(train_y), len(valid_y)), np.mean(train_y), np.mean(valid_y))\n",
    "        clf = LGBMClassifier(\n",
    "            nthread=18,\n",
    "            n_estimators=30000,\n",
    "            learning_rate=lr,\n",
    "#             max_bin =10000,\n",
    "#             num_leaves=num_leaves,\n",
    "#             colsample_bytree=colsample, # 0.67\n",
    "#             subsample=subsample,\n",
    "#             subsample_freq=0, ## disable subsampling\n",
    "#             max_depth=max_depth,\n",
    "#             reg_alpha=0.65,\n",
    "#             reg_lambda=reg_lambda,\n",
    "#             min_split_gain=0.0222415,\n",
    "#             min_child_weight=39.3259775,\n",
    "#             min_child_samples=min_child_samples,\n",
    "            silent=-1,\n",
    "            verbose=-1, )\n",
    "        if rounds is not None:\n",
    "            clf.n_estimators = rounds\n",
    "            clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "                eval_metric= 'logloss', verbose=verbose)\n",
    "            oof_preds[valid_idx] = clf.predict_proba(valid_x)[:, 1]\n",
    "            sub_preds += clf.predict_proba(test_df[feats])[:, 1] / folds.n_splits\n",
    "            holdout_preds = clf.predict_proba(holdout[feats])[:, 1] \n",
    "        else:\n",
    "            clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "                eval_metric= 'logloss', verbose=verbose, early_stopping_rounds= 200)\n",
    "            oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "            sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "            holdout_preds = clf.predict_proba(holdout[feats], num_iteration=clf.best_iteration_)[:, 1] \n",
    "            \n",
    "        holdout_final_preds += holdout_preds / folds.n_splits\n",
    "        score = log_loss(valid_y, oof_preds[valid_idx])\n",
    "        train_score = clf.best_score_['training']['binary_logloss']\n",
    "        holdout_score = log_loss(holdout[target], holdout_preds)\n",
    "        diff = abs(score - holdout_score)\n",
    "        best_rounds = rounds if rounds is not None else clf.best_iteration_\n",
    "        print('Fold %2d [%5d] AUC : ho: %.6f / te: %.6f / tr: %.6f (diff: %.6f)' % (n_fold + 1, best_rounds, holdout_score, score,  train_score, diff))\n",
    "        scores.append(score)\n",
    "        train_scores.append(train_score)\n",
    "        holdout_scores.append(holdout_score)\n",
    "        diff_val_holdout.append(diff)\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        \n",
    "        fold_importance_gain_df = pd.DataFrame()\n",
    "        fold_importance_gain_df[\"feature\"] = feats\n",
    "        fold_importance_gain_df[\"importance\"] = clf.booster_.feature_importance(importance_type='gain')\n",
    "        fold_importance_gain_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_gain_df = pd.concat([feature_importance_gain_df, fold_importance_gain_df], axis=0)\n",
    "        \n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "    holdout_roc = log_loss(holdout[target], holdout_final_preds)\n",
    "    holdout_mean = np.mean(holdout_scores)\n",
    "    full_te_mean = np.mean(scores)\n",
    "    full_tr_mean = np.mean(train_scores)\n",
    "#     print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "    print('Full HO score %.6f' % holdout_roc)\n",
    "    print('FULL HO mean {:.6f}, std {:.6f}'.format(holdout_mean, np.std(holdout_scores)))\n",
    "    print('FULL TE mean {:.6f}, std {:.6f}'.format(full_te_mean, np.std(scores)))\n",
    "    print('FULL TR mean {:.6f}, std {:.6f}'.format(full_tr_mean, np.std(train_scores)))\n",
    "    print('FULL DIFF mean {:.6f}, std {:.6f}'.format(np.mean(diff_val_holdout), np.std(diff_val_holdout)))\n",
    "    # Write submission file and plot feature importance\n",
    "    if not debug:\n",
    "        print('saving...')\n",
    "        test_df['predicted_score'] = sub_preds\n",
    "        test_df[['instance_id', 'predicted_score']].to_csv(submission_file_name, index= False)\n",
    "#     if not print_to_file:\n",
    "#         display_importances(feature_importance_df, fe_img_name)\n",
    "    feature_importance_df = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "    feature_importance_gain_df = feature_importance_gain_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "    return feature_importance_df, feature_importance_gain_df,holdout_roc,holdout_mean,full_te_mean,full_tr_mean,oof_preds,holdout_final_preds\n",
    "\n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_, fe_img_name):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fe_img_name+'.png')\n",
    "\n",
    "\n",
    "def convert_and_save_imp_df(fe_imp_df, dumpfilename):\n",
    "    fe_imp_df_mean = fe_imp_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "    pickle.dump(fe_imp_df_mean, open(dumpfilename,'wb'))\n",
    "    \n",
    "    \n",
    "\n",
    "def select_holdout(train,test,shift=1,seed=41):\n",
    "    samples_size = int(len(test) )\n",
    "    assert shift in range(1,8)\n",
    "    test_time_min = test.time.min()\n",
    "    test_time_max = test.time.max()\n",
    "    train_time_min = train.time.min()\n",
    "    train_time_max = train.time.max()\n",
    "    window_lower_bound = test_time_min - shift*60*60*24\n",
    "    window_upper_bound = test_time_max - shift*60*60*24\n",
    "    available_train = train[(train.time<=window_upper_bound) & (train.time>=window_lower_bound)]\n",
    "    holdout = available_train.sample(n=samples_size,random_state=seed).copy()\n",
    "    train_split_index = list(set(train.index) - set(holdout.index))\n",
    "    train_split = train.iloc[train_split_index].copy()\n",
    "    return train_split,holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T05:17:07.545066Z",
     "start_time": "2018-09-27T05:17:00.172Z"
    }
   },
   "outputs": [],
   "source": [
    "def runlgb(train, test, holdout):\n",
    "    colsamples = [0.07]#[0.1,0.15,0.2]#[0.03,0.04,0.05,0.06,0.07,0.08]\n",
    "    seeds = [1001]#[300,4000,50000,600000,7000000,80000000,523445,31275479] # 20\n",
    "    depth = [5]\n",
    "    leaves = [16]\n",
    "    min_child_sam = [20]#, 800]\n",
    "    subsamples = [1]#0.8, 0.7, 0.6, 0.5, 0.4] # was 1\n",
    "    reg_lambdas = [0.5]\n",
    "    # lrs = lrs.tolist()\n",
    "    lrs2 = [0.1]\n",
    "    nfolds = 7\n",
    "    rounds = [None] #[1000]#, 1300, 1600, 1900, 2200, 2500]\n",
    "    for seed in seeds:\n",
    "        for colsample in colsamples:\n",
    "            for d in depth:\n",
    "                for l in leaves:\n",
    "                    for mcs in min_child_sam:\n",
    "                        for subsample in subsamples:\n",
    "                            for reg_lambda in reg_lambdas:\n",
    "                                for lr in lrs2:\n",
    "                                    for r in rounds:\n",
    "                                        filename = 'fe_936_col{}_lr{}_n{}'.format(len(train.columns), lr, nfolds)\n",
    "                                        print('#############################################')\n",
    "                                        print(colsample, seed, d, l, mcs, subsample, reg_lambda, lr, 'nfolds:', nfolds)\n",
    "                                        print('#############################################')\n",
    "                                        numfeats = len(train.columns)\n",
    "                                        with timer(\"Run LightGBM with kfold\"):\n",
    "                                            return kfold_lightgbm(None, train, test, holdout, nfolds, filename+'.csv', filename, colsample=colsample, verbose=None, max_depth=d, num_leaves=l, min_child_samples=mcs, subsample=subsample, reg_lambda=reg_lambda, lr=lr, seed=seed, stratified=True, rounds=r,debug=True)                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T05:17:07.545767Z",
     "start_time": "2018-09-27T05:17:00.176Z"
    }
   },
   "outputs": [],
   "source": [
    "# split_all, gain_all,holdout_roc,holdout_mean,full_te_mean,full_tr_mean,oof_preds,holdout_preds = runlgb(train_df, test, holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T05:17:07.546647Z",
     "start_time": "2018-09-27T05:17:00.178Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_each_col(train_df,holdout,test,search_feature,target,init_feature,report_path='report_lbfm.csv',by='all'):\n",
    "    \"\"\"\n",
    "    train_df, the train set which does not has holdout\n",
    "    by = 'cv','ho', other == cv+ ho\n",
    "    \"\"\"\n",
    "    #1 run base line\n",
    "    \n",
    "    try:\n",
    "        report = pd.read_csv(report_path)\n",
    "        print('found report! resume from last time!')\n",
    "    except:\n",
    "        print('calculate importance to generate report...')\n",
    "        split_all, gain_all,holdout_roc,holdout_mean,full_te_mean,full_tr_mean,oof_preds,holdout_preds = runlgb(train_df, test, holdout)\n",
    "        fea = [f for f in split_all.feature.values if f in search_feature]\n",
    "        report = pd.DataFrame({'feature':fea})\n",
    "        report['order'] = report.index\n",
    "        report['holdout'] = np.nan\n",
    "        report['holdout_mean'] = np.nan\n",
    "        report['search'] = 'n'\n",
    "        report['add'] = 'n'\n",
    "        report['cv'] = np.nan\n",
    "    added = report[report['add']=='y'].copy()\n",
    "    added_col = []\n",
    "    if len(added) == 0:\n",
    "        print('calculate base line...')\n",
    "        _, _,holdout_base,_,cv_base,_,_,_ = runlgb(train_df[init_feature], \n",
    "                                                   test[init_feature], \n",
    "                                                   holdout[init_feature])\n",
    "    else:\n",
    "        added = added.sort_values('order')\n",
    "        holdout_base = added.iloc[-1]['holdout']\n",
    "        cv_base = added.iloc[-1]['cv']\n",
    "        added_col = list(added['feature'].values)\n",
    "    print('---- holdout base is {}. cv base is {}'.format(holdout_base,cv_base))\n",
    "    print('added features are: {}'.format(added_col))\n",
    "    best_col = init_feature + added_col\n",
    "\n",
    "    searched = report[report['search']=='y'].copy()\n",
    "    start_index = len(searched)\n",
    "\n",
    "\n",
    "    for i in range(start_index,len(report)):\n",
    "        col = report.iloc[i]['feature']\n",
    "        if col in best_col:\n",
    "            continue\n",
    "        print('start... {}/{} fe:{}'.format(i,len(report),col))\n",
    "        _, _,ho_cur,ho_mean_cur,cv_cur,_,_,_ = runlgb(train_df[best_col+[col]], \n",
    "                                                   test[best_col+[col]], \n",
    "                                                   holdout[best_col+[col]])\n",
    "        report.loc[i,'holdout'] = ho_cur\n",
    "        report.loc[i,'holdout_mean'] = ho_mean_cur\n",
    "        report.loc[i,'search'] = 'y'\n",
    "        report.loc[i,'cv'] = cv_cur\n",
    "        if by == 'cv' and cv_cur < cv_base:\n",
    "            cv_base = cv_cur\n",
    "            report.loc[i,'add'] = 'y'\n",
    "            print('#### {}/{} YES ADD col: {}'.format(i,len(report),col))\n",
    "            best_col = best_col + [col]\n",
    "        elif by == 'ho' and ho_cur < holdout_base:\n",
    "            holdout_base = ho_cur\n",
    "            report.loc[i,'add'] = 'y'\n",
    "            print('#### {}/{} YES ADD col: {}'.format(i,len(report),col))\n",
    "            best_col = best_col + [col]\n",
    "        elif cv_cur < cv_base and ho_cur < holdout_base:\n",
    "            cv_base = cv_cur\n",
    "            holdout_base = ho_cur\n",
    "            report.loc[i,'add'] = 'y'\n",
    "            print('#### {}/{} YES ADD col: {}'.format(i,len(report),col))\n",
    "            best_col = best_col + [col]\n",
    "        else:\n",
    "            report.loc[i,'add'] = 'n'\n",
    "            print('#### {}/{} NO ADD col: {}'.format(i,len(report),col))\n",
    "        report.to_csv(report_path,index=False)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T05:17:07.547409Z",
     "start_time": "2018-09-27T05:17:00.202Z"
    }
   },
   "outputs": [],
   "source": [
    "report_path = 'report_agg2.csv'\n",
    "target='click'\n",
    "train_index = pickle.load(open(FILE.train_index.value,'rb'))\n",
    "holdout_index = pickle.load(open(FILE.holdout_index.value,'rb'))\n",
    "train_df = train.loc[train_index].copy()\n",
    "holdout = train.loc[holdout_index].copy()\n",
    "print('train shape: {}, holdout shape: {}'.format(train_df.shape,holdout.shape))\n",
    "print('train mean: {}, holdout mean: {}'.format(train_df[target].mean(),holdout[target].mean()))\n",
    "\n",
    "add_each_col(train_df,holdout,test,search_feature,target,init_feature,report_path=report_path,by='cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T05:17:07.548173Z",
     "start_time": "2018-09-27T05:17:00.205Z"
    }
   },
   "outputs": [],
   "source": [
    "report = pd.read_csv(report_path)\n",
    "added = list(report[report['add']=='y'].feature.values)\n",
    "saved_col = init_feature + added\n",
    "train_save = train[saved_col].copy()\n",
    "print(train_save.shape)\n",
    "test_save = test[saved_col].copy()\n",
    "print(train_save.shape)\n",
    "train_save.to_pickle('../../data/lgb_feature/train_lgb_agg2.pickle')\n",
    "test_save.to_pickle('../../data/lgb_feature/test_lgb_agg2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T05:17:07.548847Z",
     "start_time": "2018-09-27T05:17:00.207Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['count_ad_i_i_lvl2', 'count_devtype', 'inner_slot_id_cumulative_count',\n",
    "       'has_tag_3003123', 'count_creative_res', 'count_creative_id',\n",
    "       'count_is_shenghuicity', 'count_app_cate_id', 'advert_id',\n",
    "       'count_creative_is_js', 'count_campaign_id', 'osv', 'orderid',\n",
    "       'has_tag_2100127', 'count_vertical_screen', 'adid', 'count_os',\n",
    "       'count_has_usertags', 'count_creative_area', 'count_ad_i_i_lvl1',\n",
    "       'count_orderid', 'f_channel', 'count_adid',\n",
    "       'count_creative_is_download', 'time_day', 'has_tag_3004262',\n",
    "       'has_tag_3004294', 'has_tag_3004506', 'has_tag_3004406',\n",
    "       'count_carrier', 'time', 'count_osv', 'user_tags', 'province',\n",
    "       'count_creative_tp_dnf', 'count_creative_type', 'creative_id',\n",
    "       'count_advert_name', 'has_tag_3003145', 'count_province', 'count_nnt',\n",
    "       'creative_tp_dnf+f_channel_cumulative_count',\n",
    "       'count_advert_industry_inner', 'count_model', 'campaign_id',\n",
    "       'count_app_id', 'user_id', 'count_creative_is_voicead',\n",
    "       'count_creative_has_deeplink', 'count_creative_res_popularity',\n",
    "       'count_advert_id', 'instance_id', 'amt_usertags', 'creative_width',\n",
    "       'creative_tp_dnf', 'make', 'count_app_paid', 'has_tag_3004105',\n",
    "       'has_tag_3003939', 'count_os_name', 'count_amt_usertags',\n",
    "       'count_creative_is_jump', 'app_id',\n",
    "       'orderid_count_in_next_n_time_unit_6hour', 'count_user_tags',\n",
    "       'count_f_channel', 'city', 'inner_slot_id', 'model', 'count_city',\n",
    "       'count_make', 'click', 'count_inner_slot_id',\n",
    "       'creative_tp_dnf+f_channel_cumulative_count', 'nnt+osv_time_to_n_next']\n",
    "finds = []\n",
    "for col in cols:\n",
    "    if col in finds:\n",
    "        print(col)\n",
    "    finds.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T05:17:07.549735Z",
     "start_time": "2018-09-27T05:17:00.213Z"
    }
   },
   "outputs": [],
   "source": [
    "# train.drop(['creative_tp_dnf+f_channel_cumulative_count'],axis=1,inplace=True)\n",
    "# test.drop(['creative_tp_dnf+f_channel_cumulative_count'],axis=1,inplace=True)\n",
    "\n",
    "# train.to_pickle('../../data/lgb_feature/train_lgb_agg2.pickle')\n",
    "# test.to_pickle('../../data/lgb_feature/test_lgb_agg2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
