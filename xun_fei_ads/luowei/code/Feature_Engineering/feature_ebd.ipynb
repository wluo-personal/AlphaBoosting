{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T15:53:44.445302Z",
     "start_time": "2018-09-25T15:53:42.262793Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "__file__=''\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../LIB/'))\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../../../../automl/automl_libs/'))\n",
    "from env import FILE\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csc_matrix, csr_matrix, hstack\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU,CuDNNGRU,Flatten,BatchNormalization\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import log_loss,roc_auc_score\n",
    "from keras.regularizers import l1\n",
    "from keras.regularizers import l2\n",
    "from keras.regularizers import l1_l2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T15:53:50.228944Z",
     "start_time": "2018-09-25T15:53:44.448586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: (1001650, 35)\n",
      "test shape is: (40024, 34)\n",
      "(1041674, 35)\n",
      "(1041674, 45)\n",
      "(1041674, 36)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_pickle(FILE.train_ori.value)\n",
    "print('train shape is: {}'.format(train.shape))\n",
    "test = pd.read_pickle(FILE.test_ori.value)\n",
    "print('test shape is: {}'.format(test.shape))\n",
    "\n",
    "# X = pd.concat([train.drop(['click'],axis=1),test])\n",
    "X = pd.concat([train,test],sort=False)\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "X_shiyi = pd.read_pickle(FILE.shiyi_fillna_ori.value)\n",
    "print(X_shiyi.shape)\n",
    "\n",
    "X = X.merge(X_shiyi[['time_hour','instance_id']],how='inner',on='instance_id')\n",
    "print(X.shape)\n",
    "ignore_columns = ['instance_id','time','click'] + ['creative_is_js', 'creative_is_voicead', 'app_paid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T15:53:50.238865Z",
     "start_time": "2018-09-25T15:53:50.234875Z"
    }
   },
   "outputs": [],
   "source": [
    "# need_process_col = list(set(X.columns) - set(ignore_columns))\n",
    "# X_ = X[need_process_col].copy()\n",
    "# counter = 0\n",
    "# for col in tqdm(X_.columns):\n",
    "# #     X_[col] = le.fit_transform(X_[col].astype(str))\n",
    "#     X_[col] = col + '_'+X_[col].astype(str)\n",
    "#     nunique = X_[col].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T15:55:33.434667Z",
     "start_time": "2018-09-25T15:53:50.247605Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:41<00:00,  3.39s/it]\n"
     ]
    }
   ],
   "source": [
    "need_process_col = list(set(X.columns) - set(ignore_columns))\n",
    "X_ = X[need_process_col].copy()\n",
    "counter = 0\n",
    "for col in tqdm(X_.columns):\n",
    "    X_[col] = le.fit_transform(X_[col].astype(str))\n",
    "    X_[col] = col + '_'+X_[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T15:55:33.443674Z",
     "start_time": "2018-09-25T15:55:33.436928Z"
    }
   },
   "outputs": [],
   "source": [
    "train_index = pickle.load(open(FILE.train_index.value,'rb'))\n",
    "holdout_index = pickle.load(open(FILE.holdout_index.value,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T16:01:28.947118Z",
     "start_time": "2018-09-25T15:55:33.445824Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [05:45<00:00, 11.51s/it]\n"
     ]
    }
   ],
   "source": [
    "val_length = 150000\n",
    "train_length = 1001650\n",
    "\n",
    "train_split, val_split, y_train, y_val = train_test_split(X_.iloc[:train_length],train.click.values,test_size = 0.1, shuffle=True)\n",
    "\n",
    "maxlen = 1\n",
    "\n",
    "# y_train = train['click'].values[:-val_length]\n",
    "# y_val = train['click'].values[-val_length:]\n",
    "cols = need_process_col\n",
    "tok_dict = {}\n",
    "input_train_dict = {}\n",
    "input_val_dict = {}\n",
    "input_test_dict = {}\n",
    "for col in tqdm(cols):\n",
    "    tok=text.Tokenizer(num_words=X_[col].nunique(),lower=False,filters='!')\n",
    "    tok.fit_on_texts(list(X_[col]))\n",
    "    tok_dict[col] = tok\n",
    "    ###################################################################################################\n",
    "#     X_train=tok.texts_to_sequences(list(X_[col].iloc[:train_length-val_length]))\n",
    "#     X_val=tok.texts_to_sequences(list(X_[col].iloc[train_length-val_length:train_length]))\n",
    "    ###################################################################################################\n",
    "    \n",
    "    X_train=tok.texts_to_sequences(list(train_split[col].values))\n",
    "    X_val=tok.texts_to_sequences(list(val_split[col].values))\n",
    "    X_test=tok.texts_to_sequences(list(X_[col].values[train_length:]))\n",
    "    ###################################################################################################\n",
    "    x_train=sequence.pad_sequences(X_train,maxlen=maxlen)\n",
    "    x_val=sequence.pad_sequences(X_val,maxlen=maxlen)\n",
    "    x_test=sequence.pad_sequences(X_test,maxlen=maxlen)\n",
    "    input_train_dict['input_'+col] = x_train\n",
    "    input_val_dict['input_'+col] = x_val\n",
    "    input_test_dict['input_'+col] = x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build NN model only use model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T16:01:28.954075Z",
     "start_time": "2018-09-25T16:01:28.949132Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_nn_model(cols):\n",
    "    input_list = []\n",
    "    flatten_list = []\n",
    "    for col in cols:\n",
    "        max_feature = X_[col].nunique()\n",
    "        cur_input = Input(shape=(1, ),name = 'input_'+col)\n",
    "        embed_size = int(np.log2(max_feature)/np.log2(1.5))\n",
    "        if embed_size< 3:\n",
    "            embed_size = 3\n",
    "        embed_layer = Embedding(max_feature,\n",
    "                            embed_size,\n",
    "                            input_length=1,\n",
    "                            trainable=True,\n",
    "#                                 0.0004\n",
    "                            embeddings_regularizer=l2(0.0004),\n",
    "                            name='ebd_'+col)(cur_input)\n",
    "        embed_layer = SpatialDropout1D(0.5)(embed_layer)\n",
    "        x = Flatten()(embed_layer)\n",
    "        input_list.append(cur_input)\n",
    "        flatten_list.append(x)\n",
    "    x = concatenate(flatten_list)\n",
    "    \n",
    "    x = Dense(5124, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "#     x = Dense(128, activation='relu')(x)\n",
    "#     x = Dropout(0.2)(x)\n",
    "\n",
    "    preds = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(input_list, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T16:03:46.300037Z",
     "start_time": "2018-09-25T16:01:28.955945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "901485/901485 [==============================] - 15s 16us/step - loss: 0.8883 - acc: 0.8015\n",
      "100165/100165 [==============================] - 1s 10us/step\n",
      "0.4203874662395839\n",
      "0.7635568981550331\n",
      "best logloss is: 0.4203874662395839\n",
      "Epoch 1/1\n",
      "901485/901485 [==============================] - 11s 12us/step - loss: 0.4263 - acc: 0.8052\n",
      "100165/100165 [==============================] - 0s 5us/step\n",
      "0.41895112837801113\n",
      "0.7636666845981331\n",
      "best logloss is: 0.41895112837801113\n",
      "Epoch 1/1\n",
      "901485/901485 [==============================] - 11s 12us/step - loss: 0.4262 - acc: 0.8054\n",
      "100165/100165 [==============================] - 1s 5us/step\n",
      "0.4183461695158708\n",
      "0.7649116464437378\n",
      "best logloss is: 0.4183461695158708\n",
      "Epoch 1/1\n",
      "901485/901485 [==============================] - 12s 14us/step - loss: 0.4272 - acc: 0.8057\n",
      "100165/100165 [==============================] - 1s 6us/step\n",
      "0.41848632453354273\n",
      "0.7648336642229738\n",
      "best logloss is: 0.4183461695158708\n",
      "Epoch 1/1\n",
      "901485/901485 [==============================] - 13s 14us/step - loss: 0.4274 - acc: 0.8057\n",
      "100165/100165 [==============================] - 0s 5us/step\n",
      "0.4182409781702095\n",
      "0.7655862537961091\n",
      "best logloss is: 0.4182409781702095\n",
      "Epoch 1/1\n",
      "901485/901485 [==============================] - 13s 14us/step - loss: 0.4288 - acc: 0.8058\n",
      "100165/100165 [==============================] - 1s 6us/step\n",
      "0.4184931413222091\n",
      "0.7654615250537429\n",
      "best logloss is: 0.4182409781702095\n",
      "Epoch 1/1\n",
      "901485/901485 [==============================] - 15s 16us/step - loss: 0.4319 - acc: 0.8059\n",
      "100165/100165 [==============================] - 0s 4us/step\n",
      "0.41781966656599917\n",
      "0.7663708741271593\n",
      "best logloss is: 0.41781966656599917\n",
      "Epoch 1/1\n",
      "901485/901485 [==============================] - 13s 15us/step - loss: 0.4366 - acc: 0.8059\n",
      "100165/100165 [==============================] - 1s 6us/step\n",
      "0.417875502121062\n",
      "0.7658542322805961\n",
      "best logloss is: 0.41781966656599917\n",
      "Epoch 1/1\n",
      "901485/901485 [==============================] - 12s 13us/step - loss: 0.4471 - acc: 0.8118\n",
      "100165/100165 [==============================] - 0s 4us/step\n",
      "0.4203812382298121\n",
      "0.7631177592473477\n",
      "best logloss is: 0.41781966656599917\n",
      "Epoch 1/1\n",
      "901485/901485 [==============================] - 11s 13us/step - loss: 0.4315 - acc: 0.8381\n",
      "100165/100165 [==============================] - 0s 5us/step\n",
      "0.4315372867053648\n",
      "0.7571623404507732\n"
     ]
    }
   ],
   "source": [
    "model = get_nn_model(cols)\n",
    "tolerance = 2\n",
    "cur_to = 0\n",
    "best_logloss = None\n",
    "best_weights = None\n",
    "while True:\n",
    "    model.fit(input_train_dict, y_train, \n",
    "              batch_size=5000, \n",
    "              epochs=1,\n",
    "              verbose=1,\n",
    "              shuffle=True,\n",
    "              )\n",
    "    preds = model.predict(input_val_dict,5000,verbose=1)\n",
    "    logloss = log_loss(y_val,preds)\n",
    "    roc = roc_auc_score(y_val,preds)\n",
    "    print(logloss)\n",
    "    print(roc)\n",
    "    if best_logloss is None:\n",
    "        best_logloss = logloss\n",
    "        best_weights = model.get_weights()\n",
    "    else:\n",
    "        if best_logloss > logloss:\n",
    "            best_logloss = logloss\n",
    "            best_weights = model.get_weights()\n",
    "            cur_to = 0\n",
    "        else:\n",
    "            cur_to +=1\n",
    "    if cur_to == 3:\n",
    "        break\n",
    "    print('best logloss is: {}'.format(best_logloss))\n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T16:03:46.529741Z",
     "start_time": "2018-09-25T16:03:46.302347Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aaa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m?\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maaa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'aaa' is not defined"
     ]
    }
   ],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T16:03:46.530632Z",
     "start_time": "2018-09-25T15:53:42.307Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = model.predict(input_test_dict,5000,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T16:03:46.531724Z",
     "start_time": "2018-09-25T15:53:42.315Z"
    }
   },
   "outputs": [],
   "source": [
    "save = test[['instance_id']].copy()\n",
    "save['predicted_score'] = preds\n",
    "save.to_csv('submitnn_ebd_ini.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T16:03:46.532883Z",
     "start_time": "2018-09-25T15:53:42.319Z"
    }
   },
   "outputs": [],
   "source": [
    "col = 'model'\n",
    "ebd_mat = model.get_layer('ebd_{}'.format(col)).get_weights()[0]\n",
    "print('ebd_mat shape is: {}'.format(ebd_mat.shape))\n",
    "seq = tok_dict[col]\n",
    "tseq = seq.texts_to_sequences(X_[col])\n",
    "pseq = sequence.pad_sequences(tseq).squeeze()\n",
    "extract = ebd_mat[pseq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T16:03:46.533813Z",
     "start_time": "2018-09-25T15:53:42.326Z"
    }
   },
   "outputs": [],
   "source": [
    "extract_list = []\n",
    "for col in tqdm(need_process_col):\n",
    "    ebd_mat = model.get_layer('ebd_{}'.format(col)).get_weights()[0]\n",
    "    seq = tok_dict[col]\n",
    "    tseq = seq.texts_to_sequences(X_[col])\n",
    "    pseq = sequence.pad_sequences(tseq).squeeze()\n",
    "    extract = ebd_mat[pseq]\n",
    "    extract_list.append(extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T16:03:46.534833Z",
     "start_time": "2018-09-25T15:53:42.331Z"
    }
   },
   "outputs": [],
   "source": [
    "X_matrix = np.concatenate(extract_list,axis=1)\n",
    "print(X_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T16:03:46.535833Z",
     "start_time": "2018-09-25T15:53:42.335Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(X_matrix,open(FILE.X_fe_emd_format.value.format('col{}_hasUtag'.format(X_matrix.shape[1])),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
