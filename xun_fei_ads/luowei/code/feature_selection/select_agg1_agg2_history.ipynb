{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T03:31:38.939333Z",
     "start_time": "2018-09-14T03:31:38.595019Z"
    }
   },
   "outputs": [],
   "source": [
    "__file__=''\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../LIB/'))\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../../../../automl/automl_libs/'))\n",
    "from env import FILE\n",
    "import utils\n",
    "from itertools import combinations\n",
    "from feature_engineering import *\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T03:31:42.702496Z",
     "start_time": "2018-09-14T03:31:39.595807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: (1001650, 809)\n",
      "test shape is: (40024, 808)\n"
     ]
    }
   ],
   "source": [
    "# train = pd.read_pickle(FILE.train_agg_v1.value)\n",
    "train = pd.read_pickle(FILE.train_agg_v1_select.value)\n",
    "print('train shape is: {}'.format(train.shape))\n",
    "# test = pd.read_pickle(FILE.test_agg_v1.value)\n",
    "test = pd.read_pickle(FILE.test_agg_v1_select.value)\n",
    "print('test shape is: {}'.format(test.shape))\n",
    "\n",
    "# train = train1.iloc[:,:200].copy()\n",
    "# train['click'] = train1['click'].copy()\n",
    "# test = test1.iloc[:,:200].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# down_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T03:31:48.409859Z",
     "start_time": "2018-09-14T03:31:44.839Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# def down_sample(train,seed=23,seed_shuffle=9,target='click'):\n",
    "#     df_posi = train[train[target]==1].copy()\n",
    "#     print(df_posi.shape)\n",
    "#     df_neg = train[train[target]==0].copy()\n",
    "#     print(df_neg.shape)\n",
    "#     df_neg_sample = df_neg.sample(n=len(df_posi),random_state=seed)\n",
    "#     print('done selecting')\n",
    "#     df = pd.concat([df_posi,df_neg_sample])\n",
    "#     df = df.sample(frac=1,random_state=seed_shuffle)\n",
    "#     print('done sampling')\n",
    "#     df = df.reset_index(drop=True)\n",
    "#     print(df.shape)\n",
    "#     return df\n",
    "# train_sample = down_sample(train,seed=23,seed_shuffle=9,target='click')\n",
    "# # del train\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T03:31:51.663678Z",
     "start_time": "2018-09-14T03:31:51.492827Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print_to_file = False \n",
    "test_run = False \n",
    "\n",
    "def get_time(timezone='America/New_York', time_format='%Y-%m-%d %H:%M:%S'):\n",
    "    from datetime import datetime\n",
    "    from dateutil import tz\n",
    "\n",
    "    # METHOD 1: Hardcode zones:\n",
    "    from_zone = tz.gettz('UTC')\n",
    "    to_zone = tz.gettz(timezone)\n",
    "\n",
    "    utc = datetime.utcnow()\n",
    "\n",
    "    # Tell the datetime object that it's in UTC time zone since \n",
    "    # datetime objects are 'naive' by default\n",
    "    utc = utc.replace(tzinfo=from_zone)\n",
    "\n",
    "    # Convert time zone\n",
    "    est = utc.astimezone(to_zone)\n",
    "\n",
    "    return est.strftime(time_format)\n",
    "\n",
    "import sys, time\n",
    "class Logger(object):\n",
    "    def __init__(self, logtofile=True, logfilename='log'):\n",
    "        self.terminal = sys.stdout\n",
    "        self.logfile = \"{}_{}.log\".format(logfilename, int(time.time()))\n",
    "        self.logtofile = logtofile\n",
    "\n",
    "    def write(self, message):\n",
    "        #         self.terminal.write(message)\n",
    "        if self.logtofile:\n",
    "            self.log = open(self.logfile, \"a\")\n",
    "            self.log.write('[' + get_time() + '] ' + message)\n",
    "            self.log.close()\n",
    "\n",
    "    def flush(self):\n",
    "        # this flush method is needed for python 3 compatibility.\n",
    "        # this handles the flush command by doing nothing.\n",
    "        # you might want to specify some extra behavior here.\n",
    "        pass\n",
    "\n",
    "\n",
    "def divert_printout_to_file():\n",
    "    sys.stdout = Logger(logfilename='logfile')\n",
    "\n",
    "if print_to_file:\n",
    "    divert_printout_to_file()  # note: comment this to use pdb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "def kfold_lightgbm(df, train_df, test_df, holdout, num_folds, submission_file_name, fe_img_name, stratified = False, debug= False, colsample=0.67, max_depth=8, num_leaves=31, min_child_samples=20, subsample=0.7, reg_lambda=0.3, lr=0.04, seed=1001, verbose=100, rounds=None):\n",
    "    print(train_df.shape, test_df.shape, holdout.shape)\n",
    "    print('MEAN: train({}) vs holdout({}): '.format(len(train_df), len(holdout)), train_df['click'].mean(), holdout['click'].mean())\n",
    "    # Divide in training/validation and test data\n",
    "    if df is not None:\n",
    "        train_df = df[df['click'].notnull()]\n",
    "        test_df = df[df['click'].isnull()]\n",
    "        print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "        del df\n",
    "        gc.collect()\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=seed)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=seed)\n",
    "        \n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    holdout_final_preds = np.zeros(holdout.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feature_importance_gain_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in ['click','instance_id','index','time']]\n",
    "    train_scores = []\n",
    "    holdout_scores = []\n",
    "    scores = []\n",
    "    diff_val_holdout = []\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['click'])):\n",
    "#         print('valid index : ',list(valid_idx)[:5])\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['click'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['click'].iloc[valid_idx]\n",
    "#         print('MEAN: train({}) vs valid({}): '.format(len(train_y), len(valid_y)), np.mean(train_y), np.mean(valid_y))\n",
    "\n",
    "        clf = LGBMClassifier(\n",
    "            nthread=10,\n",
    "            n_estimators=30000,\n",
    "            learning_rate=lr,\n",
    "            num_leaves=num_leaves,\n",
    "            colsample_bytree=colsample, # 0.67\n",
    "            subsample=subsample,\n",
    "            subsample_freq=0, ## disable subsampling\n",
    "            max_depth=max_depth,\n",
    "#             reg_alpha=0.65,\n",
    "            reg_lambda=reg_lambda,\n",
    "#             min_split_gain=0.0222415,\n",
    "#             min_child_weight=39.3259775,\n",
    "            min_child_samples=min_child_samples,\n",
    "            silent=-1,\n",
    "            verbose=-1, )\n",
    "        if rounds is not None:\n",
    "            clf.n_estimators = rounds\n",
    "            clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "                eval_metric='logloss', verbose=verbose)\n",
    "            oof_preds[valid_idx] = clf.predict_proba(valid_x)[:, 1]\n",
    "            sub_preds += clf.predict_proba(test_df[feats])[:, 1] / folds.n_splits\n",
    "            holdout_preds = clf.predict_proba(holdout[feats])[:, 1] \n",
    "        else:\n",
    "            clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "                eval_metric= 'logloss', verbose=verbose, early_stopping_rounds= 200)\n",
    "            oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "            sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "            holdout_preds = clf.predict_proba(holdout[feats], num_iteration=clf.best_iteration_)[:, 1] \n",
    "            \n",
    "        holdout_final_preds += holdout_preds / folds.n_splits\n",
    "        score = log_loss(valid_y, oof_preds[valid_idx])\n",
    "        train_score = clf.best_score_['training']['binary_logloss']\n",
    "        holdout_score = log_loss(holdout['click'], holdout_preds)\n",
    "        diff = abs(score - holdout_score)\n",
    "        best_rounds = rounds if rounds is not None else clf.best_iteration_\n",
    "        print('Fold %2d [%5d] AUC : ho: %.6f / te: %.6f / tr: %.6f (diff: %.6f)' % (n_fold + 1, best_rounds, holdout_score, score,  train_score, diff))\n",
    "        scores.append(score)\n",
    "        train_scores.append(train_score)\n",
    "        holdout_scores.append(holdout_score)\n",
    "        diff_val_holdout.append(diff)\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        \n",
    "        fold_importance_gain_df = pd.DataFrame()\n",
    "        fold_importance_gain_df[\"feature\"] = feats\n",
    "        fold_importance_gain_df[\"importance\"] = clf.booster_.feature_importance(importance_type='gain')\n",
    "        fold_importance_gain_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_gain_df = pd.concat([feature_importance_gain_df, fold_importance_gain_df], axis=0)\n",
    "        \n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "    holdout_roc = roc_auc_score(holdout['click'], holdout_final_preds)\n",
    "    holdout_mean = np.mean(holdout_scores)\n",
    "    full_te_mean = np.mean(scores)\n",
    "    full_tr_mean = np.mean(train_scores)\n",
    "#     print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "    print('Full HO score %.6f' % holdout_roc)\n",
    "    print('FULL HO mean {:.6f}, std {:.6f}'.format(holdout_mean, np.std(holdout_scores)))\n",
    "    print('FULL TE mean {:.6f}, std {:.6f}'.format(full_te_mean, np.std(scores)))\n",
    "    print('FULL TR mean {:.6f}, std {:.6f}'.format(full_tr_mean, np.std(train_scores)))\n",
    "    print('FULL DIFF mean {:.6f}, std {:.6f}'.format(np.mean(diff_val_holdout), np.std(diff_val_holdout)))\n",
    "    # Write submission file and plot feature importance\n",
    "    if not debug:\n",
    "        test_df['predicted_score'] = sub_preds\n",
    "        test_df[['instance_id', 'predicted_score']].to_csv(submission_file_name, index= False)\n",
    "#     if not print_to_file:\n",
    "#         display_importances(feature_importance_df, fe_img_name)\n",
    "    feature_importance_df = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "    feature_importance_gain_df = feature_importance_gain_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "    return feature_importance_df, feature_importance_gain_df,holdout_roc,holdout_mean,full_te_mean,full_tr_mean,oof_preds \n",
    "\n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_, fe_img_name):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fe_img_name+'.png')\n",
    "\n",
    "\n",
    "def convert_and_save_imp_df(fe_imp_df, dumpfilename):\n",
    "    fe_imp_df_mean = fe_imp_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "    pickle.dump(fe_imp_df_mean, open(dumpfilename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T04:07:23.314559Z",
     "start_time": "2018-09-14T04:07:23.304115Z"
    }
   },
   "outputs": [],
   "source": [
    "def runlgb(train, test, holdout):\n",
    "    colsamples = [0.07]#[0.1,0.15,0.2]#[0.03,0.04,0.05,0.06,0.07,0.08]\n",
    "    seeds = [20]#[300,4000,50000,600000,7000000,80000000,523445,31275479] # 20\n",
    "    depth = [5]\n",
    "    leaves = [16]\n",
    "    min_child_sam = [20]#, 800]\n",
    "    subsamples = [1]#0.8, 0.7, 0.6, 0.5, 0.4] # was 1\n",
    "    reg_lambdas = [0.5]\n",
    "    # lrs = lrs.tolist()\n",
    "    lrs2 = [0.05]\n",
    "    nfolds = 5\n",
    "    rounds = [None] #[1000]#, 1300, 1600, 1900, 2200, 2500]\n",
    "    for seed in seeds:\n",
    "        for colsample in colsamples:\n",
    "            for d in depth:\n",
    "                for l in leaves:\n",
    "                    for mcs in min_child_sam:\n",
    "                        for subsample in subsamples:\n",
    "                            for reg_lambda in reg_lambdas:\n",
    "                                for lr in lrs2:\n",
    "                                    for r in rounds:\n",
    "                                        filename = 'fe_936_col{}_lr{}_n{}'.format(len(train.columns), lr, nfolds)\n",
    "                                        print('#############################################')\n",
    "                                        print(colsample, seed, d, l, mcs, subsample, reg_lambda, lr, 'nfolds:', nfolds)\n",
    "                                        print('#############################################')\n",
    "                                        numfeats = len(train.columns)\n",
    "                                        with timer(\"Run LightGBM with kfold\"):\n",
    "                                            return kfold_lightgbm(None, train, test, holdout, nfolds, filename+'.csv', filename, colsample=colsample, verbose=100, max_depth=d, num_leaves=l, min_child_samples=mcs, subsample=subsample, reg_lambda=reg_lambda, lr=lr, seed=seed, stratified=True, rounds=r,debug=False)                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T03:32:04.918356Z",
     "start_time": "2018-09-14T03:32:00.468276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN: train(1001549) vs holdout(101):  0.19846158300792074 0.1782178217821782\n",
      "(1001549, 809) (40024, 808) (101, 809)\n"
     ]
    }
   ],
   "source": [
    "train_df, holdout = train_test_split(train, test_size=1/10000, random_state=42)\n",
    "#42\n",
    "print('MEAN: train({}) vs holdout({}): '.format(len(train_df), len(holdout)), train_df['click'].mean(), holdout['click'].mean())\n",
    "print(train_df.shape, test.shape, holdout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T03:36:56.097699Z",
     "start_time": "2018-09-14T03:32:09.264612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################################\n",
      "0.07 20 5 16 20 1 0.5 0.05 nfolds: 5\n",
      "#############################################\n",
      "(1001650, 809) (40024, 808) (101, 809)\n",
      "MEAN: train(1001650) vs holdout(101):  0.19845954175610242 0.1782178217821782\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.419999\tvalid_1's binary_logloss: 0.420441\n",
      "[200]\ttraining's binary_logloss: 0.416112\tvalid_1's binary_logloss: 0.417871\n",
      "[300]\ttraining's binary_logloss: 0.413917\tvalid_1's binary_logloss: 0.417052\n",
      "[400]\ttraining's binary_logloss: 0.41237\tvalid_1's binary_logloss: 0.41675\n",
      "[500]\ttraining's binary_logloss: 0.411016\tvalid_1's binary_logloss: 0.416597\n",
      "[600]\ttraining's binary_logloss: 0.409863\tvalid_1's binary_logloss: 0.416547\n",
      "[700]\ttraining's binary_logloss: 0.40866\tvalid_1's binary_logloss: 0.416485\n",
      "[800]\ttraining's binary_logloss: 0.40747\tvalid_1's binary_logloss: 0.416429\n",
      "[900]\ttraining's binary_logloss: 0.406333\tvalid_1's binary_logloss: 0.416383\n",
      "[1000]\ttraining's binary_logloss: 0.405312\tvalid_1's binary_logloss: 0.416385\n",
      "Early stopping, best iteration is:\n",
      "[889]\ttraining's binary_logloss: 0.406442\tvalid_1's binary_logloss: 0.416373\n",
      "Fold  1 [  889] AUC : ho: 0.373577 / te: 0.416373 / tr: 0.406442 (diff: 0.042796)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.41978\tvalid_1's binary_logloss: 0.421182\n",
      "[200]\ttraining's binary_logloss: 0.415918\tvalid_1's binary_logloss: 0.418757\n",
      "[300]\ttraining's binary_logloss: 0.4137\tvalid_1's binary_logloss: 0.418017\n",
      "[400]\ttraining's binary_logloss: 0.412139\tvalid_1's binary_logloss: 0.417736\n",
      "[500]\ttraining's binary_logloss: 0.410801\tvalid_1's binary_logloss: 0.41758\n",
      "[600]\ttraining's binary_logloss: 0.409637\tvalid_1's binary_logloss: 0.417544\n",
      "[700]\ttraining's binary_logloss: 0.408415\tvalid_1's binary_logloss: 0.417479\n",
      "[800]\ttraining's binary_logloss: 0.407266\tvalid_1's binary_logloss: 0.417463\n",
      "[900]\ttraining's binary_logloss: 0.40613\tvalid_1's binary_logloss: 0.41741\n",
      "[1000]\ttraining's binary_logloss: 0.405104\tvalid_1's binary_logloss: 0.417426\n",
      "[1100]\ttraining's binary_logloss: 0.404079\tvalid_1's binary_logloss: 0.417424\n",
      "Early stopping, best iteration is:\n",
      "[907]\ttraining's binary_logloss: 0.406043\tvalid_1's binary_logloss: 0.417408\n",
      "Fold  2 [  907] AUC : ho: 0.368358 / te: 0.417408 / tr: 0.406043 (diff: 0.049050)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.4198\tvalid_1's binary_logloss: 0.42146\n",
      "[200]\ttraining's binary_logloss: 0.415911\tvalid_1's binary_logloss: 0.418964\n",
      "[300]\ttraining's binary_logloss: 0.413668\tvalid_1's binary_logloss: 0.418203\n",
      "[400]\ttraining's binary_logloss: 0.412085\tvalid_1's binary_logloss: 0.417865\n",
      "[500]\ttraining's binary_logloss: 0.410763\tvalid_1's binary_logloss: 0.417733\n",
      "[600]\ttraining's binary_logloss: 0.409586\tvalid_1's binary_logloss: 0.41771\n",
      "[700]\ttraining's binary_logloss: 0.408379\tvalid_1's binary_logloss: 0.417652\n",
      "[800]\ttraining's binary_logloss: 0.407207\tvalid_1's binary_logloss: 0.417598\n",
      "[900]\ttraining's binary_logloss: 0.406069\tvalid_1's binary_logloss: 0.417532\n",
      "[1000]\ttraining's binary_logloss: 0.405052\tvalid_1's binary_logloss: 0.417552\n",
      "[1100]\ttraining's binary_logloss: 0.404036\tvalid_1's binary_logloss: 0.41755\n",
      "Early stopping, best iteration is:\n",
      "[903]\ttraining's binary_logloss: 0.406036\tvalid_1's binary_logloss: 0.417526\n",
      "Fold  3 [  903] AUC : ho: 0.376503 / te: 0.417526 / tr: 0.406036 (diff: 0.041023)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.41964\tvalid_1's binary_logloss: 0.42151\n",
      "[200]\ttraining's binary_logloss: 0.415772\tvalid_1's binary_logloss: 0.419172\n",
      "[300]\ttraining's binary_logloss: 0.413556\tvalid_1's binary_logloss: 0.418482\n",
      "[400]\ttraining's binary_logloss: 0.411991\tvalid_1's binary_logloss: 0.418214\n",
      "[500]\ttraining's binary_logloss: 0.410651\tvalid_1's binary_logloss: 0.418083\n",
      "[600]\ttraining's binary_logloss: 0.409466\tvalid_1's binary_logloss: 0.418054\n",
      "[700]\ttraining's binary_logloss: 0.408258\tvalid_1's binary_logloss: 0.418018\n",
      "[800]\ttraining's binary_logloss: 0.407071\tvalid_1's binary_logloss: 0.417999\n",
      "[900]\ttraining's binary_logloss: 0.405957\tvalid_1's binary_logloss: 0.418022\n",
      "Early stopping, best iteration is:\n",
      "[798]\ttraining's binary_logloss: 0.40709\tvalid_1's binary_logloss: 0.417997\n",
      "Fold  4 [  798] AUC : ho: 0.371524 / te: 0.417997 / tr: 0.407090 (diff: 0.046473)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.419749\tvalid_1's binary_logloss: 0.421364\n",
      "[200]\ttraining's binary_logloss: 0.41589\tvalid_1's binary_logloss: 0.41894\n",
      "[300]\ttraining's binary_logloss: 0.413672\tvalid_1's binary_logloss: 0.418079\n",
      "[400]\ttraining's binary_logloss: 0.412119\tvalid_1's binary_logloss: 0.417818\n",
      "[500]\ttraining's binary_logloss: 0.410761\tvalid_1's binary_logloss: 0.417666\n",
      "[600]\ttraining's binary_logloss: 0.409589\tvalid_1's binary_logloss: 0.4176\n",
      "[700]\ttraining's binary_logloss: 0.408345\tvalid_1's binary_logloss: 0.417528\n",
      "[800]\ttraining's binary_logloss: 0.407163\tvalid_1's binary_logloss: 0.417465\n",
      "[900]\ttraining's binary_logloss: 0.406058\tvalid_1's binary_logloss: 0.417381\n",
      "[1000]\ttraining's binary_logloss: 0.405014\tvalid_1's binary_logloss: 0.417357\n",
      "[1100]\ttraining's binary_logloss: 0.403974\tvalid_1's binary_logloss: 0.417315\n",
      "[1200]\ttraining's binary_logloss: 0.403017\tvalid_1's binary_logloss: 0.41734\n",
      "[1300]\ttraining's binary_logloss: 0.402052\tvalid_1's binary_logloss: 0.417336\n",
      "Early stopping, best iteration is:\n",
      "[1127]\ttraining's binary_logloss: 0.403722\tvalid_1's binary_logloss: 0.417303\n",
      "Fold  5 [ 1127] AUC : ho: 0.362886 / te: 0.417303 / tr: 0.403722 (diff: 0.054417)\n",
      "Full HO score 0.831995\n",
      "FULL HO mean 0.370570, std 0.004671\n",
      "FULL TE mean 0.417321, std 0.000530\n",
      "FULL TR mean 0.405866, std 0.001139\n",
      "FULL DIFF mean 0.046752, std 0.004745\n",
      "Run LightGBM with kfold - done in 287s\n"
     ]
    }
   ],
   "source": [
    "feature_importance_df, feature_importance_gain_df,holdout_roc,holdout_mean,full_te_mean,full_tr_mean,oof_preds = runlgb(train, test, holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-13T20:40:27.108273Z",
     "start_time": "2018-09-13T20:31:08.526215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################################\n",
      "0.07 20 5 16 20 1 0.5 0.05 nfolds: 5\n",
      "#############################################\n",
      "(1001650, 1668) (40024, 1667) (101, 1668)\n",
      "MEAN: train(1001650) vs holdout(101):  0.19845954175610242 0.1782178217821782\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.419158\tvalid_1's binary_logloss: 0.419554\n",
      "[200]\ttraining's binary_logloss: 0.415661\tvalid_1's binary_logloss: 0.41747\n",
      "[300]\ttraining's binary_logloss: 0.413587\tvalid_1's binary_logloss: 0.416878\n",
      "[400]\ttraining's binary_logloss: 0.4119\tvalid_1's binary_logloss: 0.416654\n",
      "[500]\ttraining's binary_logloss: 0.410438\tvalid_1's binary_logloss: 0.41655\n",
      "[600]\ttraining's binary_logloss: 0.40917\tvalid_1's binary_logloss: 0.416511\n",
      "[700]\ttraining's binary_logloss: 0.407961\tvalid_1's binary_logloss: 0.416508\n",
      "[800]\ttraining's binary_logloss: 0.406799\tvalid_1's binary_logloss: 0.416514\n",
      "Early stopping, best iteration is:\n",
      "[670]\ttraining's binary_logloss: 0.408342\tvalid_1's binary_logloss: 0.416489\n",
      "Fold  1 [  670] AUC : ho: 0.376165 / te: 0.416489 / tr: 0.408342 (diff: 0.040324)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.418922\tvalid_1's binary_logloss: 0.420342\n",
      "[200]\ttraining's binary_logloss: 0.415332\tvalid_1's binary_logloss: 0.418331\n",
      "[300]\ttraining's binary_logloss: 0.413267\tvalid_1's binary_logloss: 0.417799\n",
      "[400]\ttraining's binary_logloss: 0.411589\tvalid_1's binary_logloss: 0.417592\n",
      "[500]\ttraining's binary_logloss: 0.410118\tvalid_1's binary_logloss: 0.417462\n",
      "[600]\ttraining's binary_logloss: 0.408871\tvalid_1's binary_logloss: 0.417417\n",
      "[700]\ttraining's binary_logloss: 0.407651\tvalid_1's binary_logloss: 0.417417\n",
      "[800]\ttraining's binary_logloss: 0.406467\tvalid_1's binary_logloss: 0.417392\n",
      "[900]\ttraining's binary_logloss: 0.405303\tvalid_1's binary_logloss: 0.417383\n",
      "[1000]\ttraining's binary_logloss: 0.404187\tvalid_1's binary_logloss: 0.417402\n",
      "Early stopping, best iteration is:\n",
      "[868]\ttraining's binary_logloss: 0.405669\tvalid_1's binary_logloss: 0.417367\n",
      "Fold  2 [  868] AUC : ho: 0.369307 / te: 0.417367 / tr: 0.405669 (diff: 0.048060)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.418925\tvalid_1's binary_logloss: 0.42063\n",
      "[200]\ttraining's binary_logloss: 0.415361\tvalid_1's binary_logloss: 0.418581\n",
      "[300]\ttraining's binary_logloss: 0.41326\tvalid_1's binary_logloss: 0.41803\n",
      "[400]\ttraining's binary_logloss: 0.411573\tvalid_1's binary_logloss: 0.417832\n",
      "[500]\ttraining's binary_logloss: 0.410109\tvalid_1's binary_logloss: 0.417727\n",
      "[600]\ttraining's binary_logloss: 0.408828\tvalid_1's binary_logloss: 0.417684\n",
      "[700]\ttraining's binary_logloss: 0.407618\tvalid_1's binary_logloss: 0.417649\n",
      "[800]\ttraining's binary_logloss: 0.406451\tvalid_1's binary_logloss: 0.417674\n",
      "[900]\ttraining's binary_logloss: 0.405274\tvalid_1's binary_logloss: 0.417679\n",
      "Early stopping, best iteration is:\n",
      "[700]\ttraining's binary_logloss: 0.407618\tvalid_1's binary_logloss: 0.417649\n",
      "Fold  3 [  700] AUC : ho: 0.369640 / te: 0.417649 / tr: 0.407618 (diff: 0.048009)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.418802\tvalid_1's binary_logloss: 0.420827\n",
      "[200]\ttraining's binary_logloss: 0.415257\tvalid_1's binary_logloss: 0.418887\n",
      "[300]\ttraining's binary_logloss: 0.413149\tvalid_1's binary_logloss: 0.418345\n",
      "[400]\ttraining's binary_logloss: 0.411456\tvalid_1's binary_logloss: 0.418132\n",
      "[500]\ttraining's binary_logloss: 0.409973\tvalid_1's binary_logloss: 0.418057\n",
      "[600]\ttraining's binary_logloss: 0.40869\tvalid_1's binary_logloss: 0.418015\n",
      "[700]\ttraining's binary_logloss: 0.407451\tvalid_1's binary_logloss: 0.417999\n",
      "[800]\ttraining's binary_logloss: 0.406257\tvalid_1's binary_logloss: 0.417998\n",
      "Early stopping, best iteration is:\n",
      "[682]\ttraining's binary_logloss: 0.407671\tvalid_1's binary_logloss: 0.417985\n",
      "Fold  4 [  682] AUC : ho: 0.371629 / te: 0.417985 / tr: 0.407671 (diff: 0.046356)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.418985\tvalid_1's binary_logloss: 0.420564\n",
      "[200]\ttraining's binary_logloss: 0.415447\tvalid_1's binary_logloss: 0.418501\n",
      "[300]\ttraining's binary_logloss: 0.413382\tvalid_1's binary_logloss: 0.417905\n",
      "[400]\ttraining's binary_logloss: 0.411693\tvalid_1's binary_logloss: 0.417658\n",
      "[500]\ttraining's binary_logloss: 0.410217\tvalid_1's binary_logloss: 0.417511\n",
      "[600]\ttraining's binary_logloss: 0.408964\tvalid_1's binary_logloss: 0.417451\n",
      "[700]\ttraining's binary_logloss: 0.407731\tvalid_1's binary_logloss: 0.417407\n",
      "[800]\ttraining's binary_logloss: 0.406551\tvalid_1's binary_logloss: 0.417392\n",
      "[900]\ttraining's binary_logloss: 0.40534\tvalid_1's binary_logloss: 0.417368\n",
      "[1000]\ttraining's binary_logloss: 0.404225\tvalid_1's binary_logloss: 0.417388\n",
      "Early stopping, best iteration is:\n",
      "[878]\ttraining's binary_logloss: 0.405571\tvalid_1's binary_logloss: 0.41736\n",
      "Fold  5 [  878] AUC : ho: 0.364280 / te: 0.417360 / tr: 0.405571 (diff: 0.053080)\n",
      "Full HO score 0.834672\n",
      "FULL HO mean 0.370204, std 0.003841\n",
      "FULL TE mean 0.417370, std 0.000497\n",
      "FULL TR mean 0.406974, std 0.001135\n",
      "FULL DIFF mean 0.047166, std 0.004098\n",
      "Run LightGBM with kfold - done in 559s\n"
     ]
    }
   ],
   "source": [
    "feature_importance_df, feature_importance_gain_df,holdout_roc,holdout_mean,full_te_mean,full_tr_mean,oof_preds = runlgb(train, test, holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T03:37:39.198737Z",
     "start_time": "2018-09-14T03:37:39.193962Z"
    }
   },
   "outputs": [],
   "source": [
    "# base_gain = feature_importance_gain_df.copy()\n",
    "# base_split = feature_importance_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T04:12:06.827901Z",
     "start_time": "2018-09-14T04:12:06.821016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797\n"
     ]
    }
   ],
   "source": [
    "gain_fe = base_gain.iloc[:800]['feature'].values\n",
    "split_fe = base_split.iloc[:800]['feature'].values\n",
    "all_fe = set(gain_fe).intersection(set(split_fe))\n",
    "print(len(all_fe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T04:12:42.185025Z",
     "start_time": "2018-09-14T04:12:40.407429Z"
    }
   },
   "outputs": [],
   "source": [
    "all_fe = list(all_fe)\n",
    "all_fe.append('instance_id')\n",
    "train_fe = list(all_fe).copy()\n",
    "train_fe.append('click')\n",
    "\n",
    "train_new = train[train_fe].copy()\n",
    "test_new = test[all_fe].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T04:12:49.783549Z",
     "start_time": "2018-09-14T04:12:45.412978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN: train(1001549) vs holdout(101):  0.19846158300792074 0.1782178217821782\n",
      "(1001549, 799) (40024, 809) (101, 799)\n"
     ]
    }
   ],
   "source": [
    "train_df, holdout = train_test_split(train_new, test_size=1/10000, random_state=42)\n",
    "#42\n",
    "print('MEAN: train({}) vs holdout({}): '.format(len(train_df), len(holdout)), train_df['click'].mean(), holdout['click'].mean())\n",
    "print(train_df.shape, test.shape, holdout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T04:17:37.271016Z",
     "start_time": "2018-09-14T04:12:55.428979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################################\n",
      "0.07 20 5 16 20 1 0.5 0.05 nfolds: 5\n",
      "#############################################\n",
      "(1001650, 799) (40024, 798) (101, 799)\n",
      "MEAN: train(1001650) vs holdout(101):  0.19845954175610242 0.1782178217821782\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.420173\tvalid_1's binary_logloss: 0.420356\n",
      "[200]\ttraining's binary_logloss: 0.416333\tvalid_1's binary_logloss: 0.417832\n",
      "[300]\ttraining's binary_logloss: 0.414112\tvalid_1's binary_logloss: 0.417094\n",
      "[400]\ttraining's binary_logloss: 0.41237\tvalid_1's binary_logloss: 0.416684\n",
      "[500]\ttraining's binary_logloss: 0.411016\tvalid_1's binary_logloss: 0.416611\n",
      "[600]\ttraining's binary_logloss: 0.409883\tvalid_1's binary_logloss: 0.416565\n",
      "[700]\ttraining's binary_logloss: 0.408637\tvalid_1's binary_logloss: 0.4165\n",
      "[800]\ttraining's binary_logloss: 0.407571\tvalid_1's binary_logloss: 0.416513\n",
      "[900]\ttraining's binary_logloss: 0.406516\tvalid_1's binary_logloss: 0.416513\n",
      "Early stopping, best iteration is:\n",
      "[737]\ttraining's binary_logloss: 0.408197\tvalid_1's binary_logloss: 0.416481\n",
      "Fold  1 [  737] AUC : ho: 0.375329 / te: 0.416481 / tr: 0.408197 (diff: 0.041153)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.419901\tvalid_1's binary_logloss: 0.421077\n",
      "[200]\ttraining's binary_logloss: 0.416087\tvalid_1's binary_logloss: 0.418696\n",
      "[300]\ttraining's binary_logloss: 0.41386\tvalid_1's binary_logloss: 0.418026\n",
      "[400]\ttraining's binary_logloss: 0.412092\tvalid_1's binary_logloss: 0.417628\n",
      "[500]\ttraining's binary_logloss: 0.410703\tvalid_1's binary_logloss: 0.417517\n",
      "[600]\ttraining's binary_logloss: 0.409511\tvalid_1's binary_logloss: 0.41743\n",
      "[700]\ttraining's binary_logloss: 0.408304\tvalid_1's binary_logloss: 0.417377\n",
      "[800]\ttraining's binary_logloss: 0.407236\tvalid_1's binary_logloss: 0.417349\n",
      "[900]\ttraining's binary_logloss: 0.406174\tvalid_1's binary_logloss: 0.417326\n",
      "[1000]\ttraining's binary_logloss: 0.405116\tvalid_1's binary_logloss: 0.417325\n",
      "[1100]\ttraining's binary_logloss: 0.404024\tvalid_1's binary_logloss: 0.417301\n",
      "[1200]\ttraining's binary_logloss: 0.402987\tvalid_1's binary_logloss: 0.417307\n",
      "[1300]\ttraining's binary_logloss: 0.401938\tvalid_1's binary_logloss: 0.417317\n",
      "Early stopping, best iteration is:\n",
      "[1115]\ttraining's binary_logloss: 0.403835\tvalid_1's binary_logloss: 0.417278\n",
      "Fold  2 [ 1115] AUC : ho: 0.366285 / te: 0.417278 / tr: 0.403835 (diff: 0.050993)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.419851\tvalid_1's binary_logloss: 0.421329\n",
      "[200]\ttraining's binary_logloss: 0.416065\tvalid_1's binary_logloss: 0.418958\n",
      "[300]\ttraining's binary_logloss: 0.413837\tvalid_1's binary_logloss: 0.418211\n",
      "[400]\ttraining's binary_logloss: 0.412104\tvalid_1's binary_logloss: 0.417807\n",
      "[500]\ttraining's binary_logloss: 0.410731\tvalid_1's binary_logloss: 0.417687\n",
      "[600]\ttraining's binary_logloss: 0.409546\tvalid_1's binary_logloss: 0.417661\n",
      "[700]\ttraining's binary_logloss: 0.408324\tvalid_1's binary_logloss: 0.417638\n",
      "[800]\ttraining's binary_logloss: 0.407237\tvalid_1's binary_logloss: 0.417628\n",
      "[900]\ttraining's binary_logloss: 0.406137\tvalid_1's binary_logloss: 0.417631\n",
      "[1000]\ttraining's binary_logloss: 0.40505\tvalid_1's binary_logloss: 0.417632\n",
      "Early stopping, best iteration is:\n",
      "[835]\ttraining's binary_logloss: 0.40686\tvalid_1's binary_logloss: 0.417613\n",
      "Fold  3 [  835] AUC : ho: 0.373319 / te: 0.417613 / tr: 0.406860 (diff: 0.044295)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.4198\tvalid_1's binary_logloss: 0.421625\n",
      "[200]\ttraining's binary_logloss: 0.415973\tvalid_1's binary_logloss: 0.419304\n",
      "[300]\ttraining's binary_logloss: 0.413726\tvalid_1's binary_logloss: 0.41862\n",
      "[400]\ttraining's binary_logloss: 0.411976\tvalid_1's binary_logloss: 0.41828\n",
      "[500]\ttraining's binary_logloss: 0.410599\tvalid_1's binary_logloss: 0.418154\n",
      "[600]\ttraining's binary_logloss: 0.409427\tvalid_1's binary_logloss: 0.418105\n",
      "[700]\ttraining's binary_logloss: 0.408213\tvalid_1's binary_logloss: 0.41808\n",
      "[800]\ttraining's binary_logloss: 0.407121\tvalid_1's binary_logloss: 0.418073\n",
      "[900]\ttraining's binary_logloss: 0.406071\tvalid_1's binary_logloss: 0.418088\n",
      "[1000]\ttraining's binary_logloss: 0.405017\tvalid_1's binary_logloss: 0.418097\n",
      "Early stopping, best iteration is:\n",
      "[833]\ttraining's binary_logloss: 0.406776\tvalid_1's binary_logloss: 0.418051\n",
      "Fold  4 [  833] AUC : ho: 0.368318 / te: 0.418051 / tr: 0.406776 (diff: 0.049733)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.419934\tvalid_1's binary_logloss: 0.421327\n",
      "[200]\ttraining's binary_logloss: 0.416113\tvalid_1's binary_logloss: 0.418853\n",
      "[300]\ttraining's binary_logloss: 0.413915\tvalid_1's binary_logloss: 0.418104\n",
      "[400]\ttraining's binary_logloss: 0.412196\tvalid_1's binary_logloss: 0.417728\n",
      "[500]\ttraining's binary_logloss: 0.410829\tvalid_1's binary_logloss: 0.417597\n",
      "[600]\ttraining's binary_logloss: 0.409637\tvalid_1's binary_logloss: 0.417534\n",
      "[700]\ttraining's binary_logloss: 0.408419\tvalid_1's binary_logloss: 0.417473\n",
      "[800]\ttraining's binary_logloss: 0.407314\tvalid_1's binary_logloss: 0.417436\n",
      "[900]\ttraining's binary_logloss: 0.406244\tvalid_1's binary_logloss: 0.417404\n",
      "[1000]\ttraining's binary_logloss: 0.40518\tvalid_1's binary_logloss: 0.417406\n",
      "[1100]\ttraining's binary_logloss: 0.404073\tvalid_1's binary_logloss: 0.417404\n",
      "Early stopping, best iteration is:\n",
      "[956]\ttraining's binary_logloss: 0.405646\tvalid_1's binary_logloss: 0.417386\n",
      "Fold  5 [  956] AUC : ho: 0.362573 / te: 0.417386 / tr: 0.405646 (diff: 0.054813)\n",
      "Full HO score 0.836011\n",
      "FULL HO mean 0.369165, std 0.004641\n",
      "FULL TE mean 0.417362, std 0.000514\n",
      "FULL TR mean 0.406263, std 0.001459\n",
      "FULL DIFF mean 0.048197, std 0.004874\n",
      "Run LightGBM with kfold - done in 282s\n"
     ]
    }
   ],
   "source": [
    "df_split, df_gain,holdout_roc,holdout_mean,full_te_mean,full_tr_mean,oof_preds = runlgb(train_new, test_new, holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-13T20:44:43.625392Z",
     "start_time": "2018-09-13T20:44:36.460139Z"
    }
   },
   "outputs": [],
   "source": [
    "train_new.to_pickle(FILE.train_agg_v1_select.value)\n",
    "test_new.to_pickle(FILE.test_agg_v1_select.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T04:23:09.821692Z",
     "start_time": "2018-09-14T04:23:09.805221Z"
    }
   },
   "outputs": [],
   "source": [
    "base_split.to_csv('../../data/features/agg_v1_select_feature_importance_split.csv',index=False)\n",
    "base_gain.to_csv('../../data/features/agg_v1_select_feature_importance_gain.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
