{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T12:27:59.362523Z",
     "start_time": "2018-09-16T12:27:58.905337Z"
    }
   },
   "outputs": [],
   "source": [
    "__file__=''\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../LIB/'))\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../../../../automl/automl_libs/'))\n",
    "from env import FILE\n",
    "import utils\n",
    "from itertools import combinations\n",
    "from feature_engineering import *\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "from sklearn.metrics import log_loss\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T12:28:02.034481Z",
     "start_time": "2018-09-16T12:28:00.081167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: (1001650, 33)\n",
      "train_append shape is: (1001650, 330)\n",
      "test shape is: (40024, 32)\n",
      "test_append shape is: (40024, 329)\n"
     ]
    }
   ],
   "source": [
    "# train = pd.read_pickle(FILE.train_agg_v1.value)\n",
    "train = pd.read_pickle(FILE.train_base_line.value)\n",
    "print('train shape is: {}'.format(train.shape))\n",
    "train_append = pd.read_pickle(FILE.train_agg_v2_select.value)\n",
    "print('train_append shape is: {}'.format(train_append.shape))\n",
    "test = pd.read_pickle(FILE.test_base_line.value)\n",
    "print('test shape is: {}'.format(test.shape))\n",
    "test_append = pd.read_pickle(FILE.test_agg_v2_select.value)\n",
    "print('test_append shape is: {}'.format(test_append.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T15:04:10.766509Z",
     "start_time": "2018-09-16T15:04:10.729956Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print_to_file = False \n",
    "test_run = False \n",
    "\n",
    "def get_time(timezone='America/New_York', time_format='%Y-%m-%d %H:%M:%S'):\n",
    "    from datetime import datetime\n",
    "    from dateutil import tz\n",
    "\n",
    "    # METHOD 1: Hardcode zones:\n",
    "    from_zone = tz.gettz('UTC')\n",
    "    to_zone = tz.gettz(timezone)\n",
    "\n",
    "    utc = datetime.utcnow()\n",
    "\n",
    "    # Tell the datetime object that it's in UTC time zone since \n",
    "    # datetime objects are 'naive' by default\n",
    "    utc = utc.replace(tzinfo=from_zone)\n",
    "\n",
    "    # Convert time zone\n",
    "    est = utc.astimezone(to_zone)\n",
    "\n",
    "    return est.strftime(time_format)\n",
    "\n",
    "import sys, time\n",
    "class Logger(object):\n",
    "    def __init__(self, logtofile=True, logfilename='log'):\n",
    "        self.terminal = sys.stdout\n",
    "        self.logfile = \"{}_{}.log\".format(logfilename, int(time.time()))\n",
    "        self.logtofile = logtofile\n",
    "\n",
    "    def write(self, message):\n",
    "        #         self.terminal.write(message)\n",
    "        if self.logtofile:\n",
    "            self.log = open(self.logfile, \"a\")\n",
    "            self.log.write('[' + get_time() + '] ' + message)\n",
    "            self.log.close()\n",
    "\n",
    "    def flush(self):\n",
    "        # this flush method is needed for python 3 compatibility.\n",
    "        # this handles the flush command by doing nothing.\n",
    "        # you might want to specify some extra behavior here.\n",
    "        pass\n",
    "\n",
    "\n",
    "def divert_printout_to_file():\n",
    "    sys.stdout = Logger(logfilename='logfile')\n",
    "\n",
    "if print_to_file:\n",
    "    divert_printout_to_file()  # note: comment this to use pdb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "def kfold_lightgbm(df, train_df, test_df, holdout, num_folds, submission_file_name, fe_img_name, stratified = False, debug= False, colsample=0.67, max_depth=8, num_leaves=31, min_child_samples=20, subsample=0.7, reg_lambda=0.3, lr=0.04, seed=1001, verbose=100, rounds=None, target='click'):\n",
    "    print(train_df.shape, test_df.shape, holdout.shape)\n",
    "    print('MEAN: train({}) vs holdout({}): '.format(len(train_df), len(holdout)), train_df[target].mean(), holdout[target].mean())\n",
    "    # Divide in training/validation and test data\n",
    "    if df is not None:\n",
    "        train_df = df[df[target].notnull()]\n",
    "        test_df = df[df[target].isnull()]\n",
    "        print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "        del df\n",
    "        gc.collect()\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=seed)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=seed)\n",
    "        \n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    holdout_final_preds = np.zeros(holdout.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feature_importance_gain_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in [target,'time','instance_id','index']]\n",
    "    train_scores = []\n",
    "    holdout_scores = []\n",
    "    scores = []\n",
    "    diff_val_holdout = []\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df[target])):\n",
    "#         print('valid index : ',list(valid_idx)[:5])\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df[target].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df[target].iloc[valid_idx]\n",
    "#         print('MEAN: train({}) vs valid({}): '.format(len(train_y), len(valid_y)), np.mean(train_y), np.mean(valid_y))\n",
    "\n",
    "        clf = LGBMClassifier(\n",
    "            nthread=10,\n",
    "            n_estimators=30000,\n",
    "            learning_rate=lr,\n",
    "            device='gpu',\n",
    "#             max_bin =10000,\n",
    "#             num_leaves=num_leaves,\n",
    "#             colsample_bytree=colsample, # 0.67\n",
    "#             subsample=subsample,\n",
    "#             subsample_freq=0, ## disable subsampling\n",
    "#             max_depth=max_depth,\n",
    "#             reg_alpha=0.65,\n",
    "#             reg_lambda=reg_lambda,\n",
    "#             min_split_gain=0.0222415,\n",
    "#             min_child_weight=39.3259775,\n",
    "#             min_child_samples=min_child_samples,\n",
    "            silent=-1,\n",
    "            verbose=-1, )\n",
    "        if rounds is not None:\n",
    "            clf.n_estimators = rounds\n",
    "            clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "                eval_metric= 'logloss', verbose=verbose)\n",
    "            oof_preds[valid_idx] = clf.predict_proba(valid_x)[:, 1]\n",
    "            sub_preds += clf.predict_proba(test_df[feats])[:, 1] / folds.n_splits\n",
    "            holdout_preds = clf.predict_proba(holdout[feats])[:, 1] \n",
    "        else:\n",
    "            clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "                eval_metric= 'logloss', verbose=verbose, early_stopping_rounds= 200)\n",
    "            oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "            sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "            holdout_preds = clf.predict_proba(holdout[feats], num_iteration=clf.best_iteration_)[:, 1] \n",
    "            \n",
    "        holdout_final_preds += holdout_preds / folds.n_splits\n",
    "        score = log_loss(valid_y, oof_preds[valid_idx])\n",
    "        train_score = clf.best_score_['training']['binary_logloss']\n",
    "        holdout_score = log_loss(holdout[target], holdout_preds)\n",
    "        diff = abs(score - holdout_score)\n",
    "        best_rounds = rounds if rounds is not None else clf.best_iteration_\n",
    "        print('Fold %2d [%5d] AUC : ho: %.6f / te: %.6f / tr: %.6f (diff: %.6f)' % (n_fold + 1, best_rounds, holdout_score, score,  train_score, diff))\n",
    "        scores.append(score)\n",
    "        train_scores.append(train_score)\n",
    "        holdout_scores.append(holdout_score)\n",
    "        diff_val_holdout.append(diff)\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        \n",
    "        fold_importance_gain_df = pd.DataFrame()\n",
    "        fold_importance_gain_df[\"feature\"] = feats\n",
    "        fold_importance_gain_df[\"importance\"] = clf.booster_.feature_importance(importance_type='gain')\n",
    "        fold_importance_gain_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_gain_df = pd.concat([feature_importance_gain_df, fold_importance_gain_df], axis=0)\n",
    "        \n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "    holdout_roc = log_loss(holdout[target], holdout_final_preds)\n",
    "    holdout_mean = np.mean(holdout_scores)\n",
    "    full_te_mean = np.mean(scores)\n",
    "    full_tr_mean = np.mean(train_scores)\n",
    "#     print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "    print('Full HO score %.6f' % holdout_roc)\n",
    "    print('FULL HO mean {:.6f}, std {:.6f}'.format(holdout_mean, np.std(holdout_scores)))\n",
    "    print('FULL TE mean {:.6f}, std {:.6f}'.format(full_te_mean, np.std(scores)))\n",
    "    print('FULL TR mean {:.6f}, std {:.6f}'.format(full_tr_mean, np.std(train_scores)))\n",
    "    print('FULL DIFF mean {:.6f}, std {:.6f}'.format(np.mean(diff_val_holdout), np.std(diff_val_holdout)))\n",
    "    # Write submission file and plot feature importance\n",
    "    if not debug:\n",
    "        print('saving...')\n",
    "        test_df['predicted_score'] = sub_preds\n",
    "        test_df[['instance_id', 'predicted_score']].to_csv(submission_file_name, index= False)\n",
    "#     if not print_to_file:\n",
    "#         display_importances(feature_importance_df, fe_img_name)\n",
    "    feature_importance_df = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "    feature_importance_gain_df = feature_importance_gain_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "    return feature_importance_df, feature_importance_gain_df,holdout_roc,holdout_mean,full_te_mean,full_tr_mean,oof_preds\n",
    "\n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_, fe_img_name):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fe_img_name+'.png')\n",
    "\n",
    "\n",
    "def convert_and_save_imp_df(fe_imp_df, dumpfilename):\n",
    "    fe_imp_df_mean = fe_imp_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "    pickle.dump(fe_imp_df_mean, open(dumpfilename,'wb'))\n",
    "    \n",
    "    \n",
    "\n",
    "def select_holdout(train,test,shift=1,seed=6):\n",
    "    samples_size = int(len(test) )\n",
    "    assert shift in range(1,8)\n",
    "    test_time_min = test.time.min()\n",
    "    test_time_max = test.time.max()\n",
    "    train_time_min = train.time.min()\n",
    "    train_time_max = train.time.max()\n",
    "    window_lower_bound = test_time_min - shift*60*60*24\n",
    "    window_upper_bound = test_time_max - shift*60*60*24\n",
    "    available_train = train[(train.time<=window_upper_bound) & (train.time>=window_lower_bound)]\n",
    "    holdout = available_train.sample(n=samples_size,random_state=seed).copy()\n",
    "    train_split_index = list(set(train.index) - set(holdout.index))\n",
    "    train_split = train.iloc[train_split_index].copy()\n",
    "    return train_split,holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T12:28:13.899244Z",
     "start_time": "2018-09-16T12:28:13.895282Z"
    }
   },
   "outputs": [],
   "source": [
    "def runlgb(train, test, holdout):\n",
    "    colsamples = [0.07]#[0.1,0.15,0.2]#[0.03,0.04,0.05,0.06,0.07,0.08]\n",
    "    seeds = [20]#[300,4000,50000,600000,7000000,80000000,523445,31275479] # 20\n",
    "    depth = [5]\n",
    "    leaves = [16]\n",
    "    min_child_sam = [20]#, 800]\n",
    "    subsamples = [1]#0.8, 0.7, 0.6, 0.5, 0.4] # was 1\n",
    "    reg_lambdas = [0.5]\n",
    "    # lrs = lrs.tolist()\n",
    "    lrs2 = [0.1]\n",
    "    nfolds = 5\n",
    "    rounds = [None] #[1000]#, 1300, 1600, 1900, 2200, 2500]\n",
    "    for seed in seeds:\n",
    "        for colsample in colsamples:\n",
    "            for d in depth:\n",
    "                for l in leaves:\n",
    "                    for mcs in min_child_sam:\n",
    "                        for subsample in subsamples:\n",
    "                            for reg_lambda in reg_lambdas:\n",
    "                                for lr in lrs2:\n",
    "                                    for r in rounds:\n",
    "                                        filename = 'fe_936_col{}_lr{}_n{}'.format(len(train.columns), lr, nfolds)\n",
    "                                        print('#############################################')\n",
    "                                        print(colsample, seed, d, l, mcs, subsample, reg_lambda, lr, 'nfolds:', nfolds)\n",
    "                                        print('#############################################')\n",
    "                                        numfeats = len(train.columns)\n",
    "                                        with timer(\"Run LightGBM with kfold\"):\n",
    "                                            return kfold_lightgbm(None, train, test, holdout, nfolds, filename+'.csv', filename, colsample=colsample, verbose=None, max_depth=d, num_leaves=l, min_child_samples=mcs, subsample=subsample, reg_lambda=reg_lambda, lr=lr, seed=seed, stratified=True, rounds=r,debug=True)                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T14:55:01.779020Z",
     "start_time": "2018-09-16T14:55:01.769281Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_loop_each(train,test,train_append,test_append,file_name='add_each_col_score_0915.csv',ignore_list = ['click','instance_id','time'],order_col='instance_id'):\n",
    "    assert np.sum(train[order_col].values != train_append[order_col].values) == 0\n",
    "    assert np.sum(test[order_col].values != test_append[order_col].values) == 0\n",
    "    try:\n",
    "        df_add_result = pd.read_csv(file_name)\n",
    "        processed_list = list(df_add_result['colName'].values)\n",
    "        best = df_add_result.sort_values(['holdout_overall','val_mean']).iloc[0]\n",
    "        best_ho_full = best['holdout_overall']\n",
    "        best_ho_mean = best['holdout_mean']\n",
    "        best_cv_mean = best['val_mean']\n",
    "        \n",
    "    except:\n",
    "        df_add_result = pd.DataFrame()\n",
    "        processed_list = []\n",
    "        best_ho_full = 0\n",
    "        best_ho_mean = 0 \n",
    "        best_cv_mean = 0\n",
    "    need_process_list = list(set(list(train_append.columns))-set(train.columns)-set(processed_list)-set(ignore_list))\n",
    "    need_process_list = ['noadd'] + need_process_list\n",
    "    counter = 1\n",
    "    for col in need_process_list:\n",
    "        print('processing {}, {}/{}'.format(col,counter,len(need_process_list)))\n",
    "        if col in processed_list:\n",
    "            continue\n",
    "        if col not in ignore_list:\n",
    "            train_add = train.copy()\n",
    "            test_add = test.copy()\n",
    "            if col != 'noadd':\n",
    "                train_add[col] = train_append[col].copy()\n",
    "                test_add[col] = test_append[col].copy()\n",
    " \n",
    "            train_df,holdout = select_holdout(train_add,test_add,shift=1,seed=41)\n",
    "            print('train shape: {}, holdout shape: {}'.format(train_df.shape,holdout.shape))\n",
    "            print('train mean: {}, holdout mean: {}'.format(train_df.click.mean(),holdout.click.mean()))\n",
    "            feature_importance_df, feature_importance_gain_df,holdout_roc,holdout_mean,full_te_mean,full_tr_mean,oof_preds = runlgb(train_df, test_add, holdout)\n",
    "            score_new_df = pd.DataFrame({'colName':[col],\n",
    "                                         'holdout_overall':[holdout_roc],\n",
    "                                         'holdout_mean':[holdout_mean],\n",
    "                                         'val_mean':[full_te_mean],\n",
    "                                         'train_mean':[full_tr_mean]})\n",
    "            df_add_result = pd.concat([df_add_result,score_new_df],ignore_index=True,sort=False)\n",
    "            df_add_result.to_csv(file_name,index=False)\n",
    "            processed_list.append(col)\n",
    "            counter+=1\n",
    "            \n",
    "            best = df_add_result.sort_values(['holdout_overall','val_mean']).iloc[0]\n",
    "            best_ho_full = best['holdout_overall']\n",
    "            best_ho_mean = best['holdout_mean']\n",
    "            best_cv_mean = best['val_mean']\n",
    "            print('best holdout overall score is: {}'.format(best_ho_full))\n",
    "            print('best holdout mean score is: {}'.format(best_ho_mean))\n",
    "            print('best cv mean score is: {}'.format(best_cv_mean))\n",
    "    return df_add_result\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T17:12:21.674513Z",
     "start_time": "2018-09-16T17:12:21.631595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 ...\n",
      "processing noadd, 1/1\n",
      "current add list is:['user3_ads1_channel0_hour0_count']\n",
      "round 2 ...\n",
      "processing noadd, 1/1\n"
     ]
    }
   ],
   "source": [
    "file_name_formater = 'add_each_col_score_0915_round{}.csv'\n",
    "dump_name = 'add_cols_0915.pkl'\n",
    "order_col = 'instance_id'\n",
    "\n",
    "assert np.sum(train[order_col].values != train_append[order_col].values) == 0\n",
    "assert np.sum(test[order_col].values != test_append[order_col].values) == 0 \n",
    "try:\n",
    "    add_list = pickle.load(dump_name,'rb')\n",
    "except:\n",
    "    add_list = []\n",
    "r = len(add_list)+1\n",
    "for col in add_list:\n",
    "    train[col] = train_append[col].copy()\n",
    "    test[col] = test_append[col].copy()\n",
    "\n",
    "while True:\n",
    "    print('round {} ...'.format(r))\n",
    "    df_add_result = add_loop_each(train,test,train_append,test_append,file_name_formater.format(r))\n",
    "    df_add_result_base = df_add_result[df_add_result['colName']=='noadd'].iloc[0]\n",
    "    base_ho = df_add_result_base['holdout_overall']\n",
    "    base_cv = df_add_result_base['val_mean']\n",
    "    df_add_result_better = df_add_result[(df_add_result['holdout_overall']<base_ho) &(df_add_result['val_mean']<base_cv)].copy()\n",
    "    if len(df_add_result_better) == 0:\n",
    "        break\n",
    "    else:\n",
    "        df_add_result_better = df_add_result_better.sort_values(['holdout_overall','val_mean'])\n",
    "        col = df_add_result_better.iloc[0]['colName']\n",
    "        train[col] = train_append[col].copy()\n",
    "        test[col] = test_append[col].copy()\n",
    "        add_list.append(col)\n",
    "        pickle.dump(add_list,open(dump_name,'wb'))\n",
    "        r+=1\n",
    "        print('current add list is:{}'.format(add_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T17:12:31.377157Z",
     "start_time": "2018-09-16T17:12:31.372878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user3_ads1_channel0_hour0_count']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
