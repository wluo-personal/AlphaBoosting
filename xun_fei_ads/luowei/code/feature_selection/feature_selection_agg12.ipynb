{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T05:39:19.716490Z",
     "start_time": "2018-09-15T05:39:19.378982Z"
    }
   },
   "outputs": [],
   "source": [
    "__file__=''\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../LIB/'))\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'../../../../automl/automl_libs/'))\n",
    "from env import FILE\n",
    "import utils\n",
    "from itertools import combinations\n",
    "from feature_engineering import *\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T05:39:22.050619Z",
     "start_time": "2018-09-15T05:39:20.814951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: (1001650, 188)\n",
      "test shape is: (40024, 187)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_pickle(FILE.train_agg_v1_select.value)\n",
    "# train = pd.read_pickle(FILE.train_base_line.value)\n",
    "print('train shape is: {}'.format(train.shape))\n",
    "test = pd.read_pickle(FILE.test_agg_v1_select.value)\n",
    "# test = pd.read_pickle(FILE.test_base_line.value)\n",
    "print('test shape is: {}'.format(test.shape))\n",
    "\n",
    "# train = train1.iloc[:,:200].copy()\n",
    "# train['click'] = train1['click'].copy()\n",
    "# test = test1.iloc[:,:200].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T05:39:24.754691Z",
     "start_time": "2018-09-15T05:39:24.564613Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print_to_file = False \n",
    "test_run = False \n",
    "\n",
    "def get_time(timezone='America/New_York', time_format='%Y-%m-%d %H:%M:%S'):\n",
    "    from datetime import datetime\n",
    "    from dateutil import tz\n",
    "\n",
    "    # METHOD 1: Hardcode zones:\n",
    "    from_zone = tz.gettz('UTC')\n",
    "    to_zone = tz.gettz(timezone)\n",
    "\n",
    "    utc = datetime.utcnow()\n",
    "\n",
    "    # Tell the datetime object that it's in UTC time zone since \n",
    "    # datetime objects are 'naive' by default\n",
    "    utc = utc.replace(tzinfo=from_zone)\n",
    "\n",
    "    # Convert time zone\n",
    "    est = utc.astimezone(to_zone)\n",
    "\n",
    "    return est.strftime(time_format)\n",
    "\n",
    "import sys, time\n",
    "class Logger(object):\n",
    "    def __init__(self, logtofile=True, logfilename='log'):\n",
    "        self.terminal = sys.stdout\n",
    "        self.logfile = \"{}_{}.log\".format(logfilename, int(time.time()))\n",
    "        self.logtofile = logtofile\n",
    "\n",
    "    def write(self, message):\n",
    "        #         self.terminal.write(message)\n",
    "        if self.logtofile:\n",
    "            self.log = open(self.logfile, \"a\")\n",
    "            self.log.write('[' + get_time() + '] ' + message)\n",
    "            self.log.close()\n",
    "\n",
    "    def flush(self):\n",
    "        # this flush method is needed for python 3 compatibility.\n",
    "        # this handles the flush command by doing nothing.\n",
    "        # you might want to specify some extra behavior here.\n",
    "        pass\n",
    "\n",
    "\n",
    "def divert_printout_to_file():\n",
    "    sys.stdout = Logger(logfilename='logfile')\n",
    "\n",
    "if print_to_file:\n",
    "    divert_printout_to_file()  # note: comment this to use pdb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "def kfold_lightgbm(df, train_df, test_df, holdout, num_folds, submission_file_name, fe_img_name, stratified = False, debug= False, colsample=0.67, max_depth=8, num_leaves=31, min_child_samples=20, subsample=0.7, reg_lambda=0.3, lr=0.04, seed=1001, verbose=100, rounds=None, target='click'):\n",
    "    print(train_df.shape, test_df.shape, holdout.shape)\n",
    "    print('MEAN: train({}) vs holdout({}): '.format(len(train_df), len(holdout)), train_df[target].mean(), holdout[target].mean())\n",
    "    # Divide in training/validation and test data\n",
    "    if df is not None:\n",
    "        train_df = df[df[target].notnull()]\n",
    "        test_df = df[df[target].isnull()]\n",
    "        print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "        del df\n",
    "        gc.collect()\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=seed)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=seed)\n",
    "        \n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    holdout_final_preds = np.zeros(holdout.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feature_importance_gain_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in [target,'time','instance_id','index']]\n",
    "    train_scores = []\n",
    "    holdout_scores = []\n",
    "    scores = []\n",
    "    diff_val_holdout = []\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df[target])):\n",
    "#         print('valid index : ',list(valid_idx)[:5])\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df[target].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df[target].iloc[valid_idx]\n",
    "#         print('MEAN: train({}) vs valid({}): '.format(len(train_y), len(valid_y)), np.mean(train_y), np.mean(valid_y))\n",
    "\n",
    "        clf = LGBMClassifier(\n",
    "            nthread=10,\n",
    "            n_estimators=30000,\n",
    "            learning_rate=lr,\n",
    "#             num_leaves=num_leaves,\n",
    "#             colsample_bytree=colsample, # 0.67\n",
    "#             subsample=subsample,\n",
    "#             subsample_freq=0, ## disable subsampling\n",
    "#             max_depth=max_depth,\n",
    "#             reg_alpha=0.65,\n",
    "#             reg_lambda=reg_lambda,\n",
    "#             min_split_gain=0.0222415,\n",
    "#             min_child_weight=39.3259775,\n",
    "#             min_child_samples=min_child_samples,\n",
    "            silent=-1,\n",
    "            verbose=-1, )\n",
    "        if rounds is not None:\n",
    "            clf.n_estimators = rounds\n",
    "            clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "                eval_metric= 'logloss', verbose=verbose)\n",
    "            oof_preds[valid_idx] = clf.predict_proba(valid_x)[:, 1]\n",
    "            sub_preds += clf.predict_proba(test_df[feats])[:, 1] / folds.n_splits\n",
    "            holdout_preds = clf.predict_proba(holdout[feats])[:, 1] \n",
    "        else:\n",
    "            clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "                eval_metric= 'logloss', verbose=verbose, early_stopping_rounds= 100)\n",
    "            oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "            sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "            holdout_preds = clf.predict_proba(holdout[feats], num_iteration=clf.best_iteration_)[:, 1] \n",
    "            \n",
    "        holdout_final_preds += holdout_preds / folds.n_splits\n",
    "        score = log_loss(valid_y, oof_preds[valid_idx])\n",
    "        train_score = clf.best_score_['training']['binary_logloss']\n",
    "        holdout_score = log_loss(holdout[target], holdout_preds)\n",
    "        diff = abs(score - holdout_score)\n",
    "        best_rounds = rounds if rounds is not None else clf.best_iteration_\n",
    "        print('Fold %2d [%5d] AUC : ho: %.6f / te: %.6f / tr: %.6f (diff: %.6f)' % (n_fold + 1, best_rounds, holdout_score, score,  train_score, diff))\n",
    "        scores.append(score)\n",
    "        train_scores.append(train_score)\n",
    "        holdout_scores.append(holdout_score)\n",
    "        diff_val_holdout.append(diff)\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        \n",
    "        fold_importance_gain_df = pd.DataFrame()\n",
    "        fold_importance_gain_df[\"feature\"] = feats\n",
    "        fold_importance_gain_df[\"importance\"] = clf.booster_.feature_importance(importance_type='gain')\n",
    "        fold_importance_gain_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_gain_df = pd.concat([feature_importance_gain_df, fold_importance_gain_df], axis=0)\n",
    "        \n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "    holdout_roc = log_loss(holdout[target], holdout_final_preds)\n",
    "    holdout_mean = np.mean(holdout_scores)\n",
    "    full_te_mean = np.mean(scores)\n",
    "    full_tr_mean = np.mean(train_scores)\n",
    "#     print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "    print('Full HO score %.6f' % holdout_roc)\n",
    "    print('FULL HO mean {:.6f}, std {:.6f}'.format(holdout_mean, np.std(holdout_scores)))\n",
    "    print('FULL TE mean {:.6f}, std {:.6f}'.format(full_te_mean, np.std(scores)))\n",
    "    print('FULL TR mean {:.6f}, std {:.6f}'.format(full_tr_mean, np.std(train_scores)))\n",
    "    print('FULL DIFF mean {:.6f}, std {:.6f}'.format(np.mean(diff_val_holdout), np.std(diff_val_holdout)))\n",
    "    # Write submission file and plot feature importance\n",
    "    if not debug:\n",
    "        print('saving...')\n",
    "        test_df['predicted_score'] = sub_preds\n",
    "        test_df[['instance_id', 'predicted_score']].to_csv(submission_file_name, index= False)\n",
    "#     if not print_to_file:\n",
    "#         display_importances(feature_importance_df, fe_img_name)\n",
    "    feature_importance_df = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "    feature_importance_gain_df = feature_importance_gain_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "    return feature_importance_df, feature_importance_gain_df,holdout_roc,holdout_mean,full_te_mean,full_tr_mean,oof_preds\n",
    "\n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_, fe_img_name):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fe_img_name+'.png')\n",
    "\n",
    "\n",
    "def convert_and_save_imp_df(fe_imp_df, dumpfilename):\n",
    "    fe_imp_df_mean = fe_imp_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "    pickle.dump(fe_imp_df_mean, open(dumpfilename,'wb'))\n",
    "    \n",
    "    \n",
    "# seed = 6\n",
    "# seed = 41\n",
    "def select_holdout(train,test,shift=1,seed=41):\n",
    "    samples_size = int(len(test) )\n",
    "    assert shift in range(1,8)\n",
    "    test_time_min = test.time.min()\n",
    "    test_time_max = test.time.max()\n",
    "    train_time_min = train.time.min()\n",
    "    train_time_max = train.time.max()\n",
    "    window_lower_bound = test_time_min - shift*60*60*24\n",
    "    window_upper_bound = test_time_max - shift*60*60*24\n",
    "    available_train = train[(train.time<=window_upper_bound) & (train.time>=window_lower_bound)]\n",
    "    holdout = available_train.sample(n=samples_size,random_state=seed).copy()\n",
    "    train_split_index = list(set(train.index) - set(holdout.index))\n",
    "    train_split = train.iloc[train_split_index].copy()\n",
    "    return train_split,holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T05:39:32.137755Z",
     "start_time": "2018-09-15T05:39:32.127277Z"
    }
   },
   "outputs": [],
   "source": [
    "def runlgb(train, test, holdout):\n",
    "    colsamples = [0.07]#[0.1,0.15,0.2]#[0.03,0.04,0.05,0.06,0.07,0.08]\n",
    "    seeds = [20]#[300,4000,50000,600000,7000000,80000000,523445,31275479] # 20\n",
    "    depth = [5]\n",
    "    leaves = [16]\n",
    "    min_child_sam = [20]#, 800]\n",
    "    subsamples = [1]#0.8, 0.7, 0.6, 0.5, 0.4] # was 1\n",
    "    reg_lambdas = [0.5]\n",
    "    # lrs = lrs.tolist()\n",
    "    lrs2 = [0.05]\n",
    "    nfolds = 5\n",
    "    rounds = [None] #[1000]#, 1300, 1600, 1900, 2200, 2500]\n",
    "    for seed in seeds:\n",
    "        for colsample in colsamples:\n",
    "            for d in depth:\n",
    "                for l in leaves:\n",
    "                    for mcs in min_child_sam:\n",
    "                        for subsample in subsamples:\n",
    "                            for reg_lambda in reg_lambdas:\n",
    "                                for lr in lrs2:\n",
    "                                    for r in rounds:\n",
    "                                        filename = 'fe_936_col{}_lr{}_n{}'.format(len(train.columns), lr, nfolds)\n",
    "                                        print('#############################################')\n",
    "                                        print(colsample, seed, d, l, mcs, subsample, reg_lambda, lr, 'nfolds:', nfolds)\n",
    "                                        print('#############################################')\n",
    "                                        numfeats = len(train.columns)\n",
    "                                        with timer(\"Run LightGBM with kfold\"):\n",
    "                                            return kfold_lightgbm(None, train, test, holdout, nfolds, filename+'.csv', filename, colsample=colsample, verbose=100, max_depth=d, num_leaves=l, min_child_samples=mcs, subsample=subsample, reg_lambda=reg_lambda, lr=lr, seed=seed, stratified=True, rounds=r,debug=False)                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T05:41:48.983231Z",
     "start_time": "2018-09-15T05:39:47.371300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (961626, 188), holdout shape: (40024, 188)\n",
      "train mean: 0.19845954175610242, holdout mean: 0.21007395562662404\n",
      "#############################################\n",
      "0.07 20 5 16 20 1 0.5 0.05 nfolds: 5\n",
      "#############################################\n",
      "(961626, 188) (40024, 187) (40024, 188)\n",
      "MEAN: train(961626) vs holdout(40024):  0.19797613625255556 0.21007395562662404\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409141\tvalid_1's binary_logloss: 0.418117\n",
      "[200]\ttraining's binary_logloss: 0.402071\tvalid_1's binary_logloss: 0.418283\n",
      "Early stopping, best iteration is:\n",
      "[122]\ttraining's binary_logloss: 0.40732\tvalid_1's binary_logloss: 0.418014\n",
      "Fold  1 [  122] AUC : ho: 0.428276 / te: 0.418014 / tr: 0.407320 (diff: 0.010262)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409393\tvalid_1's binary_logloss: 0.416878\n",
      "[200]\ttraining's binary_logloss: 0.402528\tvalid_1's binary_logloss: 0.417063\n",
      "Early stopping, best iteration is:\n",
      "[115]\ttraining's binary_logloss: 0.408137\tvalid_1's binary_logloss: 0.41685\n",
      "Fold  2 [  115] AUC : ho: 0.427660 / te: 0.416850 / tr: 0.408137 (diff: 0.010810)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409277\tvalid_1's binary_logloss: 0.417398\n",
      "[200]\ttraining's binary_logloss: 0.402182\tvalid_1's binary_logloss: 0.417581\n",
      "Early stopping, best iteration is:\n",
      "[124]\ttraining's binary_logloss: 0.407153\tvalid_1's binary_logloss: 0.417299\n",
      "Fold  3 [  124] AUC : ho: 0.428007 / te: 0.417299 / tr: 0.407153 (diff: 0.010708)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409089\tvalid_1's binary_logloss: 0.418337\n",
      "[200]\ttraining's binary_logloss: 0.402\tvalid_1's binary_logloss: 0.418615\n",
      "Early stopping, best iteration is:\n",
      "[126]\ttraining's binary_logloss: 0.406868\tvalid_1's binary_logloss: 0.418297\n",
      "Fold  4 [  126] AUC : ho: 0.428102 / te: 0.418297 / tr: 0.406868 (diff: 0.009804)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409236\tvalid_1's binary_logloss: 0.417106\n",
      "[200]\ttraining's binary_logloss: 0.402206\tvalid_1's binary_logloss: 0.417381\n",
      "Early stopping, best iteration is:\n",
      "[111]\ttraining's binary_logloss: 0.408267\tvalid_1's binary_logloss: 0.417084\n",
      "Fold  5 [  111] AUC : ho: 0.428322 / te: 0.417084 / tr: 0.408267 (diff: 0.011237)\n",
      "Full HO score 0.427510\n",
      "FULL HO mean 0.428073, std 0.000236\n",
      "FULL TE mean 0.417509, std 0.000554\n",
      "FULL TR mean 0.407549, std 0.000554\n",
      "FULL DIFF mean 0.010564, std 0.000491\n",
      "saving...\n",
      "Run LightGBM with kfold - done in 121s\n"
     ]
    }
   ],
   "source": [
    "train_df,holdout = select_holdout(train,test,shift=1,seed=41)\n",
    "print('train shape: {}, holdout shape: {}'.format(train_df.shape,holdout.shape))\n",
    "print('train mean: {}, holdout mean: {}'.format(train.click.mean(),holdout.click.mean()))\n",
    "\n",
    "\n",
    "feature_importance_df, feature_importance_gain_df,holdout_roc,holdout_mean,full_te_mean,full_tr_mean,oof_preds = runlgb(train_df, test, holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T05:41:55.155756Z",
     "start_time": "2018-09-15T05:41:55.150890Z"
    }
   },
   "outputs": [],
   "source": [
    "base_gain = feature_importance_gain_df.copy()\n",
    "base_split = feature_importance_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T04:42:52.393719Z",
     "start_time": "2018-09-15T04:42:52.375288Z"
    }
   },
   "outputs": [],
   "source": [
    "base_split.to_csv('../../data/features/agg_v1_select_feature_importance_split.csv',index=False)\n",
    "base_gain.to_csv('../../data/features/agg_v1_select_feature_importance_gain.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T04:28:19.694069Z",
     "start_time": "2018-09-15T04:28:19.581905Z"
    }
   },
   "outputs": [],
   "source": [
    "train = train_new.copy()\n",
    "test = test_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T05:42:04.658372Z",
     "start_time": "2018-09-15T05:42:04.651488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185\n"
     ]
    }
   ],
   "source": [
    "gain_fe = base_gain.iloc[:200]['feature'].values\n",
    "split_fe = base_split.iloc[:200]['feature'].values\n",
    "all_fe = set(gain_fe).intersection(set(split_fe))\n",
    "print(len(all_fe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T05:42:10.661305Z",
     "start_time": "2018-09-15T05:42:10.162701Z"
    }
   },
   "outputs": [],
   "source": [
    "all_fe = list(all_fe)\n",
    "all_fe.append('instance_id')\n",
    "all_fe.append('time')\n",
    "train_fe = list(all_fe).copy()\n",
    "train_fe.append('click')\n",
    "\n",
    "train_new = train[train_fe].copy()\n",
    "test_new = test[all_fe].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-15T05:42:15.817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (961626, 188), holdout shape: (40024, 188)\n",
      "train mean: 0.19845954175610242, holdout mean: 0.21007395562662404\n",
      "#############################################\n",
      "0.07 20 5 16 20 1 0.5 0.05 nfolds: 5\n",
      "#############################################\n",
      "(961626, 188) (40024, 187) (40024, 188)\n",
      "MEAN: train(961626) vs holdout(40024):  0.19797613625255556 0.21007395562662404\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409144\tvalid_1's binary_logloss: 0.41813\n",
      "[200]\ttraining's binary_logloss: 0.402073\tvalid_1's binary_logloss: 0.418299\n",
      "Early stopping, best iteration is:\n",
      "[115]\ttraining's binary_logloss: 0.407871\tvalid_1's binary_logloss: 0.418035\n",
      "Fold  1 [  115] AUC : ho: 0.428221 / te: 0.418035 / tr: 0.407871 (diff: 0.010185)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409393\tvalid_1's binary_logloss: 0.416878\n",
      "[200]\ttraining's binary_logloss: 0.402528\tvalid_1's binary_logloss: 0.417063\n",
      "Early stopping, best iteration is:\n",
      "[115]\ttraining's binary_logloss: 0.408137\tvalid_1's binary_logloss: 0.41685\n",
      "Fold  2 [  115] AUC : ho: 0.427660 / te: 0.416850 / tr: 0.408137 (diff: 0.010810)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409203\tvalid_1's binary_logloss: 0.417337\n",
      "[200]\ttraining's binary_logloss: 0.402171\tvalid_1's binary_logloss: 0.417535\n",
      "Early stopping, best iteration is:\n",
      "[122]\ttraining's binary_logloss: 0.407304\tvalid_1's binary_logloss: 0.417277\n",
      "Fold  3 [  122] AUC : ho: 0.427952 / te: 0.417277 / tr: 0.407304 (diff: 0.010675)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409089\tvalid_1's binary_logloss: 0.418337\n",
      "[200]\ttraining's binary_logloss: 0.402\tvalid_1's binary_logloss: 0.418615\n"
     ]
    }
   ],
   "source": [
    "train_df,holdout = select_holdout(train_new,test,shift=1,seed=41)\n",
    "print('train shape: {}, holdout shape: {}'.format(train_df.shape,holdout.shape))\n",
    "print('train mean: {}, holdout mean: {}'.format(train.click.mean(),holdout.click.mean()))\n",
    "feature_importance_df, feature_importance_gain_df,holdout_roc,holdout_mean,full_te_mean,full_tr_mean,oof_preds = runlgb(train_df, test_new, holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T04:58:14.729137Z",
     "start_time": "2018-09-15T04:57:24.684899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (961626, 62), holdout shape: (40024, 62)\n",
      "train mean: 0.19845954175610242, holdout mean: 0.21007395562662404\n",
      "#############################################\n",
      "0.07 20 5 16 20 1 0.5 0.05 nfolds: 5\n",
      "#############################################\n",
      "(961626, 62) (40024, 61) (40024, 62)\n",
      "MEAN: train(961626) vs holdout(40024):  0.19797613625255556 0.21007395562662404\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409218\tvalid_1's binary_logloss: 0.418057\n",
      "[200]\ttraining's binary_logloss: 0.402489\tvalid_1's binary_logloss: 0.418387\n",
      "Early stopping, best iteration is:\n",
      "[115]\ttraining's binary_logloss: 0.40804\tvalid_1's binary_logloss: 0.418027\n",
      "Fold  1 [  115] AUC : ho: 0.428232 / te: 0.418027 / tr: 0.408040 (diff: 0.010205)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409469\tvalid_1's binary_logloss: 0.416872\n",
      "[200]\ttraining's binary_logloss: 0.40302\tvalid_1's binary_logloss: 0.417178\n",
      "Early stopping, best iteration is:\n",
      "[107]\ttraining's binary_logloss: 0.408842\tvalid_1's binary_logloss: 0.416842\n",
      "Fold  2 [  107] AUC : ho: 0.427819 / te: 0.416842 / tr: 0.408842 (diff: 0.010977)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409364\tvalid_1's binary_logloss: 0.417295\n",
      "[200]\ttraining's binary_logloss: 0.40261\tvalid_1's binary_logloss: 0.417665\n",
      "Early stopping, best iteration is:\n",
      "[119]\ttraining's binary_logloss: 0.407736\tvalid_1's binary_logloss: 0.417252\n",
      "Fold  3 [  119] AUC : ho: 0.428060 / te: 0.417252 / tr: 0.407736 (diff: 0.010808)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409244\tvalid_1's binary_logloss: 0.418358\n",
      "Early stopping, best iteration is:\n",
      "[91]\ttraining's binary_logloss: 0.409982\tvalid_1's binary_logloss: 0.418344\n",
      "Fold  4 [   91] AUC : ho: 0.428209 / te: 0.418344 / tr: 0.409982 (diff: 0.009865)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409382\tvalid_1's binary_logloss: 0.417076\n",
      "[200]\ttraining's binary_logloss: 0.402577\tvalid_1's binary_logloss: 0.417489\n",
      "Early stopping, best iteration is:\n",
      "[101]\ttraining's binary_logloss: 0.409304\tvalid_1's binary_logloss: 0.417074\n",
      "Fold  5 [  101] AUC : ho: 0.428432 / te: 0.417074 / tr: 0.409304 (diff: 0.011357)\n",
      "Full HO score 0.427626\n",
      "FULL HO mean 0.428150, std 0.000204\n",
      "FULL TE mean 0.417508, std 0.000577\n",
      "FULL TR mean 0.408781, std 0.000820\n",
      "FULL DIFF mean 0.010642, std 0.000538\n",
      "Run LightGBM with kfold - done in 50s\n"
     ]
    }
   ],
   "source": [
    "train_df,holdout = select_holdout(train_new,test,shift=1,seed=41)\n",
    "print('train shape: {}, holdout shape: {}'.format(train_df.shape,holdout.shape))\n",
    "print('train mean: {}, holdout mean: {}'.format(train.click.mean(),holdout.click.mean()))\n",
    "feature_importance_df, feature_importance_gain_df,holdout_roc,holdout_mean,full_te_mean,full_tr_mean,oof_preds = runlgb(train_df, test_new, holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T04:59:42.476391Z",
     "start_time": "2018-09-15T04:59:01.295179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (961626, 45), holdout shape: (40024, 45)\n",
      "train mean: 0.19845954175610242, holdout mean: 0.21007395562662404\n",
      "#############################################\n",
      "0.07 20 5 16 20 1 0.5 0.05 nfolds: 5\n",
      "#############################################\n",
      "(961626, 45) (40024, 44) (40024, 45)\n",
      "MEAN: train(961626) vs holdout(40024):  0.19797613625255556 0.21007395562662404\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409279\tvalid_1's binary_logloss: 0.41808\n",
      "[200]\ttraining's binary_logloss: 0.402835\tvalid_1's binary_logloss: 0.418483\n",
      "Early stopping, best iteration is:\n",
      "[110]\ttraining's binary_logloss: 0.408406\tvalid_1's binary_logloss: 0.418039\n",
      "Fold  1 [  110] AUC : ho: 0.428234 / te: 0.418039 / tr: 0.408406 (diff: 0.010195)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409534\tvalid_1's binary_logloss: 0.416897\n",
      "[200]\ttraining's binary_logloss: 0.40328\tvalid_1's binary_logloss: 0.417231\n",
      "Early stopping, best iteration is:\n",
      "[111]\ttraining's binary_logloss: 0.408625\tvalid_1's binary_logloss: 0.416859\n",
      "Fold  2 [  111] AUC : ho: 0.427676 / te: 0.416859 / tr: 0.408625 (diff: 0.010817)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409429\tvalid_1's binary_logloss: 0.417429\n",
      "[200]\ttraining's binary_logloss: 0.402979\tvalid_1's binary_logloss: 0.417838\n",
      "Early stopping, best iteration is:\n",
      "[109]\ttraining's binary_logloss: 0.408637\tvalid_1's binary_logloss: 0.417396\n",
      "Fold  3 [  109] AUC : ho: 0.428137 / te: 0.417396 / tr: 0.408637 (diff: 0.010742)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409306\tvalid_1's binary_logloss: 0.418437\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's binary_logloss: 0.410002\tvalid_1's binary_logloss: 0.41841\n",
      "Fold  4 [   92] AUC : ho: 0.428172 / te: 0.418410 / tr: 0.410002 (diff: 0.009762)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.409493\tvalid_1's binary_logloss: 0.417165\n",
      "[200]\ttraining's binary_logloss: 0.403127\tvalid_1's binary_logloss: 0.417586\n",
      "Early stopping, best iteration is:\n",
      "[105]\ttraining's binary_logloss: 0.409054\tvalid_1's binary_logloss: 0.417141\n",
      "Fold  5 [  105] AUC : ho: 0.428192 / te: 0.417141 / tr: 0.409054 (diff: 0.011051)\n",
      "Full HO score 0.427557\n",
      "FULL HO mean 0.428082, std 0.000206\n",
      "FULL TE mean 0.417569, std 0.000574\n",
      "FULL TR mean 0.408945, std 0.000569\n",
      "FULL DIFF mean 0.010513, std 0.000469\n",
      "Run LightGBM with kfold - done in 41s\n"
     ]
    }
   ],
   "source": [
    "train_df,holdout = select_holdout(train_new,test,shift=1,seed=41)\n",
    "print('train shape: {}, holdout shape: {}'.format(train_df.shape,holdout.shape))\n",
    "print('train mean: {}, holdout mean: {}'.format(train.click.mean(),holdout.click.mean()))\n",
    "feature_importance_df, feature_importance_gain_df,holdout_roc,holdout_mean,full_te_mean,full_tr_mean,oof_preds = runlgb(train_df, test_new, holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T04:42:22.488612Z",
     "start_time": "2018-09-15T04:42:19.605086Z"
    }
   },
   "outputs": [],
   "source": [
    "train_new.to_pickle(FILE.train_agg_v1_select.value)\n",
    "test_new.to_pickle(FILE.test_agg_v1_select.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_col = ['make+creative_id_reverse_cumulative_count','nnt+orderid_count_in_next_n_time_unit_1hour','make+creative_id_reverse_cumulative_count','campaign_id+make_reverse_cumulative_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T05:05:15.070010Z",
     "start_time": "2018-09-15T05:05:15.053262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>creative_type</td>\n",
       "      <td>819928.372135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inner_slot_id</td>\n",
       "      <td>253888.971476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model</td>\n",
       "      <td>161398.841167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>advert_name</td>\n",
       "      <td>64511.492930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ad_i_i_lvl1</td>\n",
       "      <td>16544.012985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>osv</td>\n",
       "      <td>15284.677523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>user_tags</td>\n",
       "      <td>13473.810912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>make</td>\n",
       "      <td>12001.070224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>creative_id</td>\n",
       "      <td>7934.480057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>f_channel</td>\n",
       "      <td>4022.870403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>advert_industry_inner</td>\n",
       "      <td>3625.364047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>creative_area</td>\n",
       "      <td>2749.314780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>app_id</td>\n",
       "      <td>2262.064267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>campaign_id</td>\n",
       "      <td>2012.387422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>make+creative_id_reverse_cumulative_count</td>\n",
       "      <td>1799.757441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>orderid</td>\n",
       "      <td>1763.267802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>adid</td>\n",
       "      <td>1670.967839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>nnt+orderid_count_in_next_n_time_unit_1hour</td>\n",
       "      <td>1152.345722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>campaign_id+make_reverse_cumulative_count</td>\n",
       "      <td>887.208826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>user_tags+creative_id_cumulative_count</td>\n",
       "      <td>657.371083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>os_name+adid_count_in_next_n_time_unit_1hour</td>\n",
       "      <td>545.378218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>nnt+app_cate_id_cumulative_count</td>\n",
       "      <td>538.296862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>city</td>\n",
       "      <td>518.601160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>nnt+creative_tp_dnf_count_in_next_n_time_unit_...</td>\n",
       "      <td>511.660498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>creative_has_deeplink+make_reverse_cumulative_...</td>\n",
       "      <td>487.229081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>os_name+creative_id_reverse_cumulative_count</td>\n",
       "      <td>445.317138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>nnt+creative_tp_dnf_count_in_next_n_time_unit_...</td>\n",
       "      <td>444.564580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>osv+model_time_to_n_next</td>\n",
       "      <td>428.855001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>f_channel+orderid_count_in_next_n_time_unit_1hour</td>\n",
       "      <td>410.319837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>app_cate_id+time_hour_cumulative_count</td>\n",
       "      <td>379.924937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>osv+model_time_to_n_previous</td>\n",
       "      <td>377.550903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>make+creative_id_cumulative_count</td>\n",
       "      <td>376.830458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>creative_is_download+creative_id_reverse_cumul...</td>\n",
       "      <td>370.061481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>nnt+orderid_count_in_next_n_time_unit_6hour</td>\n",
       "      <td>364.004117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>adid+f_channel_count_in_next_n_time_unit_6hour</td>\n",
       "      <td>362.504400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>app_cate_id+creative_type_reverse_cumulative_c...</td>\n",
       "      <td>348.459820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>campaign_id+app_cate_id_count_in_next_n_time_u...</td>\n",
       "      <td>339.701083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>f_channel+city_count_in_next_n_time_unit_1hour</td>\n",
       "      <td>311.506519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>app_cate_id+make_cumulative_count</td>\n",
       "      <td>302.634281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>creative_id+city_count</td>\n",
       "      <td>242.888341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>nnt+province_count_in_next_n_time_unit_6hour</td>\n",
       "      <td>172.856241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>f_channel+creative_id_reverse_cumulative_count</td>\n",
       "      <td>118.674539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              feature     importance\n",
       "0                                       creative_type  819928.372135\n",
       "1                                       inner_slot_id  253888.971476\n",
       "2                                               model  161398.841167\n",
       "3                                         advert_name   64511.492930\n",
       "4                                         ad_i_i_lvl1   16544.012985\n",
       "5                                                 osv   15284.677523\n",
       "6                                           user_tags   13473.810912\n",
       "7                                                make   12001.070224\n",
       "8                                         creative_id    7934.480057\n",
       "9                                           f_channel    4022.870403\n",
       "10                              advert_industry_inner    3625.364047\n",
       "11                                      creative_area    2749.314780\n",
       "12                                             app_id    2262.064267\n",
       "13                                        campaign_id    2012.387422\n",
       "14          make+creative_id_reverse_cumulative_count    1799.757441\n",
       "15                                            orderid    1763.267802\n",
       "16                                               adid    1670.967839\n",
       "17        nnt+orderid_count_in_next_n_time_unit_1hour    1152.345722\n",
       "18          campaign_id+make_reverse_cumulative_count     887.208826\n",
       "19             user_tags+creative_id_cumulative_count     657.371083\n",
       "20       os_name+adid_count_in_next_n_time_unit_1hour     545.378218\n",
       "21                   nnt+app_cate_id_cumulative_count     538.296862\n",
       "22                                               city     518.601160\n",
       "23  nnt+creative_tp_dnf_count_in_next_n_time_unit_...     511.660498\n",
       "24  creative_has_deeplink+make_reverse_cumulative_...     487.229081\n",
       "25       os_name+creative_id_reverse_cumulative_count     445.317138\n",
       "26  nnt+creative_tp_dnf_count_in_next_n_time_unit_...     444.564580\n",
       "27                           osv+model_time_to_n_next     428.855001\n",
       "28  f_channel+orderid_count_in_next_n_time_unit_1hour     410.319837\n",
       "29             app_cate_id+time_hour_cumulative_count     379.924937\n",
       "30                       osv+model_time_to_n_previous     377.550903\n",
       "31                  make+creative_id_cumulative_count     376.830458\n",
       "32  creative_is_download+creative_id_reverse_cumul...     370.061481\n",
       "33        nnt+orderid_count_in_next_n_time_unit_6hour     364.004117\n",
       "34     adid+f_channel_count_in_next_n_time_unit_6hour     362.504400\n",
       "35  app_cate_id+creative_type_reverse_cumulative_c...     348.459820\n",
       "36  campaign_id+app_cate_id_count_in_next_n_time_u...     339.701083\n",
       "37     f_channel+city_count_in_next_n_time_unit_1hour     311.506519\n",
       "38                  app_cate_id+make_cumulative_count     302.634281\n",
       "39                             creative_id+city_count     242.888341\n",
       "40       nnt+province_count_in_next_n_time_unit_6hour     172.856241\n",
       "41     f_channel+creative_id_reverse_cumulative_count     118.674539"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_gain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
