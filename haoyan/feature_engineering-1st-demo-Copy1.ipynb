{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = \"/home/kai/data/kaggle/talkingdata/data/\"\n",
    "# train = pd.read_csv(PATH + 'train_cleaned_final.csv')\n",
    "train = pd.read_feather(PATH + 'train_cleaned_final.ftr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.iloc[:500000]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orders = {}\n",
    "feature_col = ['ip', \n",
    "              'app', \n",
    "              'device', \n",
    "              'os', \n",
    "              'channel',\n",
    "              'day',\n",
    "              'hour',]\n",
    "\n",
    "# feature_col = ['ip', \n",
    "#               'app', \n",
    "#               'device', \n",
    "#               'os', \n",
    "#               'channel']\n",
    "for col in feature_col:\n",
    "    orders[col] = 10 ** (int(np.log(train[col].max() + 1) / np.log(10)) + 1)\n",
    "def get_group(df, cols):\n",
    "    \"\"\"\n",
    "    define an encoding method which can ganrantee the adding value will be unique.\n",
    "    eg: artist_name_composer will be a combination of (artist_name,composer) and the encoding will reflect the unqiue combination of those two\n",
    "    \"\"\"\n",
    "    group = df[cols[0]].copy()\n",
    "    for col in cols[1:]:\n",
    "        group = group * orders[col] + df[col]\n",
    "        \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'app': 1000,\n",
       " 'channel': 1000,\n",
       " 'day': 10,\n",
       " 'device': 10000,\n",
       " 'hour': 100,\n",
       " 'ip': 1000000,\n",
       " 'os': 1000}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA LSA NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "Next, we tried categorical feature embedding by using LDA/NMF/LSA. Here is the pseudo code to compute LDA topics of IPs related to app.\n",
    "We computed this feature for all the 20 (=5*(5-1)) combinations of 5 raw features and set the topic size to 5. This ended up with 100 new features. We also computed similar features using NMF and PCA, in total 300 new features. 0.9821 with a single LGB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _lda_nmf_lsa_from_sklearn(key_col_series, value_col_series, n_components):\n",
    "    dictionary = {}\n",
    "    key_list = []\n",
    "    for i in range(key_col_series.shape[0]):\n",
    "        dictionary.setdefault(key_col_series.iloc[i], []).append(str(value_col_series.iloc[i]))\n",
    "    key_list = list(dictionary.keys())\n",
    "    sentences = [' '.join(dictionary[key]) for key in key_list]\n",
    "    matrix = CountVectorizer().fit_transform(sentences)\n",
    "    \n",
    "    lda_dict = dict(zip(key_list, LatentDirichletAllocation(n_components=n_components, n_jobs=16).fit_transform(matrix)))\n",
    "    nmf_dict = dict(zip(key_list, NMF(n_components=n_components).fit_transform(matrix)))\n",
    "    lsa_dict = dict(zip(key_list, TruncatedSVD(n_components=n_components).fit_transform(matrix)))\n",
    "    \n",
    "    return key_col_series.map(lda_dict), key_col_series.map(nmf_dict), key_col_series.map(lsa_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: get key col and value col\n",
    "the key will be categorized according to values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     83230\n",
      "1     17357\n",
      "2     35810\n",
      "3     45745\n",
      "4    161007\n",
      "Name: ip, dtype: int64\n",
      "---\n",
      "0     3\n",
      "1     3\n",
      "2     3\n",
      "3    14\n",
      "4     3\n",
      "Name: app, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "key_col_series = train.ip\n",
    "value_col_series = train.app\n",
    "dictionary = {}\n",
    "key_list = []\n",
    "print(key_col_series.iloc[:5])\n",
    "print('---')\n",
    "print(value_col_series.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: construct dictionary,\n",
    "    eg {'ip1':['app1','app2','app4','app1']}\n",
    "slow ... need improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(key_col_series.shape[0]):\n",
    "    dictionary.setdefault(key_col_series.iloc[i], []).append(str(value_col_series.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '18', '15', '12', '9', '12', '9', '3']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[91749]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: construct sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31395\n",
      "31395\n"
     ]
    }
   ],
   "source": [
    "key_list = list(dictionary.keys())\n",
    "sentences = [' '.join(dictionary[key]) for key in key_list]\n",
    "print(len(sentences))\n",
    "print(len(train.ip.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[83230, 17357, 35810]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3 2 12 9 2 15 6 2 3 3 9 25 25 2 9 3 15 2 14 6 13 15 3 1 12 3 26 6 8 8 9 15 8 1 12 14 3 13 10 18 9 2 11 2 3 14 15 18 3 2 12 9 3 2 36 2',\n",
       " '3 3 2 18 12 15 18 12 15 2 9 3 12 15 18 9 12 9 12 9 12 18 15 2 12 15 18 14 2 18 1 9 18 9 15 12 18 9 18 15 12 18 15 12 12 15 18 12 15 18 8 15 9 8 1 8 18 8 9 15 8 8 1 12 3 9 24 20 6 12 3 12 9 12 21 3 12 3 6 3 3 3 25 25 2 9 3 15 12 15 3 2 14 13 1 12',\n",
       " '3 3 2 9 13 12 2 15 11 18 3 233 12 3 8 9 15 1 8 12 8 9 15 8 1 8 8 15 9 8 1 8 9 12 12 9 3 9 15 12 18']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: get CountVectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show original version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10': 0,\n",
       " '11': 1,\n",
       " '12': 2,\n",
       " '13': 3,\n",
       " '14': 4,\n",
       " '15': 5,\n",
       " '18': 6,\n",
       " '20': 7,\n",
       " '21': 8,\n",
       " '233': 9,\n",
       " '24': 10,\n",
       " '25': 11,\n",
       " '26': 12,\n",
       " '36': 13}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_ori = CountVectorizer()\n",
    "exp_ori.fit(sentences[:3])\n",
    "exp_ori.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12',\n",
       " '15',\n",
       " '25',\n",
       " '25',\n",
       " '15',\n",
       " '14',\n",
       " '13',\n",
       " '15',\n",
       " '12',\n",
       " '26',\n",
       " '15',\n",
       " '12',\n",
       " '14',\n",
       " '13',\n",
       " '10',\n",
       " '18',\n",
       " '11',\n",
       " '14',\n",
       " '15',\n",
       " '18',\n",
       " '12',\n",
       " '36']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.findall('(?u)\\\\b\\\\w\\\\w+\\\\b', sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show modified version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 0,\n",
       " '10': 1,\n",
       " '11': 2,\n",
       " '12': 3,\n",
       " '13': 4,\n",
       " '14': 5,\n",
       " '15': 6,\n",
       " '18': 7,\n",
       " '2': 8,\n",
       " '20': 9,\n",
       " '21': 10,\n",
       " '233': 11,\n",
       " '24': 12,\n",
       " '25': 13,\n",
       " '26': 14,\n",
       " '3': 15,\n",
       " '36': 16,\n",
       " '6': 17,\n",
       " '8': 18,\n",
       " '9': 19}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_mod = CountVectorizer(token_pattern='\\\\b\\\\w+\\\\b')\n",
    "exp_mod.fit(sentences[:3])\n",
    "exp_mod.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3',\n",
       " '2',\n",
       " '12',\n",
       " '9',\n",
       " '2',\n",
       " '15',\n",
       " '6',\n",
       " '2',\n",
       " '3',\n",
       " '3',\n",
       " '9',\n",
       " '25',\n",
       " '25',\n",
       " '2',\n",
       " '9',\n",
       " '3',\n",
       " '15',\n",
       " '2',\n",
       " '14',\n",
       " '6',\n",
       " '13',\n",
       " '15',\n",
       " '3',\n",
       " '1',\n",
       " '12',\n",
       " '3',\n",
       " '26',\n",
       " '6',\n",
       " '8',\n",
       " '8',\n",
       " '9',\n",
       " '15',\n",
       " '8',\n",
       " '1',\n",
       " '12',\n",
       " '14',\n",
       " '3',\n",
       " '13',\n",
       " '10',\n",
       " '18',\n",
       " '9',\n",
       " '2',\n",
       " '11',\n",
       " '2',\n",
       " '3',\n",
       " '14',\n",
       " '15',\n",
       " '18',\n",
       " '3',\n",
       " '2',\n",
       " '12',\n",
       " '9',\n",
       " '3',\n",
       " '2',\n",
       " '36',\n",
       " '2']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.findall('(?u)\\\\b\\\\w+\\\\b', sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get CountVectorize Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31395, 182)\n",
      "182\n"
     ]
    }
   ],
   "source": [
    "cvt = CountVectorizer(token_pattern='\\\\b\\\\w+\\\\b')\n",
    "matrix = cvt.fit_transform(sentences)\n",
    "print(matrix.shape)\n",
    "print(len(cvt.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0,\n",
       " '1': 1,\n",
       " '10': 2,\n",
       " '100': 3,\n",
       " '102': 4,\n",
       " '103': 5,\n",
       " '104': 6,\n",
       " '105': 7,\n",
       " '107': 8,\n",
       " '108': 9,\n",
       " '109': 10,\n",
       " '11': 11,\n",
       " '110': 12,\n",
       " '112': 13,\n",
       " '115': 14,\n",
       " '118': 15,\n",
       " '119': 16,\n",
       " '12': 17,\n",
       " '120': 18,\n",
       " '121': 19,\n",
       " '122': 20,\n",
       " '124': 21,\n",
       " '125': 22,\n",
       " '126': 23,\n",
       " '127': 24,\n",
       " '128': 25,\n",
       " '13': 26,\n",
       " '130': 27,\n",
       " '136': 28,\n",
       " '137': 29,\n",
       " '14': 30,\n",
       " '141': 31,\n",
       " '143': 32,\n",
       " '145': 33,\n",
       " '146': 34,\n",
       " '148': 35,\n",
       " '15': 36,\n",
       " '150': 37,\n",
       " '151': 38,\n",
       " '152': 39,\n",
       " '153': 40,\n",
       " '154': 41,\n",
       " '155': 42,\n",
       " '158': 43,\n",
       " '159': 44,\n",
       " '16': 45,\n",
       " '160': 46,\n",
       " '162': 47,\n",
       " '165': 48,\n",
       " '166': 49,\n",
       " '167': 50,\n",
       " '168': 51,\n",
       " '17': 52,\n",
       " '170': 53,\n",
       " '172': 54,\n",
       " '173': 55,\n",
       " '175': 56,\n",
       " '176': 57,\n",
       " '18': 58,\n",
       " '181': 59,\n",
       " '182': 60,\n",
       " '183': 61,\n",
       " '186': 62,\n",
       " '188': 63,\n",
       " '19': 64,\n",
       " '190': 65,\n",
       " '192': 66,\n",
       " '193': 67,\n",
       " '194': 68,\n",
       " '197': 69,\n",
       " '2': 70,\n",
       " '20': 71,\n",
       " '202': 72,\n",
       " '207': 73,\n",
       " '208': 74,\n",
       " '21': 75,\n",
       " '210': 76,\n",
       " '215': 77,\n",
       " '218': 78,\n",
       " '22': 79,\n",
       " '222': 80,\n",
       " '229': 81,\n",
       " '23': 82,\n",
       " '231': 83,\n",
       " '233': 84,\n",
       " '239': 85,\n",
       " '24': 86,\n",
       " '244': 87,\n",
       " '25': 88,\n",
       " '250': 89,\n",
       " '251': 90,\n",
       " '258': 91,\n",
       " '26': 92,\n",
       " '261': 93,\n",
       " '265': 94,\n",
       " '267': 95,\n",
       " '268': 96,\n",
       " '27': 97,\n",
       " '279': 98,\n",
       " '28': 99,\n",
       " '284': 100,\n",
       " '29': 101,\n",
       " '292': 102,\n",
       " '294': 103,\n",
       " '299': 104,\n",
       " '3': 105,\n",
       " '303': 106,\n",
       " '305': 107,\n",
       " '315': 108,\n",
       " '32': 109,\n",
       " '33': 110,\n",
       " '34': 111,\n",
       " '347': 112,\n",
       " '35': 113,\n",
       " '36': 114,\n",
       " '363': 115,\n",
       " '37': 116,\n",
       " '38': 117,\n",
       " '381': 118,\n",
       " '39': 119,\n",
       " '4': 120,\n",
       " '40': 121,\n",
       " '42': 122,\n",
       " '425': 123,\n",
       " '45': 124,\n",
       " '46': 125,\n",
       " '469': 126,\n",
       " '47': 127,\n",
       " '48': 128,\n",
       " '481': 129,\n",
       " '49': 130,\n",
       " '5': 131,\n",
       " '50': 132,\n",
       " '502': 133,\n",
       " '51': 134,\n",
       " '52': 135,\n",
       " '53': 136,\n",
       " '536': 137,\n",
       " '538': 138,\n",
       " '54': 139,\n",
       " '541': 140,\n",
       " '55': 141,\n",
       " '556': 142,\n",
       " '56': 143,\n",
       " '561': 144,\n",
       " '563': 145,\n",
       " '57': 146,\n",
       " '58': 147,\n",
       " '6': 148,\n",
       " '60': 149,\n",
       " '61': 150,\n",
       " '610': 151,\n",
       " '64': 152,\n",
       " '645': 153,\n",
       " '65': 154,\n",
       " '66': 155,\n",
       " '68': 156,\n",
       " '71': 157,\n",
       " '72': 158,\n",
       " '73': 159,\n",
       " '74': 160,\n",
       " '75': 161,\n",
       " '78': 162,\n",
       " '79': 163,\n",
       " '8': 164,\n",
       " '80': 165,\n",
       " '81': 166,\n",
       " '82': 167,\n",
       " '83': 168,\n",
       " '84': 169,\n",
       " '85': 170,\n",
       " '86': 171,\n",
       " '87': 172,\n",
       " '88': 173,\n",
       " '89': 174,\n",
       " '9': 175,\n",
       " '93': 176,\n",
       " '94': 177,\n",
       " '95': 178,\n",
       " '96': 179,\n",
       " '98': 180,\n",
       " '99': 181}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvt.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0,  2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
       "          4,  0,  0],\n",
       "        [ 0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         19,  0,  0],\n",
       "        [ 0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
       "          6,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.todense()[:3,:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31395, 5)\n"
     ]
    }
   ],
   "source": [
    "n_components = 5\n",
    "lda = LatentDirichletAllocation(n_components=n_components, n_jobs=16)\n",
    "lda_matrix = lda.fit_transform(matrix)\n",
    "print(lda_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.38234714,  0.15388811,  0.34257712,  0.00351763,  0.11767   ],\n",
       "       [ 0.48401368,  0.03006309,  0.2964955 ,  0.02644147,  0.16298626],\n",
       "       [ 0.39849761,  0.00502299,  0.15465864,  0.00478641,  0.43703436],\n",
       "       ..., \n",
       "       [ 0.59885595,  0.10000118,  0.10000034,  0.10000262,  0.1011399 ],\n",
       "       [ 0.10026713,  0.59973186,  0.10000099,  0.1       ,  0.10000002],\n",
       "       [ 0.10000005,  0.10000001,  0.59999992,  0.10000002,  0.1       ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31395, 5)\n"
     ]
    }
   ],
   "source": [
    "n_components = 5\n",
    "lsa = TruncatedSVD(n_components=n_components)\n",
    "lsa_matrix = lsa.fit_transform(matrix)\n",
    "print(lsa_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.58257976e+01,   6.33504563e+00,  -2.31089716e-01,\n",
       "          7.35113939e-01,  -2.74577238e+00],\n",
       "       [  3.21226539e+01,  -2.03122058e+00,  -3.40827026e+00,\n",
       "         -5.19094325e+00,   2.18860761e+00],\n",
       "       [  1.28249003e+01,  -7.70624606e-01,  -6.30415413e+00,\n",
       "          2.81931284e+00,  -4.69762422e-01],\n",
       "       ..., \n",
       "       [  6.93655755e-04,   2.54613819e-04,  -9.72083629e-04,\n",
       "         -1.91898134e-03,  -1.41713870e-04],\n",
       "       [  2.77296705e-01,   9.21656987e-01,   2.17072966e-01,\n",
       "          1.37395002e-01,   5.78570024e-03],\n",
       "       [  6.23419986e-02,   1.21431713e-03,  -1.39939508e-02,\n",
       "         -4.25817591e-02,  -6.00880278e-02]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7452255 ,  0.10477226,  0.06171417,  0.02703803,  0.01768965])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lsa.explained_variance_ratio_)\n",
    "# Percentage of variance explained by each of the selected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2841.98029924,  1023.70565176,   786.0277348 ,   518.38397405,\n",
       "         419.08936088])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.singular_values_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF\n",
    "Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31395, 5)\n"
     ]
    }
   ],
   "source": [
    "n_components = 5\n",
    "nmf = NMF(n_components=n_components)\n",
    "nmf_matrix = nmf.fit_transform(matrix)\n",
    "print(nmf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.93122060e-01,   2.91712583e-01,   1.32214252e-01,\n",
       "          1.25294191e-01,   0.00000000e+00],\n",
       "       [  4.53978766e-01,   1.60075496e-01,   2.30497781e-01,\n",
       "          3.85526244e-01,   9.57598566e-02],\n",
       "       [  1.62191821e-01,   5.43949114e-02,   3.08390301e-01,\n",
       "          6.31075561e-02,   3.59693074e-03],\n",
       "       ..., \n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          4.40005367e-05,   2.68245633e-05],\n",
       "       [  0.00000000e+00,   2.81677037e-02,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  5.88626976e-04,   2.42124497e-04,   5.00473551e-05,\n",
       "          1.67553030e-03,   0.00000000e+00]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = ['apple apple1 apple2']\n",
    "t2 = ['apple apple1  apple3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = CountVectorizer()\n",
    "x.fit(t1)\n",
    "t1_t = x.transform(t1)\n",
    "t2_t = x.transform(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1_t.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 0]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_t.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Click in next N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orders = {}\n",
    "feature_col = ['ip', \n",
    "              'app', \n",
    "              'device', \n",
    "              'os', \n",
    "              'channel',\n",
    "              'day',\n",
    "              'hour',]\n",
    "\n",
    "# feature_col = ['ip', \n",
    "#               'app', \n",
    "#               'device', \n",
    "#               'os', \n",
    "#               'channel']\n",
    "for col in feature_col:\n",
    "    orders[col] = 10 ** (int(np.log(train[col].max() + 1) / np.log(10)) + 1)\n",
    "def get_group(df, cols):\n",
    "    \"\"\"\n",
    "    define an encoding method which can ganrantee the adding value will be unique.\n",
    "    eg: artist_name_composer will be a combination of (artist_name,composer) and the encoding will reflect the unqiue combination of those two\n",
    "    \"\"\"\n",
    "    group = df[cols[0]].copy()\n",
    "    for col in cols[1:]:\n",
    "        group = group * orders[col] + df[col]\n",
    "        \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _click_count_in_next_n_hour(df, cols, n_hour, time_col):\n",
    "    # dataframe should maintain a descending order\n",
    "    rev_train = df.sort_index(ascending=False)\n",
    "    encodings = get_group(rev_train, cols).values\n",
    "    times = rev_train[time_col].values\n",
    "    \n",
    "    dict_count = defaultdict(int)\n",
    "    result = []\n",
    "    bound = 0\n",
    "    for cur in tqdm(range(len(encodings))):\n",
    "\n",
    "        while times[bound] - times[cur] > n_hour:\n",
    "            dict_count[encodings[bound]] -= 1\n",
    "            bound += 1\n",
    "        encoding = encodings[cur]\n",
    "        result.append(dict_count[encoding])\n",
    "        dict_count[encoding] += 1\n",
    "    return result[::-1]\n",
    "        \n",
    "def _click_count_in_previous_n_hour(df, cols, n_hour, time_col):\n",
    "    # dataframe should maintain an ascending order\n",
    "    encodings = get_group(df, cols).values\n",
    "    times = df[time_col].values\n",
    "    \n",
    "    dict_count = defaultdict(int)\n",
    "    result = []\n",
    "    bound = 0\n",
    "    for cur in tqdm(range(len(encodings))):\n",
    "\n",
    "        while times[cur] - times[bound] > n_hour:\n",
    "            dict_count[encodings[bound]] -= 1\n",
    "            bound += 1\n",
    "        encoding = encodings[cur]\n",
    "        result.append(dict_count[encoding])\n",
    "        dict_count[encoding] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184903890/184903890 [03:27<00:00, 889670.75it/s]\n"
     ]
    }
   ],
   "source": [
    "gap = 60*60*6\n",
    "nextc = _click_count_in_next_n_hour(train,['app'],gap, 'timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184903890/184903890 [03:16<00:00, 939635.59it/s]\n"
     ]
    }
   ],
   "source": [
    "gap = 60*60*6\n",
    "prevc = _click_count_in_previous_n_hour(train,['app'],gap, 'timestamp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
