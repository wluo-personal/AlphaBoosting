{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T12:43:43.943591Z",
     "start_time": "2018-05-24T12:43:43.938092Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_log(s, rewrite=False):\n",
    "    mode = 'w' if rewrite else 'a'\n",
    "    with open('log.txt', mode) as f:\n",
    "        f.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T18:01:52.933917Z",
     "start_time": "2018-05-22T17:59:55.352089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index     ip  app  device  os  channel           click_time  is_attributed  \\\n",
      "0      0  83230    3       1  13      379  2017-11-06 14:32:21              0   \n",
      "1      1  17357    3       1  19      379  2017-11-06 14:33:34              0   \n",
      "2      2  35810    3       1  13      379  2017-11-06 14:34:12              0   \n",
      "\n",
      "   click_id  is_test  \n",
      "0         0        0  \n",
      "1         0        0  \n",
      "2         0        0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PATH = '/home/kai/data/kaggle/talkingdata/data/'\n",
    "# nrows = 10\n",
    "nrows = None\n",
    "dtypes = {\n",
    "    'ip':            'uint32',\n",
    "    'app':           'uint16',\n",
    "    'device':        'uint16',\n",
    "    'os':            'uint16',\n",
    "    'channel':       'uint16',\n",
    "    'is_attributed': 'uint8',\n",
    "    'click_id':      'uint32'\n",
    "}\n",
    "train = pd.read_csv(PATH + 'train.csv', nrows=nrows, dtype=dtypes,\n",
    "                    usecols=['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed']).reset_index()\n",
    "test = pd.read_csv(PATH + 'test_supplement.csv', nrows=nrows, dtype=dtypes,\n",
    "                    usecols=['ip', 'app', 'device', 'os', 'channel', 'click_time', 'click_id']).reset_index()\n",
    "train['click_id'] = 0\n",
    "train['is_test'] = 0\n",
    "test['is_test'] = 1\n",
    "test['is_attributed'] = 2\n",
    "print(train.head(3))\n",
    "write_log('data reading', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## machine = device + os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T18:04:30.372991Z",
     "start_time": "2018-05-22T18:01:53.089987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   app  channel  click_id                click_time  click_timestamp  day  \\\n",
      "0    3      379         0 2017-11-06 22:32:21+08:00       1509978741    6   \n",
      "1    3      379         0 2017-11-06 22:33:34+08:00       1509978814    6   \n",
      "2    3      379         0 2017-11-06 22:34:12+08:00       1509978852    6   \n",
      "\n",
      "   dayhourminute  dayhourminute10  device  hour  hourminute  hourminute10  \\\n",
      "0           9992             9990       1    22        1352          1350   \n",
      "1           9993             9990       1    22        1353          1350   \n",
      "2           9994             9990       1    22        1354          1350   \n",
      "\n",
      "   index     ip  is_attributed  is_test  machine  minute  minute10  os  \n",
      "0      0  83230              0        0     1013      32        30  13  \n",
      "1      1  17357              0        0     1019      33        30  19  \n",
      "2      2  35810              0        0     1013      34        30  13  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pytz\n",
    "\n",
    "# set time zone to be Shanghai time and split click_time into day, hour and minute\n",
    "def data_clean(df):\n",
    "    tz = pytz.timezone('Asia/Shanghai')\n",
    "    df['click_time'] = pd.to_datetime(df['click_time']).dt.tz_localize(pytz.utc).dt.tz_convert(tz)\n",
    "    df['day'] = df['click_time'].dt.day.astype('uint8')\n",
    "    df['hour'] = df['click_time'].dt.hour.astype('uint8')\n",
    "    df['minute'] = df['click_time'].dt.minute.astype('uint8')\n",
    "    df['minute10'] = (df['minute'] / 10).astype('uint8') * 10 # set to 10 minute\n",
    "    df['hourminute'] = (df['minute'].astype('uint16') + df['hour'].astype('uint16') * 60)\n",
    "    df['hourminute10'] = (df['minute10'].astype('uint16') + df['hour'].astype('uint16') * 60)\n",
    "    df['dayhourminute'] = (df['hourminute'].astype('uint32') + df['day'].astype('uint32') * 60 * 24)\n",
    "    df['dayhourminute10'] = (df['hourminute10'].astype('uint32') + df['day'].astype('uint32') * 60 * 24)\n",
    "    df['machine'] = 1000 * df['device'] + df['os']\n",
    "    df['click_timestamp'] = (df['click_time'].astype(np.int64) // 10 ** 9).astype(np.int32)\n",
    "\n",
    "    \n",
    "    \n",
    "data_clean(train)\n",
    "data_clean(test)\n",
    "df = pd.concat([train, test], ignore_index=True) # concat train and test\n",
    "\n",
    "data_type = df.dtypes.to_dict()\n",
    "\n",
    "label = 'is_attributed'\n",
    "train_len = train.shape[0]\n",
    "fdir = '/home/kai/data/kaggle/talkingdata/haoyandata1/'\n",
    "print(df.head(3))\n",
    "train.to_feather(fdir + 'train_cleaned.ftr')\n",
    "test.to_feather(fdir + 'test_cleaned.ftr')\n",
    "write_log('data cleaning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T18:04:30.771889Z",
     "start_time": "2018-05-22T18:04:30.768910Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(df, col_name, train_len):\n",
    "    df.reset_index(drop=True)\n",
    "    df[ : train_len].to_feather(fdir + 'train__' + col_name + '.ftr')\n",
    "    df[train_len : ].reset_index(drop=True).to_feather(fdir + 'test_supplement__' + col_name + '.ftr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count the click number for each feature combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T15:17:22.741156Z",
     "start_time": "2018-05-22T15:17:22.644238Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Here df is [train test_supp]\n",
    "def count(df, cols, label, train_len):\n",
    "    col_name = 'count_' + '_'.join(cols)\n",
    "    d_cols = list(cols)\n",
    "    d_cols.append(label)\n",
    "    count_result = df[d_cols].groupby(by=cols)[[label]].count().rename(index=str, columns={label: col_name}).reset_index()\n",
    "    type_map = {i: data_type[i] for i in count_result.columns.values if i in data_type.keys()}\n",
    "    _df = df.merge(count_result.astype(type_map), on=cols, how='left')\n",
    "    save(_df[[col_name]], col_name, train_len)\n",
    "    del _df, count_result\n",
    "    gc.collect()\n",
    "\n",
    "patterns = [\n",
    "    ['app','channel'],\n",
    "    ['app','device','channel','day','hour'],##\n",
    "    ['app','device','day','hour'],##\n",
    "    ['app','os','channel','day','hour'],##\n",
    "    ['ip','day'],\n",
    "    ['ip'],#\n",
    "    ['ip','app','device','channel','day'],##\n",
    "    ['ip','app','device','day'],##\n",
    "    ['ip','app','device','os','day','hour'],##\n",
    "    ['ip','app','os','channel'],##\n",
    "    ['ip','app','os','channel','day'],##\n",
    "    ['ip','os'],\n",
    "    ['app','day','hourminute'],\n",
    "    ['device','os','day','hourminute10'],##\n",
    "    ['ip','device','os','day','hourminute10']##\n",
    "]\n",
    "\n",
    "\n",
    "write_log('count')\n",
    "for p in patterns:\n",
    "    count(df, p, label, train_len)\n",
    "    write_log(str(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unique count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## group data by certain feature combination and count the number of different values of another feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T16:10:44.055822Z",
     "start_time": "2018-05-22T16:10:43.949954Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    nunique_ip_day\n",
      "0                2\n",
      "1                2\n",
      "2                2\n",
      "3                2\n",
      "4                2\n",
      "5                2\n",
      "6                1\n",
      "7                1\n",
      "8                1\n",
      "9                5\n",
      "10               5\n",
      "11               5\n",
      "12               5\n",
      "13               5\n",
      "14               5\n",
      "15               5\n",
      "16               5\n",
      "17               5\n",
      "18               5\n",
      "19               5\n",
      "20               5\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "def unique_count(df, cols, train_len):\n",
    "    col_name = 'nunique_' + '_'.join(cols)\n",
    "    count_result = df[cols].groupby(by=cols[:-1])[[cols[-1]]].nunique().rename(index=str,\\\n",
    "                                                                               columns={cols[-1]: col_name}).reset_index()\n",
    "    type_map = {i: data_type[i] for i in count_result.columns.values if i in data_type.keys()}\n",
    "    _df = df.merge(count_result.astype(type_map), on=cols[:-1], how='left')\n",
    "    print(_df[[col_name]])\n",
    "    save(_df[[col_name]], col_name, train_len)\n",
    "    del _df, count_result\n",
    "    gc.collect()\n",
    "    \n",
    "patterns = [\n",
    "    ['day','ip','machine'],\n",
    "    ['day','ip','os'],\n",
    "    ['day','ip','device'],\n",
    "    ['day','ip','app'],\n",
    "    ['day','ip','channel'],\n",
    "    ['machine','app'],\n",
    "    ['machine','ip'],\n",
    "    ['machine','channel'],\n",
    "]\n",
    "\n",
    "write_log('unique count')\n",
    "for p in patterns:\n",
    "    unique_count(df, p, train_len)\n",
    "    write_log(str(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cumulative count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## give an order number in each feature combination by each feature combination, sorted by [click_time, index, is_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T16:12:18.308310Z",
     "start_time": "2018-05-22T16:12:18.229300Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def cum_count(df, cols, train_len):\n",
    "    col_name = 'cumcount_' + '_'.join(cols)\n",
    "    result = df[cols].groupby(cols).cumcount().rename(col_name).to_frame().reset_index(drop=True)\n",
    "    save(result, col_name, train_len)\n",
    "    del result\n",
    "    gc.collect()\n",
    "    \n",
    "patterns = [\n",
    "    ['ip','app','device','os','day','hour'],\n",
    "    ['ip','day'],\n",
    "    ['app','device','os','day']\n",
    "]\n",
    "\n",
    "write_log('cummulative count')\n",
    "df.sort_values(['click_time','index','is_test'], inplace=True)\n",
    "for p in patterns:\n",
    "    cum_count(df, p, train_len)\n",
    "    write_log(str(p))\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cols1 count / cols2 count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T16:12:57.067120Z",
     "start_time": "2018-05-22T16:12:56.904326Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def _count(df, cols, label):\n",
    "    col_name = 'count_ratio_' + '_'.join(cols)\n",
    "    d_cols = list(cols)\n",
    "    d_cols.append(label)\n",
    "    count_result = df[d_cols].groupby(by=cols)[[label]].count().rename(index=str, columns={label: col_name}).reset_index()\n",
    "    type_map = {i: data_type[i] for i in count_result.columns.values if i in data_type.keys()}\n",
    "    _df = df.merge(count_result.astype(type_map), on=cols, how='left')\n",
    "    result = _df[[col_name]].copy()\n",
    "    del _df, count_result\n",
    "    gc.collect()\n",
    "    return result\n",
    "\n",
    "def count_ratio(df, cols1, cols2, label, train_len):\n",
    "    col_name = 'count_ratio_' + '_'.join(cols1) + '_' + '_'.join(cols2)\n",
    "    x1 = _count(df, cols1, label)\n",
    "    x2 = _count(df, cols2, label)\n",
    "    x1[col_name] = x1[x1.columns.values[0]] / x2[x2.columns.values[0]] # or = round(x1 / x2, 4)\n",
    "    result = x1[[col_name]]\n",
    "    save(result, col_name, train_len)\n",
    "    del x1, x2\n",
    "    gc.collect()\n",
    "    \n",
    "patterns = [\n",
    "    {'cols1':['ip'], 'cols2':['machine']},\n",
    "    {'cols1':['ip'], 'cols2':['channel']},\n",
    "    {'cols1':['machine'], 'cols2':['ip']},\n",
    "    {'cols1':['app'], 'cols2':['channel']},\n",
    "    {'cols1':['channel'], 'cols2':['app']}\n",
    "]\n",
    "\n",
    "write_log('count ratio')\n",
    "for p in patterns:\n",
    "    count_ratio(df, p['cols1'], p['cols2'], label, train_len)\n",
    "    write_log(str(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cumulative count ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cols cumcount / (cols count-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T16:19:46.185475Z",
     "start_time": "2018-05-22T16:19:46.045993Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0.000000\n",
      "1     1.000000\n",
      "2     0.000000\n",
      "3     0.333333\n",
      "4     0.666667\n",
      "5     1.000000\n",
      "6     0.000000\n",
      "7     0.500000\n",
      "8     1.000000\n",
      "9     0.000000\n",
      "10    0.333333\n",
      "11    0.000000\n",
      "12    0.500000\n",
      "13    1.000000\n",
      "14    0.000000\n",
      "15    1.000000\n",
      "16    0.000000\n",
      "17    1.000000\n",
      "18         NaN\n",
      "19    0.666667\n",
      "20    1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "def _count(df, cols, label):\n",
    "    col_name = 'count_ratio_' + '_'.join(cols)\n",
    "    d_cols = list(cols)\n",
    "    d_cols.append(label)\n",
    "    count_result = df[d_cols].groupby(by=cols)[[label]].count().rename(index=str, columns={label: col_name}).reset_index()\n",
    "    type_map = {i: data_type[i] for i in count_result.columns.values if i in data_type.keys()}\n",
    "    _df = df.merge(count_result.astype(type_map), on=cols, how='left')\n",
    "    result = _df[[col_name]].copy()\n",
    "    del _df, count_result\n",
    "    gc.collect()\n",
    "    return result\n",
    "\n",
    "def _cum_count(df, cols):\n",
    "    col_name = 'cumcount_ratio_' + '_'.join(cols)\n",
    "    result = df[cols].groupby(cols).cumcount().rename(col_name).to_frame()\n",
    "    return result.reset_index()[[col_name]]\n",
    "    \n",
    "def cum_count_ratio(df, cols, label, train_len):\n",
    "    col_name = 'cumcount_ratio_' + '_'.join(cols)\n",
    "    x1 = _cum_count(df, cols)\n",
    "    x2 = _count(df, cols, label)\n",
    "    x1[col_name] = round(x1[x1.columns.values[0]] / (x2[x2.columns.values[0]] - 1), 4).fillna(1.1)\n",
    "    result = x1[[col_name]]\n",
    "    save(result, col_name, train_len)\n",
    "    del x1, x2\n",
    "    gc.collect()\n",
    "    \n",
    "patterns = [\n",
    "    ['ip','day']\n",
    "]\n",
    "\n",
    "write_log('cumulative count ratio')\n",
    "for p in patterns:\n",
    "    cum_count_ratio(df, p, label, train_len)\n",
    "    write_log(str(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to n next click and its filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time delta from current click to the next same feature combination click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T16:23:48.303401Z",
     "start_time": "2018-05-22T16:23:48.144431Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def time_to_n_next_click(df, n, cols, time_col, train_len):\n",
    "    col_name = 'time_to_n_next_click_' + str(n) + '_' + '_'.join(cols)\n",
    "    total_cols = list(cols)\n",
    "    total_cols.append(time_col)\n",
    "    _df = df[total_cols].copy()\n",
    "    _df[col_name] = (_df.groupby(cols)[time_col].shift(-n) - _df[time_col] + 1).fillna(999999).astype(int)\n",
    "    out = _df[[col_name]].sort_index()\n",
    "    save(out, col_name, train_len)\n",
    "    del _df, out\n",
    "    gc.collect()\n",
    "    return col_name\n",
    "    \n",
    "def time_to_n_next_click_filter(name, train_len):\n",
    "    col_name = 'filter_' + name\n",
    "    in_func_train = pd.read_feather(fdir + 'train__' + name + '.ftr')\n",
    "    in_func_test = pd.read_feather(fdir + 'test_supplement__' + name + '.ftr')\n",
    "    in_func_df = pd.concat([in_func_train, in_func_test], ignore_index=True)\n",
    "    in_func_df[col_name] = 2\n",
    "    in_func_df[col_name] -= (in_func_df[name] < 1800) & (in_func_df[name] > 2)\n",
    "    in_func_df[col_name] -= (in_func_df[name] < 30) * 2\n",
    "    in_func_df\n",
    "    save(in_func_df[[col_name]], col_name, train_len)\n",
    "    del in_func_df, in_func_train, in_func_test\n",
    "    gc.collect()\n",
    "    \n",
    "patterns = [\n",
    "    ['day','ip','app','device','os']\n",
    "]\n",
    "\n",
    "write_log('time to next')\n",
    "df.sort_values(['click_time','is_attributed','click_id'], inplace=True)\n",
    "for p in patterns:\n",
    "    time_to_n_next_click_filter(time_to_n_next_click(df, 1, p, 'click_timestamp', train_len), train_len)\n",
    "    time_to_n_next_click_filter(time_to_n_next_click(df, 2, p, 'click_timestamp', train_len), train_len)\n",
    "    write_log(str(p))\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# range count (same as unique count of certain time col group by feature combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T16:24:29.781001Z",
     "start_time": "2018-05-22T16:24:29.671383Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def unique_count(df, cols, train_len):\n",
    "    col_name = 'rang_count_' + '_'.join(cols)\n",
    "    count_result = df[cols].groupby(by=cols[:-1])[[cols[-1]]].nunique().rename(index=str,\\\n",
    "                                                                               columns={cols[-1]: col_name}).reset_index()\n",
    "    type_map = {i: data_type[i] for i in count_result.columns.values if i in data_type.keys()}\n",
    "    _df = df.merge(count_result.astype(type_map), on=cols[:-1], how='left')\n",
    "    save(_df[[col_name]], col_name, train_len)\n",
    "    del _df, count_result\n",
    "    gc.collect()\n",
    "    \n",
    "patterns = [\n",
    "    ['ip','day'],\n",
    "    ['ip','hour'],\n",
    "    ['ip','dayhourminute'],\n",
    "    ['ip','dayhourminute10'],\n",
    "    ['app','os','channel','dayhourminute'],\n",
    "    ['app','os','channel','dayhourminute10'],\n",
    "    ['ip','channel','dayhourminute'],\n",
    "    ['ip','channel','dayhourminute10'],\n",
    "    ['ip','device','os','dayhourminute'],\n",
    "    ['ip','device','os','dayhourminute10'],\n",
    "]\n",
    "\n",
    "write_log('range count')\n",
    "for p in patterns:\n",
    "    unique_count(df, p, train_len)\n",
    "    write_log(str(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variance (/(N-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variance for the last col element groupby the first several columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T16:25:36.398468Z",
     "start_time": "2018-05-22T16:25:36.311305Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variance(df, cols, train_len):\n",
    "    col_name = 'variance_' + '_'.join(cols)\n",
    "    group = df[cols].groupby(by=cols[:-1])[[cols[-1]]].var().reset_index().rename(index=str, columns={cols[-1]: col_name})\n",
    "    group[col_name] = group[col_name].fillna(0).astype(int)\n",
    "    type_map = {i: data_type[i] for i in group.columns.values if i in data_type.keys()}\n",
    "    _df = df.merge(group, on=cols[:-1], how='left')\n",
    "    save(_df[[col_name]], col_name, train_len)\n",
    "    del _df, group\n",
    "    gc.collect()\n",
    "    \n",
    "patterns = [\n",
    "    ['ip','device','hour']\n",
    "]\n",
    "\n",
    "write_log('var')\n",
    "for p in patterns:\n",
    "    variance(df, p, train_len)\n",
    "    write_log(str(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# common ip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this part is to assume that std(count_ip/day)/mean(count_ip/day) will behave different when fraud comes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T16:44:59.367781Z",
     "start_time": "2018-05-22T16:44:59.214517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "0\n",
      "1\n",
      "2\n",
      "5\n",
      "7\n",
      "9\n",
      "    ip\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "8    0\n",
      "9    3\n",
      "10   3\n",
      "11   3\n",
      "12   3\n",
      "13   3\n",
      "14   3\n",
      "15   3\n",
      "16   3\n",
      "17   3\n",
      "18   3\n",
      "19   3\n",
      "20   3\n",
      "21   0\n",
      "22   0\n",
      "23   0\n",
      "24   0\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "def get_com_ip(df, col, train_len):\n",
    "    fday = df[col].min()\n",
    "    lday = df[col].max()\n",
    "    if len(df[df.day==fday]) < 1000:\n",
    "        fday += 1\n",
    "    if len(df[df.day==lday]) < 1000:\n",
    "        lday -= 1\n",
    "        \n",
    "    name = 'com_ip'\n",
    "    com_set = set()\n",
    "    for d in range(fday,lday+1):\n",
    "        if d == fday:\n",
    "            com_set = set(df[df[col]==d]['ip'].unique())\n",
    "        else:\n",
    "            com_set = com_set & set(df[df[col]==d]['ip'].unique())\n",
    "    flt_ip = df.ip.isin(com_set)\n",
    "    com_ip = ((df['ip'] + 1) * flt_ip).to_frame()\n",
    "    print(com_ip)\n",
    "    save(com_ip, name, train_len)\n",
    "    \n",
    "    del com_ip\n",
    "    gc.collect()    \n",
    "    return flt_ip\n",
    "\n",
    "\n",
    "def dump_com_ip_feature(df, flt_ip, threshold, label, train_len):\n",
    "    com_df = df[flt_ip]\n",
    "    name = 'com' + str(threshold) + '_ip'\n",
    "    cols = ['ip', 'day']\n",
    "    total_cols = cols.copy()\n",
    "    total_cols.append(label)\n",
    "    group = com_df[total_cols].groupby(by=cols)[[label]].count().reset_index().rename(index=str, columns={label: 'count'})\n",
    "    result = group[['ip','count']].groupby('ip')[['count']].agg(['mean', 'std'])['count'].reset_index()\n",
    "    result['flg'] = (100 * result['std'] / result['mean']) <= threshold\n",
    "    type_map = {i: data_type[i] for i in result.columns.values if i in data_type.keys()}\n",
    "    _df = pd.merge(df, result[['ip','flg']], on='ip', how='left').fillna(False)\n",
    "    _df[name] = (_df['ip']+1) * _df['flg']\n",
    "    save(_df[[name]], name, train_len)\n",
    "\n",
    "    del _df\n",
    "    gc.collect()\n",
    "\n",
    "write_log('common ip')\n",
    "dump_com_ip_feature(df, get_com_ip(df, 'day', train_len), 1, label, train_len)\n",
    "write_log(str(['ip', 'day']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WOE (categorical feature encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use day 7,9 to get day 8, and same for other days on training and use all training to get woe on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-22T18:08:14.588Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "def _woe(calc_df, map_df, cols, label, col_name):\n",
    "    t_cols = list(cols)\n",
    "    t_cols.append(label)\n",
    "    group = calc_df[t_cols].groupby(by=cols)[[label]].agg(['count','sum'])[label].reset_index()\n",
    "    positive = calc_df[label].sum()\n",
    "    negative = calc_df.shape[0] - positive\n",
    "#     group[col_name] = np.log((group['sum']+0.5) / positive) / ((group['count']-group['sum']+0.5) / negative)\n",
    "    group[col_name] = np.log((group['sum'] / positive) / ((group['count']-group['sum']) / negative)) + 1\n",
    "    t_cols[-1] = col_name\n",
    "    type_map = {i: data_type[i] for i in group.columns.values if i in data_type.keys()}\n",
    "    return map_df.merge(group[t_cols], on=cols, how='left')\n",
    "\n",
    "def woe(train, test, cols, label):\n",
    "    fdf = train\n",
    "    fdf = train[train['hour']>=12]\n",
    "    fdf = fdf[fdf['hour']<=22]\n",
    "    fday = train['day'].min()\n",
    "    lday = train['day'].max()\n",
    "    total_cols = list(cols)\n",
    "    total_cols.append(label)\n",
    "    col_name = 'woe_' + '_'.join(cols)\n",
    "    _df_list = [_woe(fdf[fdf.day!=day], train[train.day==day], cols, label, col_name) for day in range(fday,lday+1)]\n",
    "    _df = pd.concat(_df_list).fillna(-1).reset_index(drop=True)\n",
    "    _df[[col_name]].to_feather(fdir + 'train__' + col_name + '.ftr')\n",
    "    del _df, _df_list\n",
    "    gc.collect()\n",
    "    \n",
    "    _df = _woe(fdf, test, cols, label, col_name).fillna(-1).reset_index()\n",
    "    _df[[col_name]].to_feather(fdir + 'test_supplement__' + col_name + '.ftr')\n",
    "    del _df\n",
    "    gc.collect()\n",
    "\n",
    "patterns = [\n",
    "    ['ip'],\n",
    "    ['app'],\n",
    "    ['device'],\n",
    "    ['os'],\n",
    "    ['channel'],\n",
    "    ['ip','app'],\n",
    "    ['ip','device'],\n",
    "    ['ip','os'],\n",
    "    ['ip','channel'],\n",
    "    ['app','device'],\n",
    "    ['app','os'],\n",
    "    ['app','channel'],\n",
    "    ['ip','app','device'],\n",
    "    ['ip','app','os'],\n",
    "    ['ip','app','channel'],\n",
    "    ['ip','device','os'],\n",
    "    ['ip','device','channel'],\n",
    "    ['ip','os','channel'],\n",
    "    ['app','device','os'],\n",
    "    ['app','device','channel'],\n",
    "    ['app','os','channel'],\n",
    "    ['ip','app','device','os'],\n",
    "    ['ip','app','device','channel'],\n",
    "    ['ip','app','os','channel'],\n",
    "    ['ip','device','os','channel'],\n",
    "    ['app','device','os','channel'],\n",
    "    ['ip','nextClickLeakDayFlt'],\n",
    "    ['app','nextClickLeakDayFlt'],\n",
    "    ['device','nextClickLeakDayFlt'],\n",
    "    ['os','nextClickLeakDayFlt'],\n",
    "    ['channel','nextClickLeakDayFlt'],\n",
    "    ['ip','app','nextClickLeakDayFlt'],\n",
    "    ['ip','device','nextClickLeakDayFlt'],\n",
    "    ['ip','os','nextClickLeakDayFlt'],\n",
    "    ['ip','channel','nextClickLeakDayFlt'],\n",
    "    ['app','device','nextClickLeakDayFlt'],\n",
    "    ['app','os','nextClickLeakDayFlt'],\n",
    "    ['app','channel','nextClickLeakDayFlt'],\n",
    "    ['device','os','nextClickLeakDayFlt'],\n",
    "    ['device','channel','nextClickLeakDayFlt'],\n",
    "    ['os','channel','nextClickLeakDayFlt']\n",
    "]\n",
    "woe_train = train\n",
    "woe_train['nextClickLeakDayFlt'] = pd.read_feather(fdir + 'train__filter_time_to_n_next_click_1_day_ip_app_device_os.ftr')\n",
    "woe_test = test\n",
    "woe_test['nextClickLeakDayFlt']=pd.read_feather\\\n",
    "        (fdir + 'test_supplement__filter_time_to_n_next_click_1_day_ip_app_device_os.ftr')\n",
    "\n",
    "write_log('woe')\n",
    "for p in patterns:\n",
    "    woe(woe_train, woe_test, p, label)\n",
    "    write_log(str(p))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T18:04:31.324067Z",
     "start_time": "2018-05-22T18:00:20.488Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_log('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T18:04:31.329633Z",
     "start_time": "2018-05-22T18:00:20.659Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
