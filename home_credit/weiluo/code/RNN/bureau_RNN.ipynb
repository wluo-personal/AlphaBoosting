{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T15:03:11.908688Z",
     "start_time": "2018-09-21T15:03:10.474023Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('../LIB/')\n",
    "from env import ENV\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU,CuDNNGRU,Flatten,BatchNormalization\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "from sklearn.preprocessing.data import QuantileTransformer\n",
    "from sklearn.utils import shuffle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T15:03:11.913229Z",
     "start_time": "2018-09-21T15:03:11.910354Z"
    }
   },
   "outputs": [],
   "source": [
    "def scan_nan_portion(df):\n",
    "    portions = []\n",
    "    columns = []\n",
    "    for col in df.columns:\n",
    "        columns.append(col)\n",
    "        portions.append(np.sum(df[col].isnull())/len(df))\n",
    "    return pd.Series(data=portions, index=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T15:03:12.115461Z",
     "start_time": "2018-09-21T15:03:11.914750Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../LIB/../../data/cleaned_data/application_train.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0;31m# We want to silencce any warnings about, e.g. moved modules.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mread_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    146\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minferred_compression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                             is_text=False)\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../LIB/../../data/cleaned_data/application_train.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 return read_wrapper(\n\u001b[0;32m--> 171\u001b[0;31m                     lambda f: pc.load(f, encoding=encoding, compat=False))\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;31m# compat pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    146\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minferred_compression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                             is_text=False)\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../LIB/../../data/cleaned_data/application_train.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(path, compression)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 return read_wrapper(\n\u001b[0;32m--> 175\u001b[0;31m                     lambda f: pc.load(f, encoding=encoding, compat=True))\n\u001b[0m\u001b[1;32m    176\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    146\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minferred_compression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                             is_text=False)\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../LIB/../../data/cleaned_data/application_train.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0;31m# We want to silencce any warnings about, e.g. moved modules.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mread_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    146\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minferred_compression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                             is_text=False)\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../LIB/../../data/cleaned_data/application_train.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 return read_wrapper(\n\u001b[0;32m--> 171\u001b[0;31m                     lambda f: pc.load(f, encoding=encoding, compat=False))\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;31m# compat pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    146\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minferred_compression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                             is_text=False)\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../LIB/../../data/cleaned_data/application_train.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c17932ecf1b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_Train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplication_train_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train shape: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_Train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_Test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplication_test_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test shape: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_Test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(path, compression)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 return read_wrapper(\n\u001b[0;32m--> 175\u001b[0;31m                     lambda f: pc.load(f, encoding=encoding, compat=True))\n\u001b[0m\u001b[1;32m    176\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    145\u001b[0m         f, fh = _get_handle(path, 'rb',\n\u001b[1;32m    146\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minferred_compression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                             is_text=False)\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../LIB/../../data/cleaned_data/application_train.pkl'"
     ]
    }
   ],
   "source": [
    "X_Train = pd.read_pickle(ENV.application_train_cleaned.value)\n",
    "print('Train shape: {}'.format(X_Train.shape))\n",
    "\n",
    "X_Test = pd.read_pickle(ENV.application_test_cleaned.value)\n",
    "print('Test shape: {}'.format(X_Test.shape))\n",
    "\n",
    "X_bu = pd.read_pickle(ENV.bureau_cleaned.value)\n",
    "print('Installment shape: {}'.format(X_bu.shape))\n",
    "\n",
    "X_bu_fe = pd.read_pickle(ENV.bureau_cleaned_rnnALL.value)\n",
    "print('Installment shape: {}'.format(X_bu_fe.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T15:03:12.116342Z",
     "start_time": "2018-09-21T15:03:10.222Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings_index(sorted_df,nor_ebd):\n",
    "    embeddings_index={}\n",
    "    words_values = sorted_df['words'].values\n",
    "    for index in range(len(words_values)):\n",
    "        embeddings_index  [words_values[index]] = nor_ebd[index,:]\n",
    "    return embeddings_index\n",
    "\n",
    "def create_document(sorted_df):\n",
    "    #Create document\n",
    "    ids = sorted_df.SK_ID_CURR.values\n",
    "    words = sorted_df.words.values\n",
    "    document_dicts = {}\n",
    "\n",
    "    id_list = []\n",
    "    document_list = []\n",
    "\n",
    "    for index in range(len(ids)) :\n",
    "        if document_dicts.get(ids[index]) is None:\n",
    "            document_dicts[ids[index]] = []\n",
    "        document_dicts[ids[index]].append(words[index])\n",
    "\n",
    "    for key in document_dicts :\n",
    "        document_dicts[key] = ' '.join(document_dicts[key])\n",
    "        id_list.append(key)\n",
    "        document_list.append(document_dicts[key])\n",
    "\n",
    "\n",
    "    df_doc = pd.DataFrame({'SK_ID_CURR':id_list, 'text':document_list})  \n",
    "    df_doc_mapping  = df_doc.set_index('SK_ID_CURR').text\n",
    "\n",
    "    train = X_Train[['SK_ID_CURR','TARGET']].copy()\n",
    "    test = X_Test[['SK_ID_CURR']].copy()\n",
    "    train['text'] = train.SK_ID_CURR.map(df_doc_mapping).fillna('notfound')\n",
    "    test['text'] = test.SK_ID_CURR.map(df_doc_mapping).fillna('notfound')\n",
    "    return train,test\n",
    "\n",
    "\n",
    "def get_train_ebdMat(train,test,embeddings_index):\n",
    "    X_train = train[\"text\"].str.lower()\n",
    "    X_test = test[\"text\"].str.lower()\n",
    "    y_train = train[\"TARGET\"].values\n",
    "    tok=text.Tokenizer(num_words=max_features,lower=True,filters='')\n",
    "    tok.fit_on_texts(list(X_train)+list(X_test))\n",
    "    X_train=tok.texts_to_sequences(X_train)\n",
    "    X_test=tok.texts_to_sequences(X_test)\n",
    "    x_train=sequence.pad_sequences(X_train,maxlen=maxlen)\n",
    "    x_test=sequence.pad_sequences(X_test,maxlen=maxlen)\n",
    "    print('...get ebd mat')\n",
    "    word_index = tok.word_index\n",
    "    #prepare embedding matrix\n",
    "    num_words = min(max_features, len(word_index) + 1)\n",
    "    print('num of words: {}'.format(num_words))\n",
    "    embedding_matrix = np.zeros((num_words, embed_size))\n",
    "    print(embedding_matrix.shape)\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return x_train,x_test,y_train,embedding_matrix,num_words\n",
    "\n",
    "class_ratio =  sum(X_Train.TARGET ==0)/sum(X_Train.TARGET ==1)\n",
    "class_ratio =  1\n",
    "def get_rnn_model(num_words,embed_size,embedding_matrix):\n",
    "    sequence_input = Input(shape=(maxlen, ))\n",
    "    \n",
    "    x = Embedding(num_words, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "#     x = Bidirectional(GRU(16, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "    x = Bidirectional(CuDNNGRU(8, return_sequences=True))(x)\n",
    "    x = Conv1D(32, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([avg_pool, max_pool]) \n",
    "#     x = BatchNormalization()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "#     Adam,RMSprop,Adagrad,Adadelta,Adamax,Nadam\n",
    "#     model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-4),metrics=['accuracy'])\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_each_epoch(x,y,batch_size,model):\n",
    "    x,y = shuffle(x,y)\n",
    "    model.fit(x, y, \n",
    "              batch_size=batch_size, \n",
    "              epochs=1,\n",
    "              verbose=1,\n",
    "              class_weight={0:1,1:class_ratio})\n",
    "    return model\n",
    "\n",
    "def load_model(model,filepath):\n",
    "    model.load_weights(filepath)\n",
    "    return model\n",
    "\n",
    "def save_model(model, filepath):\n",
    "    model.save_weights(filepath)\n",
    "\n",
    "def train_each_fold(x,y,x_val,y_val,model,filepath,reportpath,predspath,\n",
    "                    batch_size=512,total_epoch=40,patience=5,saving=True):\n",
    "    ROC_AUC_SCORE = []\n",
    "    for epoch in range(total_epoch):  \n",
    "        model = train_each_epoch(x,y,batch_size,model)\n",
    "        y_pred = model.predict(x_val,batch_size=3000,verbose=1)\n",
    "        score = roc_auc_score(y_val,y_pred)\n",
    "        if len(ROC_AUC_SCORE) == 0:\n",
    "            if saving:\n",
    "                save_model(model,filepath)\n",
    "            best_score = 0 \n",
    "            if saving:\n",
    "                print('saving preds...')\n",
    "                pickle.dump(y_pred,open(predspath,'wb'))\n",
    "        else:\n",
    "            best_score = max(ROC_AUC_SCORE)\n",
    "            if score >= best_score:\n",
    "                if saving:\n",
    "                    print('saving model to... {}'.format(filepath))\n",
    "                    save_model(model,filepath)\n",
    "                    print('saving preds...')\n",
    "                    pickle.dump(y_pred,open(predspath,'wb'))\n",
    "        ROC_AUC_SCORE.append(score)\n",
    "        if saving:\n",
    "            print('saving report to... {}'.format(reportpath))\n",
    "            pickle.dump(ROC_AUC_SCORE,open(reportpath,'wb'))\n",
    "        print('======= current {} / {}'.format(epoch,total_epoch))\n",
    "        print('previous best roc is {}'.format(best_score))\n",
    "        print('current roc is {}'.format(score))\n",
    "        try:\n",
    "            best_round = ROC_AUC_SCORE.index(best_score)\n",
    "        except ValueError:\n",
    "            best_round = -1\n",
    "        if len(ROC_AUC_SCORE) > patience + best_round:\n",
    "            print('reach patience! end loop')\n",
    "            break\n",
    "            \n",
    "def train_5_folds(model_file,report_file,pred_file,pred_test_file,batch_size=512,total_epoch=400,patience=30):\n",
    "    train_fold_index = pickle.load(open(ENV.train_fold_index.value,'rb'))\n",
    "    val_fold_index = pickle.load(open(ENV.val_fold_index.value,'rb'))\n",
    "\n",
    "    for fold in range(0,len(train_fold_index)):\n",
    "        print('!!!!!!!! Begin fold: {}'.format(fold))\n",
    "        train_index = train_fold_index[fold]\n",
    "        val_index = val_fold_index[fold]\n",
    "        X_tra = x_train[train_index]\n",
    "        y_tra = y_train[train_index]\n",
    "        X_val = x_train[val_index]\n",
    "        y_val = y_train[val_index]\n",
    "        print('preparing train/val done!')\n",
    "        print('before evaluating: {}'.format(model_file))\n",
    "        model_file_evl = model_file.format(fold)\n",
    "        report_file_evl = report_file.format(fold)\n",
    "        pred_file_evl = pred_file.format(fold)\n",
    "        pred_test_file_evl = pred_test_file.format(fold)\n",
    "        model = get_rnn_model(num_words,embed_size,embedding_matrix)\n",
    "        train_each_fold(X_tra, y_tra, X_val, y_val,\n",
    "                        model,\n",
    "                        filepath=model_file_evl,reportpath=report_file_evl,predspath=pred_file_evl,\n",
    "                        batch_size=batch_size,total_epoch=total_epoch,patience=patience)\n",
    "        gc.collect()\n",
    "        #### predict test\n",
    "        model = load_model(model,model_file_evl)\n",
    "        test_preds = model.predict(x_test,batch_size=1500,verbose=1)\n",
    "        pickle.dump(test_preds,open(pred_test_file_evl,'wb'))\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bureau V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T15:03:12.117235Z",
     "start_time": "2018-09-21T15:03:10.225Z"
    }
   },
   "outputs": [],
   "source": [
    "qt = QuantileTransformer(n_quantiles=10000,output_distribution='normal')\n",
    "trans_col = trans_col = [\n",
    "             'AMT_ANNUITY',\n",
    "             'AMT_CREDIT_MAX_OVERDUE',\n",
    "             'DAYS_ENDDATE_FACT',\n",
    "             'AMT_CREDIT_SUM_LIMIT',\n",
    "             'AMT_CREDIT_SUM_DEBT',\n",
    "             'DAYS_CREDIT_ENDDATE',\n",
    "             'AMT_CREDIT_SUM',\n",
    "             'CREDIT_DAY_OVERDUE',\n",
    "             'CNT_CREDIT_PROLONG',\n",
    "             'AMT_CREDIT_SUM_OVERDUE',\n",
    "             'DAYS_CREDIT_UPDATE']\n",
    "for col in trans_col:\n",
    "    print(col)\n",
    "    X_bu[col] = qt.fit_transform(X_bu[col].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T15:03:12.118049Z",
     "start_time": "2018-09-21T15:03:10.234Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print('start')\n",
    "label_mapping = X_Train.set_index('SK_ID_CURR').TARGET\n",
    "test_mapping = pd.Series(index=X_Test.SK_ID_CURR, data=1)\n",
    "\n",
    "#previous application\n",
    "#10001358\n",
    "max_features = 1730000\n",
    "#295\n",
    "maxlen = 13\n",
    "\n",
    "sorted_df = X_bu.sort_values(['SK_ID_CURR','DAYS_CREDIT'])\n",
    "col = 'DAYS_CREDIT'\n",
    "sorted_df[col] = qt.fit_transform(sorted_df[col].values.reshape(-1,1))\n",
    "sorted_df['words'] = sorted_df.index.astype(str)\n",
    "feature = list(sorted_df.columns)\n",
    "feature.remove('SK_ID_BUREAU')\n",
    "feature.remove('SK_ID_CURR')\n",
    "feature.remove('words')\n",
    "ebd = sorted_df[feature].values\n",
    "#normalize\n",
    "print('start normalize')\n",
    "# nor_ebd = normalize(ebd, norm='max',axis=0)\n",
    "nor_ebd = ebd\n",
    "print('get embedding')\n",
    "embed_size = len(feature)\n",
    "print('ebd size is {}'.format(embed_size))\n",
    "embeddings_index = get_embeddings_index(sorted_df,nor_ebd)\n",
    "print('create document')\n",
    "train,test = create_document(sorted_df)\n",
    "print('get embedding Mat')\n",
    "x_train,x_test,y_train,embedding_matrix,num_words = get_train_ebdMat(train,test,embeddings_index)\n",
    "model_file = ENV.bureau_rnn.value\n",
    "report_file = ENV.bureau_report.value\n",
    "pred_file = ENV.bureau_preds.value\n",
    "pred_test_file = ENV.bureau_preds_test.value\n",
    "train_5_folds(model_file,report_file,pred_file,pred_test_file,batch_size=2000,total_epoch=500,patience=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T15:03:12.118826Z",
     "start_time": "2018-09-21T15:03:10.235Z"
    }
   },
   "outputs": [],
   "source": [
    "a = train.text.apply(lambda x: len(x.split(' ')))\n",
    "a.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T15:03:12.119563Z",
     "start_time": "2018-09-21T15:03:10.238Z"
    }
   },
   "outputs": [],
   "source": [
    "aaaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bureau V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T15:03:12.120382Z",
     "start_time": "2018-09-21T15:03:10.239Z"
    }
   },
   "outputs": [],
   "source": [
    "qt = QuantileTransformer(n_quantiles=10000,output_distribution='normal')\n",
    "trans_col = [\n",
    "             'AMT_ANNUITY',\n",
    "             'AMT_CREDIT_MAX_OVERDUE',\n",
    "             'DAYS_ENDDATE_FACT',\n",
    "             'AMT_CREDIT_SUM_LIMIT',\n",
    "             'AMT_CREDIT_SUM_DEBT',\n",
    "             'DAYS_CREDIT_ENDDATE',\n",
    "             'AMT_CREDIT_SUM',\n",
    "             'CREDIT_DAY_OVERDUE',\n",
    "             'CNT_CREDIT_PROLONG',\n",
    "             'BUREAU_LENGTH',\n",
    "             'AMT_CREDIT_SUM_OVERDUE',\n",
    "             'DAYS_CREDIT_UPDATE',\n",
    "    'AMT_ANNUITY_squre',\n",
    " 'AMT_CREDIT_MAX_OVERDUE_squre',\n",
    " 'DAYS_ENDDATE_FACT_squre',\n",
    " 'AMT_CREDIT_SUM_LIMIT_squre',\n",
    " 'AMT_CREDIT_SUM_DEBT_squre',\n",
    " 'DAYS_CREDIT_ENDDATE_squre',\n",
    " 'AMT_CREDIT_SUM_squre',\n",
    " 'CREDIT_DAY_OVERDUE_squre',\n",
    " 'CNT_CREDIT_PROLONG_squre',\n",
    " 'BUREAU_LENGTH_squre',\n",
    " 'AMT_CREDIT_SUM_OVERDUE_squre',\n",
    " 'DAYS_CREDIT_UPDATE_squre',\n",
    "    'AMT_ANNUITY_AMT_CREDIT_MAX_OVERDUE',\n",
    " 'AMT_ANNUITY_DAYS_ENDDATE_FACT',\n",
    " 'AMT_ANNUITY_AMT_CREDIT_SUM_LIMIT',\n",
    " 'AMT_ANNUITY_AMT_CREDIT_SUM_DEBT',\n",
    " 'AMT_ANNUITY_DAYS_CREDIT_ENDDATE',\n",
    " 'AMT_ANNUITY_AMT_CREDIT_SUM',\n",
    " 'AMT_ANNUITY_CREDIT_DAY_OVERDUE',\n",
    " 'AMT_ANNUITY_CNT_CREDIT_PROLONG',\n",
    " 'AMT_ANNUITY_BUREAU_LENGTH',\n",
    " 'AMT_ANNUITY_AMT_CREDIT_SUM_OVERDUE',\n",
    " 'AMT_ANNUITY_DAYS_CREDIT_UPDATE',\n",
    " 'AMT_CREDIT_MAX_OVERDUE_DAYS_ENDDATE_FACT',\n",
    " 'AMT_CREDIT_MAX_OVERDUE_AMT_CREDIT_SUM_LIMIT',\n",
    " 'AMT_CREDIT_MAX_OVERDUE_AMT_CREDIT_SUM_DEBT',\n",
    " 'AMT_CREDIT_MAX_OVERDUE_DAYS_CREDIT_ENDDATE',\n",
    " 'AMT_CREDIT_MAX_OVERDUE_AMT_CREDIT_SUM',\n",
    " 'AMT_CREDIT_MAX_OVERDUE_CREDIT_DAY_OVERDUE',\n",
    " 'AMT_CREDIT_MAX_OVERDUE_CNT_CREDIT_PROLONG',\n",
    " 'AMT_CREDIT_MAX_OVERDUE_BUREAU_LENGTH',\n",
    " 'AMT_CREDIT_MAX_OVERDUE_AMT_CREDIT_SUM_OVERDUE',\n",
    " 'AMT_CREDIT_MAX_OVERDUE_DAYS_CREDIT_UPDATE',\n",
    " 'DAYS_ENDDATE_FACT_AMT_CREDIT_SUM_LIMIT',\n",
    " 'DAYS_ENDDATE_FACT_AMT_CREDIT_SUM_DEBT',\n",
    " 'DAYS_ENDDATE_FACT_DAYS_CREDIT_ENDDATE',\n",
    " 'DAYS_ENDDATE_FACT_AMT_CREDIT_SUM',\n",
    " 'DAYS_ENDDATE_FACT_CREDIT_DAY_OVERDUE',\n",
    " 'DAYS_ENDDATE_FACT_CNT_CREDIT_PROLONG',\n",
    " 'DAYS_ENDDATE_FACT_BUREAU_LENGTH',\n",
    " 'DAYS_ENDDATE_FACT_AMT_CREDIT_SUM_OVERDUE',\n",
    " 'DAYS_ENDDATE_FACT_DAYS_CREDIT_UPDATE',\n",
    " 'AMT_CREDIT_SUM_LIMIT_AMT_CREDIT_SUM_DEBT',\n",
    " 'AMT_CREDIT_SUM_LIMIT_DAYS_CREDIT_ENDDATE',\n",
    " 'AMT_CREDIT_SUM_LIMIT_AMT_CREDIT_SUM',\n",
    " 'AMT_CREDIT_SUM_LIMIT_CREDIT_DAY_OVERDUE',\n",
    " 'AMT_CREDIT_SUM_LIMIT_CNT_CREDIT_PROLONG',\n",
    " 'AMT_CREDIT_SUM_LIMIT_BUREAU_LENGTH',\n",
    " 'AMT_CREDIT_SUM_LIMIT_AMT_CREDIT_SUM_OVERDUE',\n",
    " 'AMT_CREDIT_SUM_LIMIT_DAYS_CREDIT_UPDATE',\n",
    " 'AMT_CREDIT_SUM_DEBT_DAYS_CREDIT_ENDDATE',\n",
    " 'AMT_CREDIT_SUM_DEBT_AMT_CREDIT_SUM',\n",
    " 'AMT_CREDIT_SUM_DEBT_CREDIT_DAY_OVERDUE',\n",
    " 'AMT_CREDIT_SUM_DEBT_CNT_CREDIT_PROLONG',\n",
    " 'AMT_CREDIT_SUM_DEBT_BUREAU_LENGTH',\n",
    " 'AMT_CREDIT_SUM_DEBT_AMT_CREDIT_SUM_OVERDUE',\n",
    " 'AMT_CREDIT_SUM_DEBT_DAYS_CREDIT_UPDATE',\n",
    " 'DAYS_CREDIT_ENDDATE_AMT_CREDIT_SUM',\n",
    " 'DAYS_CREDIT_ENDDATE_CREDIT_DAY_OVERDUE',\n",
    " 'DAYS_CREDIT_ENDDATE_CNT_CREDIT_PROLONG',\n",
    " 'DAYS_CREDIT_ENDDATE_BUREAU_LENGTH',\n",
    " 'DAYS_CREDIT_ENDDATE_AMT_CREDIT_SUM_OVERDUE',\n",
    " 'DAYS_CREDIT_ENDDATE_DAYS_CREDIT_UPDATE',\n",
    " 'AMT_CREDIT_SUM_CREDIT_DAY_OVERDUE',\n",
    " 'AMT_CREDIT_SUM_CNT_CREDIT_PROLONG',\n",
    " 'AMT_CREDIT_SUM_BUREAU_LENGTH',\n",
    " 'AMT_CREDIT_SUM_AMT_CREDIT_SUM_OVERDUE',\n",
    " 'AMT_CREDIT_SUM_DAYS_CREDIT_UPDATE',\n",
    " 'CREDIT_DAY_OVERDUE_CNT_CREDIT_PROLONG',\n",
    " 'CREDIT_DAY_OVERDUE_BUREAU_LENGTH',\n",
    " 'CREDIT_DAY_OVERDUE_AMT_CREDIT_SUM_OVERDUE',\n",
    " 'CREDIT_DAY_OVERDUE_DAYS_CREDIT_UPDATE',\n",
    " 'CNT_CREDIT_PROLONG_BUREAU_LENGTH',\n",
    " 'CNT_CREDIT_PROLONG_AMT_CREDIT_SUM_OVERDUE',\n",
    " 'CNT_CREDIT_PROLONG_DAYS_CREDIT_UPDATE',\n",
    " 'BUREAU_LENGTH_AMT_CREDIT_SUM_OVERDUE',\n",
    " 'BUREAU_LENGTH_DAYS_CREDIT_UPDATE',\n",
    " 'AMT_CREDIT_SUM_OVERDUE_DAYS_CREDIT_UPDATE']\n",
    "for col in trans_col:\n",
    "    print(col)\n",
    "    X_bu_fe[col] = qt.fit_transform(X_bu_fe[col].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T15:03:12.121226Z",
     "start_time": "2018-09-21T15:03:10.246Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print('start')\n",
    "label_mapping = X_Train.set_index('SK_ID_CURR').TARGET\n",
    "test_mapping = pd.Series(index=X_Test.SK_ID_CURR, data=1)\n",
    "\n",
    "#previous application\n",
    "#10001358\n",
    "max_features = 1730000\n",
    "#295\n",
    "maxlen = 13\n",
    "\n",
    "sorted_df = X_bu_fe.sort_values(['SK_ID_CURR','DAYS_CREDIT'])\n",
    "col = 'DAYS_CREDIT'\n",
    "sorted_df[col] = qt.fit_transform(sorted_df[col].values.reshape(-1,1))\n",
    "sorted_df['words'] = sorted_df.index.astype(str)\n",
    "feature = list(sorted_df.columns)\n",
    "feature.remove('SK_ID_BUREAU')\n",
    "feature.remove('SK_ID_CURR')\n",
    "feature.remove('words')\n",
    "ebd = sorted_df[feature].values\n",
    "#normalize\n",
    "print('start normalize')\n",
    "# nor_ebd = normalize(ebd, norm='max',axis=0)\n",
    "nor_ebd = ebd\n",
    "print('get embedding')\n",
    "embed_size = len(feature)\n",
    "print('ebd size is {}'.format(embed_size))\n",
    "embeddings_index = get_embeddings_index(sorted_df,nor_ebd)\n",
    "print('create document')\n",
    "train,test = create_document(sorted_df)\n",
    "print('get embedding Mat')\n",
    "x_train,x_test,y_train,embedding_matrix,num_words = get_train_ebdMat(train,test,embeddings_index)\n",
    "model_file = ENV.bureau_rnn.value\n",
    "report_file = ENV.bureau_report.value\n",
    "pred_file = ENV.bureau_preds.value\n",
    "pred_test_file = ENV.bureau_preds_test.value\n",
    "train_5_folds(model_file,report_file,pred_file,pred_test_file,batch_size=2000,total_epoch=500,patience=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T15:03:12.122329Z",
     "start_time": "2018-09-21T15:03:10.248Z"
    }
   },
   "outputs": [],
   "source": [
    "a = train.text.apply(lambda x: len(x.split(' ')))\n",
    "a.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T15:03:12.123407Z",
     "start_time": "2018-09-21T15:03:10.250Z"
    }
   },
   "outputs": [],
   "source": [
    "a = train.text.apply(lambda x: len(x.split(' ')))\n",
    "a.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
