{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T20:00:32.084262Z",
     "start_time": "2018-06-19T20:00:32.067783Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forked from excellent kernel : https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features\n",
    "# From Kaggler : https://www.kaggle.com/jsaguiar\n",
    "# Just added a few features so I thought I had to make release it as well...\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "PATH = '/home/kai/data/kaggle/homecredit/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T20:00:08.515486Z",
     "start_time": "2018-06-19T20:00:08.374588Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "# Preprocess application_train.csv and application_test.csv\n",
    "def application_train_test(num_rows = None, nan_as_category = False):\n",
    "    # Read data and merge\n",
    "    df = pd.read_csv(PATH+'application_train.csv', nrows= num_rows)\n",
    "    tl = df.shape[0]\n",
    "    test_df = pd.read_csv(PATH+'application_test.csv', nrows= num_rows)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "    df = df.append(test_df).reset_index()\n",
    "    print('2', df.TARGET[:tl].isnull().any())\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "#     df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    docs = [_f for _f in df.columns if 'FLAG_DOC' in _f]\n",
    "    live = [_f for _f in df.columns if ('FLAG_' in _f) & ('FLAG_DOC' not in _f) & ('_FLAG_' not in _f)]\n",
    "    \n",
    "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "    inc_by_org = df[['AMT_INCOME_TOTAL', 'ORGANIZATION_TYPE']].groupby('ORGANIZATION_TYPE').median()['AMT_INCOME_TOTAL']\n",
    "\n",
    "    df['NEW_CREDIT_TO_ANNUITY_RATIO'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n",
    "    df['NEW_CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
    "    df['NEW_DOC_IND_KURT'] = df[docs].kurtosis(axis=1)\n",
    "    df['NEW_LIVE_IND_SUM'] = df[live].sum(axis=1)\n",
    "    df['NEW_INC_PER_CHLD'] = df['AMT_INCOME_TOTAL'] / (1 + df['CNT_CHILDREN'])\n",
    "    df['NEW_INC_BY_ORG'] = df['ORGANIZATION_TYPE'].map(inc_by_org)\n",
    "    df['NEW_EMPLOY_TO_BIRTH_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['NEW_ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / (1 + df['AMT_INCOME_TOTAL'])\n",
    "    df['NEW_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
    "    df['NEW_EXT_SOURCES_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "    df['NEW_SCORES_STD'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "    df['NEW_SCORES_STD'] = df['NEW_SCORES_STD'].fillna(df['NEW_SCORES_STD'].mean())\n",
    "    df['NEW_CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n",
    "    df['NEW_CAR_TO_EMPLOY_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n",
    "    df['NEW_PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n",
    "    df['NEW_PHONE_TO_EMPLOY_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']\n",
    "    df['NEW_CREDIT_TO_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
    "    \n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    # Some simple new features (percentages)\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    print('traintestdone')\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T18:33:19.381200Z",
     "start_time": "2018-06-19T18:33:11.886192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 307511, test samples: 48744\n",
      "2 False\n",
      "traintestdone\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(356255, 264)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = application_train_test()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T18:33:19.505932Z",
     "start_time": "2018-06-19T18:33:19.502031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['index' 'AMT_ANNUITY' 'AMT_CREDIT' 'AMT_GOODS_PRICE' 'AMT_INCOME_TOTAL'\n",
      " 'AMT_REQ_CREDIT_BUREAU_DAY' 'AMT_REQ_CREDIT_BUREAU_HOUR'\n",
      " 'AMT_REQ_CREDIT_BUREAU_MON' 'AMT_REQ_CREDIT_BUREAU_QRT'\n",
      " 'AMT_REQ_CREDIT_BUREAU_WEEK' 'AMT_REQ_CREDIT_BUREAU_YEAR' 'APARTMENTS_AVG'\n",
      " 'APARTMENTS_MEDI' 'APARTMENTS_MODE' 'BASEMENTAREA_AVG' 'BASEMENTAREA_MEDI'\n",
      " 'BASEMENTAREA_MODE' 'CNT_CHILDREN' 'CNT_FAM_MEMBERS' 'CODE_GENDER'\n",
      " 'COMMONAREA_AVG' 'COMMONAREA_MEDI' 'COMMONAREA_MODE' 'DAYS_BIRTH'\n",
      " 'DAYS_EMPLOYED' 'DAYS_ID_PUBLISH' 'DAYS_LAST_PHONE_CHANGE'\n",
      " 'DAYS_REGISTRATION' 'DEF_30_CNT_SOCIAL_CIRCLE' 'DEF_60_CNT_SOCIAL_CIRCLE'\n",
      " 'ELEVATORS_AVG' 'ELEVATORS_MEDI' 'ELEVATORS_MODE' 'ENTRANCES_AVG'\n",
      " 'ENTRANCES_MEDI' 'ENTRANCES_MODE' 'EXT_SOURCE_1' 'EXT_SOURCE_2'\n",
      " 'EXT_SOURCE_3' 'FLAG_CONT_MOBILE' 'FLAG_DOCUMENT_10' 'FLAG_DOCUMENT_11'\n",
      " 'FLAG_DOCUMENT_12' 'FLAG_DOCUMENT_13' 'FLAG_DOCUMENT_14'\n",
      " 'FLAG_DOCUMENT_15' 'FLAG_DOCUMENT_16' 'FLAG_DOCUMENT_17'\n",
      " 'FLAG_DOCUMENT_18' 'FLAG_DOCUMENT_19' 'FLAG_DOCUMENT_2' 'FLAG_DOCUMENT_20'\n",
      " 'FLAG_DOCUMENT_21' 'FLAG_DOCUMENT_3' 'FLAG_DOCUMENT_4' 'FLAG_DOCUMENT_5'\n",
      " 'FLAG_DOCUMENT_6' 'FLAG_DOCUMENT_7' 'FLAG_DOCUMENT_8' 'FLAG_DOCUMENT_9'\n",
      " 'FLAG_EMAIL' 'FLAG_EMP_PHONE' 'FLAG_MOBIL' 'FLAG_OWN_CAR'\n",
      " 'FLAG_OWN_REALTY' 'FLAG_PHONE' 'FLAG_WORK_PHONE' 'FLOORSMAX_AVG'\n",
      " 'FLOORSMAX_MEDI' 'FLOORSMAX_MODE' 'FLOORSMIN_AVG' 'FLOORSMIN_MEDI'\n",
      " 'FLOORSMIN_MODE' 'HOUR_APPR_PROCESS_START' 'LANDAREA_AVG' 'LANDAREA_MEDI'\n",
      " 'LANDAREA_MODE' 'LIVE_CITY_NOT_WORK_CITY' 'LIVE_REGION_NOT_WORK_REGION'\n",
      " 'LIVINGAPARTMENTS_AVG' 'LIVINGAPARTMENTS_MEDI' 'LIVINGAPARTMENTS_MODE'\n",
      " 'LIVINGAREA_AVG' 'LIVINGAREA_MEDI' 'LIVINGAREA_MODE'\n",
      " 'NONLIVINGAPARTMENTS_AVG' 'NONLIVINGAPARTMENTS_MEDI'\n",
      " 'NONLIVINGAPARTMENTS_MODE' 'NONLIVINGAREA_AVG' 'NONLIVINGAREA_MEDI'\n",
      " 'NONLIVINGAREA_MODE' 'OBS_30_CNT_SOCIAL_CIRCLE' 'OBS_60_CNT_SOCIAL_CIRCLE'\n",
      " 'OWN_CAR_AGE' 'REGION_POPULATION_RELATIVE' 'REGION_RATING_CLIENT'\n",
      " 'REGION_RATING_CLIENT_W_CITY' 'REG_CITY_NOT_LIVE_CITY'\n",
      " 'REG_CITY_NOT_WORK_CITY' 'REG_REGION_NOT_LIVE_REGION'\n",
      " 'REG_REGION_NOT_WORK_REGION' 'SK_ID_CURR' 'TARGET' 'TOTALAREA_MODE'\n",
      " 'YEARS_BEGINEXPLUATATION_AVG' 'YEARS_BEGINEXPLUATATION_MEDI'\n",
      " 'YEARS_BEGINEXPLUATATION_MODE' 'YEARS_BUILD_AVG' 'YEARS_BUILD_MEDI'\n",
      " 'YEARS_BUILD_MODE' 'NEW_CREDIT_TO_ANNUITY_RATIO'\n",
      " 'NEW_CREDIT_TO_GOODS_RATIO' 'NEW_DOC_IND_KURT' 'NEW_LIVE_IND_SUM'\n",
      " 'NEW_INC_PER_CHLD' 'NEW_INC_BY_ORG' 'NEW_EMPLOY_TO_BIRTH_RATIO'\n",
      " 'NEW_ANNUITY_TO_INCOME_RATIO' 'NEW_SOURCES_PROD' 'NEW_EXT_SOURCES_MEAN'\n",
      " 'NEW_SCORES_STD' 'NEW_CAR_TO_BIRTH_RATIO' 'NEW_CAR_TO_EMPLOY_RATIO'\n",
      " 'NEW_PHONE_TO_BIRTH_RATIO' 'NEW_PHONE_TO_EMPLOY_RATIO'\n",
      " 'NEW_CREDIT_TO_INCOME_RATIO' 'EMERGENCYSTATE_MODE_No'\n",
      " 'EMERGENCYSTATE_MODE_Yes' 'FONDKAPREMONT_MODE_not specified'\n",
      " 'FONDKAPREMONT_MODE_org spec account'\n",
      " 'FONDKAPREMONT_MODE_reg oper account'\n",
      " 'FONDKAPREMONT_MODE_reg oper spec account' 'HOUSETYPE_MODE_block of flats'\n",
      " 'HOUSETYPE_MODE_specific housing' 'HOUSETYPE_MODE_terraced house'\n",
      " 'NAME_CONTRACT_TYPE_Cash loans' 'NAME_CONTRACT_TYPE_Revolving loans'\n",
      " 'NAME_EDUCATION_TYPE_Academic degree'\n",
      " 'NAME_EDUCATION_TYPE_Higher education'\n",
      " 'NAME_EDUCATION_TYPE_Incomplete higher'\n",
      " 'NAME_EDUCATION_TYPE_Lower secondary'\n",
      " 'NAME_EDUCATION_TYPE_Secondary / secondary special'\n",
      " 'NAME_FAMILY_STATUS_Civil marriage' 'NAME_FAMILY_STATUS_Married'\n",
      " 'NAME_FAMILY_STATUS_Separated' 'NAME_FAMILY_STATUS_Single / not married'\n",
      " 'NAME_FAMILY_STATUS_Unknown' 'NAME_FAMILY_STATUS_Widow'\n",
      " 'NAME_HOUSING_TYPE_Co-op apartment' 'NAME_HOUSING_TYPE_House / apartment'\n",
      " 'NAME_HOUSING_TYPE_Municipal apartment'\n",
      " 'NAME_HOUSING_TYPE_Office apartment' 'NAME_HOUSING_TYPE_Rented apartment'\n",
      " 'NAME_HOUSING_TYPE_With parents' 'NAME_INCOME_TYPE_Businessman'\n",
      " 'NAME_INCOME_TYPE_Commercial associate' 'NAME_INCOME_TYPE_Maternity leave'\n",
      " 'NAME_INCOME_TYPE_Pensioner' 'NAME_INCOME_TYPE_State servant'\n",
      " 'NAME_INCOME_TYPE_Student' 'NAME_INCOME_TYPE_Unemployed'\n",
      " 'NAME_INCOME_TYPE_Working' 'NAME_TYPE_SUITE_Children'\n",
      " 'NAME_TYPE_SUITE_Family' 'NAME_TYPE_SUITE_Group of people'\n",
      " 'NAME_TYPE_SUITE_Other_A' 'NAME_TYPE_SUITE_Other_B'\n",
      " 'NAME_TYPE_SUITE_Spouse, partner' 'NAME_TYPE_SUITE_Unaccompanied'\n",
      " 'OCCUPATION_TYPE_Accountants' 'OCCUPATION_TYPE_Cleaning staff'\n",
      " 'OCCUPATION_TYPE_Cooking staff' 'OCCUPATION_TYPE_Core staff'\n",
      " 'OCCUPATION_TYPE_Drivers' 'OCCUPATION_TYPE_HR staff'\n",
      " 'OCCUPATION_TYPE_High skill tech staff' 'OCCUPATION_TYPE_IT staff'\n",
      " 'OCCUPATION_TYPE_Laborers' 'OCCUPATION_TYPE_Low-skill Laborers'\n",
      " 'OCCUPATION_TYPE_Managers' 'OCCUPATION_TYPE_Medicine staff'\n",
      " 'OCCUPATION_TYPE_Private service staff' 'OCCUPATION_TYPE_Realty agents'\n",
      " 'OCCUPATION_TYPE_Sales staff' 'OCCUPATION_TYPE_Secretaries'\n",
      " 'OCCUPATION_TYPE_Security staff' 'OCCUPATION_TYPE_Waiters/barmen staff'\n",
      " 'ORGANIZATION_TYPE_Advertising' 'ORGANIZATION_TYPE_Agriculture'\n",
      " 'ORGANIZATION_TYPE_Bank' 'ORGANIZATION_TYPE_Business Entity Type 1'\n",
      " 'ORGANIZATION_TYPE_Business Entity Type 2'\n",
      " 'ORGANIZATION_TYPE_Business Entity Type 3' 'ORGANIZATION_TYPE_Cleaning'\n",
      " 'ORGANIZATION_TYPE_Construction' 'ORGANIZATION_TYPE_Culture'\n",
      " 'ORGANIZATION_TYPE_Electricity' 'ORGANIZATION_TYPE_Emergency'\n",
      " 'ORGANIZATION_TYPE_Government' 'ORGANIZATION_TYPE_Hotel'\n",
      " 'ORGANIZATION_TYPE_Housing' 'ORGANIZATION_TYPE_Industry: type 1'\n",
      " 'ORGANIZATION_TYPE_Industry: type 10'\n",
      " 'ORGANIZATION_TYPE_Industry: type 11'\n",
      " 'ORGANIZATION_TYPE_Industry: type 12'\n",
      " 'ORGANIZATION_TYPE_Industry: type 13' 'ORGANIZATION_TYPE_Industry: type 2'\n",
      " 'ORGANIZATION_TYPE_Industry: type 3' 'ORGANIZATION_TYPE_Industry: type 4'\n",
      " 'ORGANIZATION_TYPE_Industry: type 5' 'ORGANIZATION_TYPE_Industry: type 6'\n",
      " 'ORGANIZATION_TYPE_Industry: type 7' 'ORGANIZATION_TYPE_Industry: type 8'\n",
      " 'ORGANIZATION_TYPE_Industry: type 9' 'ORGANIZATION_TYPE_Insurance'\n",
      " 'ORGANIZATION_TYPE_Kindergarten' 'ORGANIZATION_TYPE_Legal Services'\n",
      " 'ORGANIZATION_TYPE_Medicine' 'ORGANIZATION_TYPE_Military'\n",
      " 'ORGANIZATION_TYPE_Mobile' 'ORGANIZATION_TYPE_Other'\n",
      " 'ORGANIZATION_TYPE_Police' 'ORGANIZATION_TYPE_Postal'\n",
      " 'ORGANIZATION_TYPE_Realtor' 'ORGANIZATION_TYPE_Religion'\n",
      " 'ORGANIZATION_TYPE_Restaurant' 'ORGANIZATION_TYPE_School'\n",
      " 'ORGANIZATION_TYPE_Security' 'ORGANIZATION_TYPE_Security Ministries'\n",
      " 'ORGANIZATION_TYPE_Self-employed' 'ORGANIZATION_TYPE_Services'\n",
      " 'ORGANIZATION_TYPE_Telecom' 'ORGANIZATION_TYPE_Trade: type 1'\n",
      " 'ORGANIZATION_TYPE_Trade: type 2' 'ORGANIZATION_TYPE_Trade: type 3'\n",
      " 'ORGANIZATION_TYPE_Trade: type 4' 'ORGANIZATION_TYPE_Trade: type 5'\n",
      " 'ORGANIZATION_TYPE_Trade: type 6' 'ORGANIZATION_TYPE_Trade: type 7'\n",
      " 'ORGANIZATION_TYPE_Transport: type 1'\n",
      " 'ORGANIZATION_TYPE_Transport: type 2'\n",
      " 'ORGANIZATION_TYPE_Transport: type 3'\n",
      " 'ORGANIZATION_TYPE_Transport: type 4' 'ORGANIZATION_TYPE_University'\n",
      " 'ORGANIZATION_TYPE_XNA' 'WALLSMATERIAL_MODE_Block'\n",
      " 'WALLSMATERIAL_MODE_Mixed' 'WALLSMATERIAL_MODE_Monolithic'\n",
      " 'WALLSMATERIAL_MODE_Others' 'WALLSMATERIAL_MODE_Panel'\n",
      " 'WALLSMATERIAL_MODE_Stone, brick' 'WALLSMATERIAL_MODE_Wooden'\n",
      " 'WEEKDAY_APPR_PROCESS_START_FRIDAY' 'WEEKDAY_APPR_PROCESS_START_MONDAY'\n",
      " 'WEEKDAY_APPR_PROCESS_START_SATURDAY' 'WEEKDAY_APPR_PROCESS_START_SUNDAY'\n",
      " 'WEEKDAY_APPR_PROCESS_START_THURSDAY' 'WEEKDAY_APPR_PROCESS_START_TUESDAY'\n",
      " 'WEEKDAY_APPR_PROCESS_START_WEDNESDAY' 'DAYS_EMPLOYED_PERC'\n",
      " 'INCOME_CREDIT_PERC' 'INCOME_PER_PERSON' 'ANNUITY_INCOME_PERC'\n",
      " 'PAYMENT_RATE']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T18:33:19.733870Z",
     "start_time": "2018-06-19T18:33:19.626759Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def bureau_and_balance(num_rows = None, nan_as_category = True):\n",
    "    bureau = pd.read_csv(PATH+'bureau.csv', nrows = num_rows)\n",
    "    bb = pd.read_csv(PATH+'bureau_balance.csv', nrows = num_rows)\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    \n",
    "    active_agg = active_agg.reset_index()\n",
    "    bureau_agg = bureau_agg.reset_index()\n",
    "    bureau_agg = bureau_agg.merge(active_agg, how='left', on='SK_ID_CURR')\n",
    "    print('another',bureau_agg.shape)\n",
    "    \n",
    "    \n",
    "#     bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    print('bbdone',bureau_agg.shape)\n",
    "    return bureau_agg\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-19T18:33:13.218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another (305811, 90)\n",
      "bbdone (305811, 117)\n",
      "Bureau df shape: (305811, 117)\n"
     ]
    }
   ],
   "source": [
    "bureau = bureau_and_balance()\n",
    "print(\"Bureau df shape:\", bureau.shape)\n",
    "df = df.merge(bureau, how='left', on='SK_ID_CURR')\n",
    "del bureau\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-19T18:33:13.710Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess previous_applications.csv\n",
    "def previous_applications(num_rows = None, nan_as_category = True):\n",
    "    print('prevbegin')\n",
    "    prev = pd.read_csv(PATH+'previous_application.csv', nrows = num_rows)\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    \n",
    "    approved_agg.reset_index(inplace = True)\n",
    "    prev_agg.reset_index(inplace = True)\n",
    "    print('beginagg')\n",
    "    prev_agg = prev_agg.merge(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    print('prevdone')\n",
    "    return prev_agg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-19T18:33:14.423Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " prev = previous_applications()\n",
    "print(\"Previous applications df shape:\", prev.shape)\n",
    "df = df.merge(prev, how='left', on='SK_ID_CURR')\n",
    "del prev\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-19T18:33:15.022Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess POS_CASH_balance.csv\n",
    "def pos_cash(num_rows = None, nan_as_category = True):\n",
    "    print('posbegin')\n",
    "    pos = pd.read_csv(PATH+'POS_CASH_balance.csv', nrows = num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    # Count pos cash accounts\n",
    "    pos_agg.reset_index(inplace = True)\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    print('pos')\n",
    "    return pos_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-19T18:33:15.317Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = pos_cash()\n",
    "print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "df = df.merge(pos, how='left', on='SK_ID_CURR')\n",
    "del pos\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-19T18:33:15.834Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "# Preprocess installments_payments.csv\n",
    "def installments_payments(num_rows = None, nan_as_category = True):\n",
    "    print('insbegin')\n",
    "    ins = pd.read_csv(PATH+'installments_payments.csv', nrows = num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    # Count installments accounts\n",
    "    ins_agg.reset_index(inplace = True)\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    print('ins')\n",
    "    return ins_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-19T18:33:16.494Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ins = installments_payments()\n",
    "print(\"Installments payments df shape:\", ins.shape)\n",
    "df = df.merge(ins, how='left', on='SK_ID_CURR')\n",
    "del ins\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-19T18:33:16.964Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess credit_card_balance.csv\n",
    "def credit_card_balance(num_rows = None, nan_as_category = True):\n",
    "    print('creditbegin')\n",
    "    cc = pd.read_csv(PATH+'credit_card_balance.csv', nrows = num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    # Count credit card lines\n",
    "    cc.reset_index(inplace = True)\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    print('credit')\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-19T18:33:17.823Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc = credit_card_balance()\n",
    "print(\"Credit card balance df shape:\", cc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-19T18:33:18.411Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc.reset_index(inplace = True)\n",
    "df = df.merge(cc, how='left', on='SK_ID_CURR')\n",
    "del cc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T18:47:28.769803Z",
     "start_time": "2018-06-21T18:47:26.187633Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forked from excellent kernel : https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features\n",
    "# From Kaggler : https://www.kaggle.com/jsaguiar\n",
    "# Just added a few features so I thought I had to make release it as well...\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "PATH = '/home/kai/data/kaggle/homecredit/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T18:40:12.346223Z",
     "start_time": "2018-06-20T18:40:12.335316Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T18:47:44.969835Z",
     "start_time": "2018-06-21T18:47:29.611359Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(PATH+'train_0.pkl')\n",
    "test_df = pd.read_pickle(PATH+'test_0.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T18:40:17.262689Z",
     "start_time": "2018-06-20T18:40:17.259026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307857, 899) (48398, 898)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T18:53:08.414863Z",
     "start_time": "2018-06-21T18:53:08.409725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 2039) (48744, 2038)\n"
     ]
    }
   ],
   "source": [
    "# df = train_df.append(test_df).reset_index()\n",
    "# for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "#         df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "#     # Categorical features with One-Hot encode\n",
    "# print(df.shape)\n",
    "# df, _ = one_hot_encoder(df, True)\n",
    "# train_df = df[df['TARGET'].notnull()]\n",
    "# test_df = df[df['TARGET'].isnull()]\n",
    "print(train_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T18:48:45.305537Z",
     "start_time": "2018-06-21T18:48:45.279925Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['index'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-35c2909e74a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# test_df = test_df.drop(['index','TARGET'],axis = 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, level, inplace, errors)\u001b[0m\n\u001b[1;32m   2159\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2161\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3622\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3623\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[0;32m-> 3624\u001b[0;31m                                  labels[mask])\n\u001b[0m\u001b[1;32m   3625\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3626\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['index'] not contained in axis"
     ]
    }
   ],
   "source": [
    "# train_df = train_df.drop('index',axis = 1)\n",
    "# test_df = test_df.drop(['index','TARGET'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T20:12:24.547279Z",
     "start_time": "2018-06-21T18:53:17.040792Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.760527\tvalid_1's auc: 0.751032\n",
      "[200]\ttraining's auc: 0.788594\tvalid_1's auc: 0.77259\n",
      "[300]\ttraining's auc: 0.808205\tvalid_1's auc: 0.784745\n",
      "[400]\ttraining's auc: 0.821226\tvalid_1's auc: 0.790497\n",
      "[500]\ttraining's auc: 0.831474\tvalid_1's auc: 0.79358\n",
      "[600]\ttraining's auc: 0.84042\tvalid_1's auc: 0.79549\n",
      "[700]\ttraining's auc: 0.848168\tvalid_1's auc: 0.796598\n",
      "[800]\ttraining's auc: 0.855006\tvalid_1's auc: 0.797525\n",
      "[900]\ttraining's auc: 0.861438\tvalid_1's auc: 0.798154\n",
      "[1000]\ttraining's auc: 0.8673\tvalid_1's auc: 0.798507\n",
      "[1100]\ttraining's auc: 0.873105\tvalid_1's auc: 0.798727\n",
      "[1200]\ttraining's auc: 0.878639\tvalid_1's auc: 0.798806\n",
      "[1300]\ttraining's auc: 0.883912\tvalid_1's auc: 0.799006\n",
      "[1400]\ttraining's auc: 0.888785\tvalid_1's auc: 0.799236\n",
      "[1500]\ttraining's auc: 0.893371\tvalid_1's auc: 0.799177\n",
      "Early stopping, best iteration is:\n",
      "[1396]\ttraining's auc: 0.888598\tvalid_1's auc: 0.799259\n",
      "Fold  1 AUC : 0.799262\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.76094\tvalid_1's auc: 0.743623\n",
      "[200]\ttraining's auc: 0.789907\tvalid_1's auc: 0.766064\n",
      "[300]\ttraining's auc: 0.809406\tvalid_1's auc: 0.778204\n",
      "[400]\ttraining's auc: 0.822371\tvalid_1's auc: 0.783887\n",
      "[500]\ttraining's auc: 0.832705\tvalid_1's auc: 0.786882\n",
      "[600]\ttraining's auc: 0.84178\tvalid_1's auc: 0.788664\n",
      "[700]\ttraining's auc: 0.849312\tvalid_1's auc: 0.789781\n",
      "[800]\ttraining's auc: 0.856335\tvalid_1's auc: 0.790538\n",
      "[900]\ttraining's auc: 0.862395\tvalid_1's auc: 0.79096\n",
      "[1000]\ttraining's auc: 0.868212\tvalid_1's auc: 0.79139\n",
      "[1100]\ttraining's auc: 0.873927\tvalid_1's auc: 0.79158\n",
      "[1200]\ttraining's auc: 0.879481\tvalid_1's auc: 0.79148\n",
      "[1300]\ttraining's auc: 0.884339\tvalid_1's auc: 0.791656\n",
      "[1400]\ttraining's auc: 0.889024\tvalid_1's auc: 0.791585\n",
      "[1500]\ttraining's auc: 0.893945\tvalid_1's auc: 0.791758\n",
      "[1600]\ttraining's auc: 0.898217\tvalid_1's auc: 0.791864\n",
      "[1700]\ttraining's auc: 0.90235\tvalid_1's auc: 0.791839\n",
      "[1800]\ttraining's auc: 0.906368\tvalid_1's auc: 0.791758\n",
      "Early stopping, best iteration is:\n",
      "[1631]\ttraining's auc: 0.899468\tvalid_1's auc: 0.791933\n",
      "Fold  2 AUC : 0.791924\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.758468\tvalid_1's auc: 0.749467\n",
      "[200]\ttraining's auc: 0.788936\tvalid_1's auc: 0.771014\n",
      "[300]\ttraining's auc: 0.8084\tvalid_1's auc: 0.782692\n",
      "[400]\ttraining's auc: 0.821607\tvalid_1's auc: 0.788266\n",
      "[500]\ttraining's auc: 0.831934\tvalid_1's auc: 0.791573\n",
      "[600]\ttraining's auc: 0.840647\tvalid_1's auc: 0.793412\n",
      "[700]\ttraining's auc: 0.84839\tvalid_1's auc: 0.79444\n",
      "[800]\ttraining's auc: 0.855486\tvalid_1's auc: 0.795117\n",
      "[900]\ttraining's auc: 0.86215\tvalid_1's auc: 0.795599\n",
      "[1000]\ttraining's auc: 0.868511\tvalid_1's auc: 0.796036\n",
      "[1100]\ttraining's auc: 0.874087\tvalid_1's auc: 0.796325\n",
      "[1200]\ttraining's auc: 0.879472\tvalid_1's auc: 0.796435\n",
      "[1300]\ttraining's auc: 0.884688\tvalid_1's auc: 0.796574\n",
      "[1400]\ttraining's auc: 0.889541\tvalid_1's auc: 0.796729\n",
      "[1500]\ttraining's auc: 0.894158\tvalid_1's auc: 0.796701\n",
      "[1600]\ttraining's auc: 0.898607\tvalid_1's auc: 0.796708\n",
      "[1700]\ttraining's auc: 0.902918\tvalid_1's auc: 0.79666\n",
      "Early stopping, best iteration is:\n",
      "[1554]\ttraining's auc: 0.896506\tvalid_1's auc: 0.796808\n",
      "Fold  3 AUC : 0.796794\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.760347\tvalid_1's auc: 0.749206\n",
      "[200]\ttraining's auc: 0.789549\tvalid_1's auc: 0.77038\n",
      "[300]\ttraining's auc: 0.808968\tvalid_1's auc: 0.781385\n",
      "[400]\ttraining's auc: 0.822092\tvalid_1's auc: 0.786935\n",
      "[500]\ttraining's auc: 0.832467\tvalid_1's auc: 0.789838\n",
      "[600]\ttraining's auc: 0.841016\tvalid_1's auc: 0.791591\n",
      "[700]\ttraining's auc: 0.848711\tvalid_1's auc: 0.792499\n",
      "[800]\ttraining's auc: 0.855491\tvalid_1's auc: 0.79327\n",
      "[900]\ttraining's auc: 0.861703\tvalid_1's auc: 0.793661\n",
      "[1000]\ttraining's auc: 0.867641\tvalid_1's auc: 0.79435\n",
      "[1100]\ttraining's auc: 0.873098\tvalid_1's auc: 0.794643\n",
      "[1200]\ttraining's auc: 0.878708\tvalid_1's auc: 0.794894\n",
      "[1300]\ttraining's auc: 0.883475\tvalid_1's auc: 0.795104\n",
      "[1400]\ttraining's auc: 0.888425\tvalid_1's auc: 0.795231\n",
      "[1500]\ttraining's auc: 0.893013\tvalid_1's auc: 0.795074\n",
      "[1600]\ttraining's auc: 0.89748\tvalid_1's auc: 0.795058\n",
      "Early stopping, best iteration is:\n",
      "[1409]\ttraining's auc: 0.888923\tvalid_1's auc: 0.795265\n",
      "Fold  4 AUC : 0.795256\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.7617\tvalid_1's auc: 0.742381\n",
      "[200]\ttraining's auc: 0.790254\tvalid_1's auc: 0.764407\n",
      "[300]\ttraining's auc: 0.809589\tvalid_1's auc: 0.776663\n",
      "[400]\ttraining's auc: 0.822826\tvalid_1's auc: 0.78278\n",
      "[500]\ttraining's auc: 0.833085\tvalid_1's auc: 0.786117\n",
      "[600]\ttraining's auc: 0.842107\tvalid_1's auc: 0.788266\n",
      "[700]\ttraining's auc: 0.849616\tvalid_1's auc: 0.789704\n",
      "[800]\ttraining's auc: 0.856801\tvalid_1's auc: 0.790697\n",
      "[900]\ttraining's auc: 0.86319\tvalid_1's auc: 0.791248\n",
      "[1000]\ttraining's auc: 0.869232\tvalid_1's auc: 0.791822\n",
      "[1100]\ttraining's auc: 0.874797\tvalid_1's auc: 0.792103\n",
      "[1200]\ttraining's auc: 0.879975\tvalid_1's auc: 0.792375\n",
      "[1300]\ttraining's auc: 0.885054\tvalid_1's auc: 0.792532\n",
      "[1400]\ttraining's auc: 0.889813\tvalid_1's auc: 0.792725\n",
      "[1500]\ttraining's auc: 0.894483\tvalid_1's auc: 0.792555\n",
      "[1600]\ttraining's auc: 0.898786\tvalid_1's auc: 0.792593\n",
      "Early stopping, best iteration is:\n",
      "[1431]\ttraining's auc: 0.891274\tvalid_1's auc: 0.792729\n",
      "Fold  5 AUC : 0.792748\n",
      "Full AUC score 0.795131\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "# Cross validation model\n",
    "stratified = False\n",
    "debug = False\n",
    "\n",
    "if stratified:\n",
    "    folds = StratifiedKFold(n_splits= 5, shuffle=True, random_state=1001)\n",
    "else:\n",
    "    folds = KFold(n_splits= 5, shuffle=True, random_state=45)\n",
    "# Create arrays and dataframes to store results\n",
    "oof_preds = np.zeros(train_df.shape[0])\n",
    "sub_preds = np.zeros(test_df.shape[0])\n",
    "feature_importance_df = pd.DataFrame()\n",
    "feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "    train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "    valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "    # LightGBM parameters found by Bayesian optimization\n",
    "    clf = LGBMClassifier(\n",
    "        nthread=16,\n",
    "        n_estimators=10000,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=34,\n",
    "        colsample_bytree=0.9497036,\n",
    "        subsample=0.8715623,\n",
    "        max_depth=8,\n",
    "        reg_alpha=0.041545473,\n",
    "        reg_lambda=0.0735294,\n",
    "        min_split_gain=0.0222415,\n",
    "        min_child_weight=39.3259775,\n",
    "        silent=-1,\n",
    "        verbose=-1, )\n",
    "\n",
    "    clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "        eval_metric= 'auc', verbose= 100, early_stopping_rounds= 200)\n",
    "\n",
    "    oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "    sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "#     fold_importance_df = pd.DataFrame()\n",
    "#     fold_importance_df[\"feature\"] = feats\n",
    "# #     fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "#     fold_importance_df[\"fold\"] = n_fold + 1\n",
    "#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "    del clf, train_x, train_y, valid_x, valid_y\n",
    "    gc.collect()\n",
    "\n",
    "print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "# Write submission file and plot feature importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T20:14:12.247118Z",
     "start_time": "2018-06-21T20:14:11.346979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# # Display/plot feature importance\n",
    "# def display_importances(feature_importance_df_):\n",
    "#     cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "#     best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "#     plt.figure(figsize=(8, 10))\n",
    "#     sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "#     plt.title('LightGBM Features (avg over folds)')\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('lgbm_importances01.png')\n",
    "debug = False    \n",
    "if not debug:\n",
    "    print('a')\n",
    "    test_df['TARGET'] = sub_preds\n",
    "    test_df[['SK_ID_CURR', 'TARGET']].to_csv(PATH+'inter/06_21_total_1.csv', index= False)\n",
    "# display_importances(feature_importance_df)\n",
    "\n",
    "df =test_df[['SK_ID_CURR', 'TARGET']] \n",
    "print(len(df[df['TARGET']<0]),len(df[df['TARGET']>1/2]))\n",
    "\n",
    "train_df.to_pickle(PATH + 'train_original_795.pkl')\n",
    "test_df.to_pickle(PATH + 'test_original_795.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T20:14:16.820914Z",
     "start_time": "2018-06-21T20:14:16.810612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 217\n"
     ]
    }
   ],
   "source": [
    "df =test_df[['SK_ID_CURR', 'TARGET']] \n",
    "print(len(df[df['TARGET']<0]),len(df[df['TARGET']>1/2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T17:24:36.998247Z",
     "start_time": "2018-06-19T17:24:29.765020Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.to_pickle(PATH + 'train_original_795.pkl')\n",
    "test_df.to_pickle(PATH + 'test_original_795.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T21:10:58.094478Z",
     "start_time": "2018-06-18T21:10:57.967636Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kfold_lightgbm(df, num_folds, stratified = False, debug= False):\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "    del df\n",
    "    gc.collect()\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        # LightGBM parameters found by Bayesian optimization\n",
    "        clf = LGBMClassifier(\n",
    "            nthread=8,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1, )\n",
    "\n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "            eval_metric= 'auc', verbose= 100, early_stopping_rounds= 200)\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "    # Write submission file and plot feature importance\n",
    "    if not debug:\n",
    "        test_df['TARGET'] = sub_preds\n",
    "        submission_file_name = \"original.pkl\"\n",
    "        test_df[['SK_ID_CURR', 'TARGET']].to_pickle(Path + submission_file_name)\n",
    "    display_importances(feature_importance_df)\n",
    "    return feature_importance_df\n",
    "\n",
    "# Display/plot feature importance\n",
    "# def display_importances(feature_importance_df_):\n",
    "#     cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "#     best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "#     plt.figure(figsize=(8, 10))\n",
    "#     sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "#     plt.title('LightGBM Features (avg over folds)')\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('lgbm_importances01.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(debug = False):\n",
    "#     num_rows = 10000 if debug else None\n",
    "#     df = application_train_test(num_rows)\n",
    "#     with timer(\"Process bureau and bureau_balance\"):\n",
    "#         bureau = bureau_and_balance(num_rows)\n",
    "#         print(\"Bureau df shape:\", bureau.shape)\n",
    "#         df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "#         del bureau\n",
    "#         gc.collect()\n",
    "#     with timer(\"Process previous_applications\"):\n",
    "#         prev = previous_applications(num_rows)\n",
    "#         print(\"Previous applications df shape:\", prev.shape)\n",
    "#         df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "#         del prev\n",
    "#         gc.collect()\n",
    "#     with timer(\"Process POS-CASH balance\"):\n",
    "#         pos = pos_cash(num_rows)\n",
    "#         print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "#         df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "#         del pos\n",
    "#         gc.collect()\n",
    "#     with timer(\"Process installments payments\"):\n",
    "#         ins = installments_payments(num_rows)\n",
    "#         print(\"Installments payments df shape:\", ins.shape)\n",
    "#         df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "#         del ins\n",
    "#         gc.collect()\n",
    "#     with timer(\"Process credit card balance\"):\n",
    "#         cc = credit_card_balance(num_rows)\n",
    "#         print(\"Credit card balance df shape:\", cc.shape)\n",
    "#         df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "#         del cc\n",
    "#         gc.collect()\n",
    "#     with timer(\"Run LightGBM with kfold\"):\n",
    "#         feat_importance = kfold_lightgbm(df, num_folds= 5, stratified= False, debug= debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    submission_file_name = \"submission_kernel02.csv\"\n",
    "    with timer(\"Full model run\"):\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
