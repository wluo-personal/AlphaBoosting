{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T20:04:38.698125Z",
     "start_time": "2018-06-28T20:04:36.506433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/kai/anaconda3/envs/tf_gpu/lib/python35.zip', '/home/kai/anaconda3/envs/tf_gpu/lib/python3.5', '/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/plat-linux', '/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/lib-dynload', '/home/kai/.local/lib/python3.5/site-packages', '/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages', '/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/IPython/extensions', '/home/kai/.ipython', '/home/kai/data/shiyi/AlphaBoosting/automl']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "#nb_dir = os.path.split(os.getcwd())[0]\n",
    "#if nb_dir not in sys.path:\n",
    "#    sys.path.append(nb_dir)\n",
    "    \n",
    "autolib_dir = '/home/kai/data/shiyi/AlphaBoosting/automl'\n",
    "if autolib_dir not in sys.path:\n",
    "    sys.path.append(autolib_dir)\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from automl_libs import feature_engineering as fe\n",
    "from automl_libs import encoding, kernels, nn_libs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T20:04:38.722634Z",
     "start_time": "2018-06-28T20:04:38.699761Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = ['Month', 'DayofMonth', 'DayOfWeek']#, 'DepTime', 'UniqueCarrier', 'Origin', 'Dest', 'Distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T20:04:38.752753Z",
     "start_time": "2018-06-28T20:04:38.724465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'feature_cols': ['Month', 'DayofMonth'],\n",
       "  'function': <function automl_libs.feature_engineering.count(df, cols, dummy_col, generated_feature_name, params=None)>,\n",
       "  'params': {}},\n",
       " {'feature_cols': ['Month', 'DayOfWeek'],\n",
       "  'function': <function automl_libs.feature_engineering.count(df, cols, dummy_col, generated_feature_name, params=None)>,\n",
       "  'params': {}},\n",
       " {'feature_cols': ['DayofMonth', 'DayOfWeek'],\n",
       "  'function': <function automl_libs.feature_engineering.count(df, cols, dummy_col, generated_feature_name, params=None)>,\n",
       "  'params': {}},\n",
       " {'feature_cols': ['Month', 'DayofMonth', 'DayOfWeek'],\n",
       "  'function': <function automl_libs.feature_engineering.count(df, cols, dummy_col, generated_feature_name, params=None)>,\n",
       "  'params': {}}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "#params = {'split_col': 't', 'coefficient': 10, 'n': 2, 'fillna': 22}\n",
    "def get_features_to_gen(function_list):\n",
    "    features_to_gen = []\n",
    "    for function in function_list:\n",
    "        for i in range(2, len(categorical_features)+1):\n",
    "            for combine in itertools.combinations(categorical_features, i):\n",
    "                if function.__name__ == 'count_std_over_mean':\n",
    "                    features_to_gen.append({'params': {'coefficient': 10}, 'function': function, 'feature_cols': list(combine)})\n",
    "                else:\n",
    "                    features_to_gen.append({'params': {}, 'function': function, 'feature_cols': list(combine)})\n",
    "                \n",
    "    return features_to_gen\n",
    "\n",
    "#features_to_gen = get_features_to_gen([fe.count, fe.unique_count, fe.cumulative_count,\n",
    "#                                      fe.reverse_cumulative_count, fe.variance, fe.count_std_over_mean])\n",
    "features_to_gen = get_features_to_gen([fe.count])\n",
    "\n",
    "features_to_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T20:04:38.774112Z",
     "start_time": "2018-06-28T20:04:38.754149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_to_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T20:04:38.802124Z",
     "start_time": "2018-06-28T20:04:38.776119Z"
    }
   },
   "outputs": [],
   "source": [
    "project_path = '/home/kai/data/shiyi/AlphaBoosting/automl/automl_app/project1/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T20:04:38.832035Z",
     "start_time": "2018-06-28T20:04:38.804117Z"
    }
   },
   "outputs": [],
   "source": [
    "from automl_app import logger_config\n",
    "\n",
    "logger_config.config(project_path+'log_project1.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T22:52:07.852405Z",
     "start_time": "2018-06-28T22:52:07.812981Z"
    }
   },
   "outputs": [],
   "source": [
    "def gs_params_gen(model='lgb'):\n",
    "    import time\n",
    "    import numpy as np\n",
    "    seed = int(time.time()* 1000000) % 45234634\n",
    "    np.random.seed(seed)\n",
    "    if model == 'lgb':\n",
    "        params = {    \n",
    "            'objective': 'binary',\n",
    "            'boosting': 'gbdt',\n",
    "            'num_boost_round': 50, # ignored in params. extract it and put it in input arguments in train or cv explicitly\n",
    "                                   # seems to work fine as the upper limit when combined with early_stopping_round\n",
    "            'learning_rate': np.random.choice([0.1,0.03]),#0.001]),\n",
    "            'num_leaves': np.random.choice([15,31]),#,61,127]),\n",
    "            'num_threads': 8, # best speed: set to number of real cpu cores, which is vCPU/2\n",
    "            'max_depth': -1, # no limit. This is used to deal with over-fitting when #data is small.\n",
    "            'min_data_in_leaf': np.random.randint(20,50),  #minimal number of data in one leaf. \n",
    "            'feature_fraction': np.random.randint(9,11)/10,\n",
    "            'feature_fraction_seed': seed,\n",
    "            'early_stopping_round':3,\n",
    "            'bagging_freq': 1, #0 means disable bagging. k: perform bagging at every k iteration\n",
    "            'bagging_fraction': np.random.randint(4,11)/10, #Randomly select part of data \n",
    "            'bagging_seed': seed,\n",
    "            'scale_pos_weight': 1,\n",
    "            'metric' : 'auc' \n",
    "        }\n",
    "    elif model == 'nn':\n",
    "        params = {\n",
    "            'nn_seed': int(time.time() * 1000000) % 45234634,\n",
    "            'ep_for_lr': 1,\n",
    "            'lr_init': 0.01,\n",
    "            'lr_fin': 0.005, # if == lr_init, then no lr decay\n",
    "            'batch_size': np.random.choice([32, 64, 128, 256, 512, 1024, 2048]),\n",
    "            \"pred_batch_size\": 100000,\n",
    "            'max_ep': 1,\n",
    "            'patience': 5,\n",
    "            'cat_emb_outdim': 50, # could be a constant or a dict (col name:embed out dim). e.g.:\n",
    "                                  # embed_outdim = [3, 3, 8, 8, 3]\n",
    "                                  # embed_outdim_dict = dict(zip(X_train.columns.values, embed_outdim))\n",
    "            'dense_units': 50,\n",
    "            'num_dense_n_layers': 1,\n",
    "            'drop_rate': 0.2,\n",
    "            'combined_dense_n_layers': 1\n",
    "        }\n",
    "    return params, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T22:52:08.806700Z",
     "start_time": "2018-06-28T22:52:08.774140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'bagging_fraction': 0.6,\n",
       "  'bagging_freq': 1,\n",
       "  'bagging_seed': 43981441,\n",
       "  'boosting': 'gbdt',\n",
       "  'early_stopping_round': 3,\n",
       "  'feature_fraction': 0.9,\n",
       "  'feature_fraction_seed': 43981441,\n",
       "  'learning_rate': 0.03,\n",
       "  'max_depth': -1,\n",
       "  'metric': 'auc',\n",
       "  'min_data_in_leaf': 21,\n",
       "  'num_boost_round': 50,\n",
       "  'num_leaves': 31,\n",
       "  'num_threads': 8,\n",
       "  'objective': 'binary',\n",
       "  'scale_pos_weight': 1},\n",
       " 43981441)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_params_gen('lgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T22:55:48.540763Z",
     "start_time": "2018-06-28T22:54:25.653007Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 18:54:25 | INFO  | automl_app.app.AlphaBoosting  | __init__   | #66  | Run record file [./output/last_run_record.json] found. Continue from the previous run...\n",
      "2018-06-28 18:54:25 | INFO  | automl_app.app.AlphaBoosting  | __init__   | #95  | generate todo list\n",
      "2018-06-28 18:54:25 | INFO  | automl_app.app.AlphaBoosting  | __init__   | #111 | STAGE: FEATURE_ENGINEERING\n",
      "2018-06-28 18:54:25 | INFO  | automl_app.app.AlphaBoosting  | __init__   | #115 | STAGE: VALIDATION_DOWNSAMPLING_GEN\n",
      "2018-06-28 18:54:25 | INFO  | automl_app.app.AlphaBoosting  | __init__   | #119 | STAGE: CONCAT_DATA\n",
      "2018-06-28 18:54:25 | INFO  | automl_app.app.AlphaBoosting  | __init__   | #123 | STAGE: GRID_SEARCH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 3 rounds.\n",
      "[20]\ttraining's auc: 0.725345\tvalid_1's auc: 0.721444\n",
      "[40]\ttraining's auc: 0.731653\tvalid_1's auc: 0.727403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 18:54:33 | DEBUG | automl_libs.grid_search       | _lgb_gs    | #101 | [do_preds] is True, generating predictions ...\n",
      "2018-06-28 18:54:33 | DEBUG | automl_libs.grid_search       | _lgb_gs    | #102 | Retrain model using best_round and all data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\ttraining's auc: 0.734732\tvalid_1's auc: 0.730163\n",
      "[10]\ttraining's auc: 0.719989\n",
      "[20]\ttraining's auc: 0.725223\n",
      "[30]\ttraining's auc: 0.728637\n",
      "[40]\ttraining's auc: 0.731797\n",
      "[50]\ttraining's auc: 0.734655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 18:54:40 | INFO  | automl_libs.grid_search       | _lgb_gs    | #113 | LGB predictions(1530226467) saved in ./output/gs_saved_preds/.\n",
      "2018-06-28 18:54:40 | DEBUG | automl_libs.grid_search       | gs         | #52  | ./output/lgb_grid_search.csv updated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 3 rounds.\n",
      "[20]\ttraining's auc: 0.718231\tvalid_1's auc: 0.715473\n",
      "[40]\ttraining's auc: 0.723992\tvalid_1's auc: 0.721111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 18:54:47 | DEBUG | automl_libs.grid_search       | _lgb_gs    | #101 | [do_preds] is True, generating predictions ...\n",
      "2018-06-28 18:54:47 | DEBUG | automl_libs.grid_search       | _lgb_gs    | #102 | Retrain model using best_round and all data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\ttraining's auc: 0.727073\tvalid_1's auc: 0.724007\n",
      "[10]\ttraining's auc: 0.714092\n",
      "[20]\ttraining's auc: 0.717798\n",
      "[30]\ttraining's auc: 0.721499\n",
      "[40]\ttraining's auc: 0.723418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 18:54:54 | INFO  | automl_libs.grid_search       | _lgb_gs    | #113 | LGB predictions(1530226481) saved in ./output/gs_saved_preds/.\n",
      "2018-06-28 18:54:54 | DEBUG | automl_libs.grid_search       | gs         | #52  | ./output/lgb_grid_search.csv updated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttraining's auc: 0.726572\n",
      "Training until validation scores don't improve for 3 rounds.\n",
      "[20]\ttraining's auc: 0.730128\tvalid_1's auc: 0.727025\n",
      "[40]\ttraining's auc: 0.742084\tvalid_1's auc: 0.739119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 18:55:02 | DEBUG | automl_libs.grid_search       | _lgb_gs    | #101 | [do_preds] is True, generating predictions ...\n",
      "2018-06-28 18:55:02 | DEBUG | automl_libs.grid_search       | _lgb_gs    | #102 | Retrain model using best_round and all data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\ttraining's auc: 0.746279\tvalid_1's auc: 0.743096\n",
      "[10]\ttraining's auc: 0.721854\n",
      "[20]\ttraining's auc: 0.73033\n",
      "[30]\ttraining's auc: 0.73632\n",
      "[40]\ttraining's auc: 0.742531\n",
      "[50]\ttraining's auc: 0.746537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 18:55:09 | INFO  | automl_libs.grid_search       | _lgb_gs    | #113 | LGB predictions(1530226495) saved in ./output/gs_saved_preds/.\n",
      "2018-06-28 18:55:09 | DEBUG | automl_libs.grid_search       | gs         | #52  | ./output/lgb_grid_search.csv updated\n",
      "2018-06-28 18:55:09 | INFO  | automl_app.app.AlphaBoosting  | __init__   | #127 | STAGE: STACKNET\n",
      "2018-06-28 18:55:09 | DEBUG | automl_libs.stacknet          | layer1     | #36  | data_id: flight_data_ordinal  \n",
      "\tx_train: (500000, 12)\tx_test: (100000, 12)\n",
      "\ty_train type: <class 'dict'>\n",
      "\tcompatible_models: {'LGB'}\n",
      " \n",
      "2018-06-28 18:55:09 | DEBUG | automl_libs.stacknet          | layer1     | #56  | 1530224002 already processed in StackNet, so poped it from chosen_gs_results\n",
      "2018-06-28 18:55:09 | DEBUG | automl_libs.stacknet          | layer1     | #56  | 1530224256 already processed in StackNet, so poped it from chosen_gs_results\n",
      "2018-06-28 18:55:09 | DEBUG | automl_libs.stacknet          | layer1     | #62  | {'num_leaves': 31, 'metric': 'auc', 'bagging_fraction': 0.7, 'timestamp': '2018-06-28 18:54:27', 'bagging_seed': 246820, 'objective': 'binary', 'learning_rate': 0.03, 'scale_pos_weight': 1, 'feature_fraction_seed': 246820, 'gs_timespent': '0:00:06', 'pred_timespent': '0:00:06', 'feature_fraction': 1.0, 'best_round': 50, 'boosting': 'gbdt', 'bagging_freq': 1, 'num_threads': 8, 'min_data_in_leaf': 22, 'max_depth': -1, 'val_auc': 0.7301634687386862, 'train_auc': 0.7347318892742198}\n",
      "2018-06-28 18:55:09 | DEBUG | automl_libs.stacknet          | layer1     | #62  | {'num_leaves': 15, 'metric': 'auc', 'bagging_fraction': 0.9, 'timestamp': '2018-06-28 18:54:55', 'bagging_seed': 29114168, 'objective': 'binary', 'learning_rate': 0.1, 'scale_pos_weight': 1, 'feature_fraction_seed': 29114168, 'gs_timespent': '0:00:06', 'pred_timespent': '0:00:07', 'feature_fraction': 0.9, 'best_round': 50, 'boosting': 'gbdt', 'bagging_freq': 1, 'num_threads': 8, 'min_data_in_leaf': 28, 'max_depth': -1, 'val_auc': 0.7430956719315234, 'train_auc': 0.7462788585820737}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from file\n",
      "Computing... label: label        model_data_id: 1530226495__LGB_flight_data_ordinal\n",
      "processing fold 0...\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "[10]\ttraining's auc: 0.702\n",
      "[20]\ttraining's auc: 0.706908\n",
      "[30]\ttraining's auc: 0.711612\n",
      "[40]\ttraining's auc: 0.718401\n",
      "[50]\ttraining's auc: 0.723574\n",
      "processing fold 1...\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "[10]\ttraining's auc: 0.701282\n",
      "[20]\ttraining's auc: 0.707007\n",
      "[30]\ttraining's auc: 0.712307\n",
      "[40]\ttraining's auc: 0.718261\n",
      "[50]\ttraining's auc: 0.723435\n",
      "processing fold 2...\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "[10]\ttraining's auc: 0.701466\n",
      "[20]\ttraining's auc: 0.707491\n",
      "[30]\ttraining's auc: 0.711287\n",
      "[40]\ttraining's auc: 0.717718\n",
      "[50]\ttraining's auc: 0.724038\n",
      "processing fold 3...\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "[10]\ttraining's auc: 0.701262\n",
      "[20]\ttraining's auc: 0.707493\n",
      "[30]\ttraining's auc: 0.711752\n",
      "[40]\ttraining's auc: 0.717769\n",
      "[50]\ttraining's auc: 0.723635\n",
      "processing fold 4...\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "[10]\ttraining's auc: 0.700846\n",
      "[20]\ttraining's auc: 0.707183\n",
      "[30]\ttraining's auc: 0.711104\n",
      "[40]\ttraining's auc: 0.717876\n",
      "[50]\ttraining's auc: 0.723461\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "[10]\ttraining's auc: 0.701291\n",
      "[20]\ttraining's auc: 0.706341\n",
      "[30]\ttraining's auc: 0.710686\n",
      "[40]\ttraining's auc: 0.717204\n",
      "[50]\ttraining's auc: 0.722305\n",
      "Computing... label: label        model_data_id: 1530226467__LGB_flight_data_ordinal\n",
      "processing fold 0...\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "[10]\ttraining's auc: 0.700813\n",
      "[20]\ttraining's auc: 0.702517\n",
      "[30]\ttraining's auc: 0.704057\n",
      "[40]\ttraining's auc: 0.706323\n",
      "[50]\ttraining's auc: 0.708728\n",
      "processing fold 1...\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "[10]\ttraining's auc: 0.700522\n",
      "[20]\ttraining's auc: 0.702857\n",
      "[30]\ttraining's auc: 0.70497\n",
      "[40]\ttraining's auc: 0.706667\n",
      "[50]\ttraining's auc: 0.708702\n",
      "processing fold 2...\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "[10]\ttraining's auc: 0.700164\n",
      "[20]\ttraining's auc: 0.702188\n",
      "[30]\ttraining's auc: 0.704805\n",
      "[40]\ttraining's auc: 0.70712\n",
      "[50]\ttraining's auc: 0.709274\n",
      "processing fold 3...\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "[10]\ttraining's auc: 0.700324\n",
      "[20]\ttraining's auc: 0.703524\n",
      "[30]\ttraining's auc: 0.705446\n",
      "[40]\ttraining's auc: 0.707089\n",
      "[50]\ttraining's auc: 0.709008\n",
      "processing fold 4...\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "[10]\ttraining's auc: 0.699172\n",
      "[20]\ttraining's auc: 0.702195\n",
      "[30]\ttraining's auc: 0.704147\n",
      "[40]\ttraining's auc: 0.706013\n",
      "[50]\ttraining's auc: 0.707966\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "[10]\ttraining's auc: 0.700016\n",
      "[20]\ttraining's auc: 0.702511\n",
      "[30]\ttraining's auc: 0.704144\n",
      "[40]\ttraining's auc: 0.706354\n",
      "[50]\ttraining's auc: 0.708341\n",
      "1530226467__LGB_flight_data_ordinal 0.7024653713446725\n",
      "1530226495__LGB_flight_data_ordinal 0.7128083072326961\n",
      "1530226495__LGB_flight_data_ordinal already existed in the repo. score: 0 update to 0.7430956719315234\n",
      "1530226467__LGB_flight_data_ordinal already existed in the repo. score: 0 update to 0.7301634687386862\n",
      "load from file\n",
      "0.7430956719315234\t1530226495__LGB_flight_data_ordinal\n",
      "0.7367444682426831\t1530224256__LGB_flight_data_ordinal\n",
      "0.7301634687386862\t1530226467__LGB_flight_data_ordinal\n",
      "0.7245573469879929\t1530224002__LGB_flight_data_ordinal\n",
      "0.7233306979018436\t1530220352__LGB_flight_data_ordinal\n",
      "0.7218475202885716\t1530220336__LGB_flight_data_ordinal\n",
      "[('1530226495__LGB_flight_data_ordinal', 0.7430956719315234), ('1530224256__LGB_flight_data_ordinal', 0.7367444682426831), ('1530226467__LGB_flight_data_ordinal', 0.7301634687386862), ('1530224002__LGB_flight_data_ordinal', 0.7245573469879929), ('1530220352__LGB_flight_data_ordinal', 0.7233306979018436), ('1530220336__LGB_flight_data_ordinal', 0.7218475202885716)]\n",
      "Generating Layer2 model 1530226536__LOGREG OOF\n",
      "warning: x_train is not dataframe, be careful if categorical_feature is needed when fitting models like LGB\n",
      "processing fold 0...\n",
      "warning: x_train is not dataframe, be careful if categorical_feature is needed when fitting models like LGB\n",
      "processing fold 1...\n",
      "warning: x_train is not dataframe, be careful if categorical_feature is needed when fitting models like LGB\n",
      "processing fold 2...\n",
      "warning: x_train is not dataframe, be careful if categorical_feature is needed when fitting models like LGB\n",
      "processing fold 3...\n",
      "warning: x_train is not dataframe, be careful if categorical_feature is needed when fitting models like LGB\n",
      "processing fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 18:55:48 | INFO  | automl_app.app.AlphaBoosting  | __init__   | #131 | save run record\n",
      "2018-06-28 18:55:48 | INFO  | automl_app.app.AlphaBoosting  | _save_run_record | #163 | val index is saved at ./temp_data/val_index.pkl\n",
      "2018-06-28 18:55:48 | INFO  | automl_app.app.AlphaBoosting  | _save_run_record | #165 | run record is saved at ./output/last_run_record.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1530226536__LOGREG_layer2 0.7189749294878125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<automl_app.app.AlphaBoosting at 0x7f20daba6d30>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from automl_app.app import AlphaBoosting\n",
    "\n",
    "automl_config_file = project_path + 'automl_config.json'\n",
    "run_record_file_name = project_path + 'last_run_record.json' # don't created this file\n",
    "AlphaBoosting(automl_config_file, features_to_gen, gs_params_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
